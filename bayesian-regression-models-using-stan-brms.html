<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Bayesian Regression Models using ‘Stan’: brms | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.13.2 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Bayesian Regression Models using ‘Stan’: brms | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Bayesian Regression Models using ‘Stan’: brms | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />

<meta name="author" content="Shravan Vasishth, Bruno Nicenboim, and Daniel Schad" />


<meta name="date" content="2019-11-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deriving-the-posterior-through-sampling.html"/>
<link rel="next" href="summarizing-and-evaluation-the-posterior-distribution.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="discrete-random-variables-an-example-using-the-binomial-distribution.html"><a href="discrete-random-variables-an-example-using-the-binomial-distribution.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayes-factor-definition.html"><a href="bayes-factor-definition.html"><i class="fa fa-check"></i><b>3</b> Bayes factor: Definition</a></li>
<li class="chapter" data-level="4" data-path="introduction-to-bayesian-data-analysis-computationally.html"><a href="introduction-to-bayesian-data-analysis-computationally.html"><i class="fa fa-check"></i><b>4</b> Introduction to Bayesian data analysis computationally</a><ul>
<li class="chapter" data-level="4.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>4.1</b> Deriving the posterior through sampling</a></li>
<li class="chapter" data-level="4.2" data-path="bayesian-regression-models-using-stan-brms.html"><a href="bayesian-regression-models-using-stan-brms.html"><i class="fa fa-check"></i><b>4.2</b> Bayesian Regression Models using ‘Stan’: brms</a><ul>
<li class="chapter" data-level="4.2.1" data-path="bayesian-regression-models-using-stan-brms.html"><a href="bayesian-regression-models-using-stan-brms.html#a-simple-linear-model-a-single-participant-pressing-a-button-repeatedly"><i class="fa fa-check"></i><b>4.2.1</b> A simple linear model: A single participant pressing a button repeatedly</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="summarizing-and-evaluation-the-posterior-distribution.html"><a href="summarizing-and-evaluation-the-posterior-distribution.html"><i class="fa fa-check"></i><b>4.3</b> Summarizing and evaluation the posterior distribution</a><ul>
<li class="chapter" data-level="4.3.1" data-path="summarizing-and-evaluation-the-posterior-distribution.html"><a href="summarizing-and-evaluation-the-posterior-distribution.html#output-of-brms"><i class="fa fa-check"></i><b>4.3.1</b> Output of brms</a></li>
<li class="chapter" data-level="4.3.2" data-path="summarizing-and-evaluation-the-posterior-distribution.html"><a href="summarizing-and-evaluation-the-posterior-distribution.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>4.3.2</b> Posterior predictive checks</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="evaluating-the-priors-of-our-models.html"><a href="evaluating-the-priors-of-our-models.html"><i class="fa fa-check"></i><b>4.4</b> Evaluating the priors of our models</a><ul>
<li class="chapter" data-level="4.4.1" data-path="evaluating-the-priors-of-our-models.html"><a href="evaluating-the-priors-of-our-models.html#prior-predictive-distribution"><i class="fa fa-check"></i><b>4.4.1</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="4.4.2" data-path="evaluating-the-priors-of-our-models.html"><a href="evaluating-the-priors-of-our-models.html#influence-of-priors-and-sensitivity-analysis"><i class="fa fa-check"></i><b>4.4.2</b> Influence of priors and sensitivity analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>5</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-regression-models-using-stan-brms" class="section level2">
<h2><span class="header-section-number">4.2</span> Bayesian Regression Models using ‘Stan’: brms</h2>
<p>The surge in popularity of Bayesian statistics is closely tied to the increase in computing power and the appearance of probabilistic programming languages, such as WinBUGS <span class="citation">(Lunn et al. <a href="#ref-lunn2000winbugs">2000</a>)</span>, JAGS <span class="citation">(Plummer <a href="#ref-plummer2016jags">2016</a>)</span>, and more recently <code>pmc3</code> [@] and <code>Stan</code> <span class="citation">(Stan Development Team <a href="#ref-Stan">2019</a>)</span>. These statistical packages allow the user to define models without having to deal with the complexities of the sampling process for the most part. However, they require learning a new language since they require the user to fully specify the statistical model in a different language. Furthermore, some knowledge is needed to correctly parameterize the models and to avoid convergence issues. There are some alternatives that allow Bayesian inference in <code>R</code> without having to fully specify the model “by hand”. The packages <code>rstanarm</code> <span class="citation">(<span class="citeproc-not-found" data-reference-id="r-rstanarm"><strong>???</strong></span>)</span> and brms <span class="citation">(Bürkner <a href="#ref-R-brms">2019</a>)</span> emulates many popular R model-fitting functions, such as (g)lmer, using Stan for the back-end estimation and sampling. In addition, the BayesFactor [@] package emulates other standard frequentist tests (t-test, ANOVA, linear mod-els, etc.), and provides the Bayes factor given some pre-specified priors. Fora simpler option, JASP [@] provides a graphical user interface,and is an alternative to SPSS.</p>
<p>We will focus on <code>brms</code> in the first part of the book. This is because it can be useful for a smooth transition between frequentist linear mixed models and Bayesian ones. While <code>brms</code> is powerful enough to satisfy the statistical needs of many cognitive scientists, it has the added benefit that the Stan code can be inspected (with <code>make_stancode</code> and <code>make_standata</code>) allowing the users to augment the models or learn from them to eventually transition to write the full models in <code>Stan</code>.</p>
<!-- Stan is a probabilistic programming language for statistical inference written in C++ that can be accessed through several interfaces (e.g., R, Python, Matlab, etc.). We will focus on the package `rstan` [@R-rstan] that integrates Stan [@carpenterStanProbabilisticProgramming2017] to R [@R-base]. In Stan, we first define how the structure of the data looks like, the parameters we want to estimate, and then the priors  and likelihood. Stan uses a variant of Hamiltonian Monte Carlo (HMC), called No-U-Turn sampler (NUTS) [@JMLR:v15:hoffman14a], which is much more efficient than most handcrafted samplers, and also than the traditional Gibbs sampler used in other probabilistic languages such as (Win)BUGS [@lunn2000winbugs] and JAGS [@plummer2016jags]. -->
<p><!-- Inlme4syntax, the model we would fit wouldbelmer(log(rt) ~ cond + (cond|subj)+(cond|item))We can fit an analogous Bayesian linear mixed model with thestanlmerfunction from therstanarmpackage (Gabry and Goodrich, 2016); see the codein Listing 1. The main novelty in the syntax is the specification of the priorsfor each parameter. Some other details need to be specified, suchas the desirednumber of chains and iterations that the MCMC algorithm requires toconvergeto the posterior distribution of the parameter of interest. To speed up compu-tation, the number of processors (cores) available in their computer can also bespecified. --></p>
<p><!-- --></p>
<!-- ### Fitting linear models {#sec:RTs} -->
<!-- Suppose $y$ is a vector of continuous responses; assume for now that it is coming from a normal distribution: -->
<!-- \begin{equation} -->
<!-- y \sim Normal(\mu,\sigma) -->
<!-- \end{equation} -->
<!-- This is essentially a linear model, which is usually written in the following way: -->
<!-- \begin{equation} -->
<!-- y = \mu + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\sigma) -->
<!-- \end{equation} -->
<!-- We expand on this simple model next. -->
<div id="a-simple-linear-model-a-single-participant-pressing-a-button-repeatedly" class="section level3">
<h3><span class="header-section-number">4.2.1</span> A simple linear model: A single participant pressing a button repeatedly</h3>
<p>We’ll use the following example to illustrate the basic steps for fitting a model. Let’ say we have data of a participant repeatedly pressing the space-bar as fast as possible without paying attention to any stimuli. We would like to know how long it takes to press a key when there is no decision involved.</p>
<p>Let’s model the data with the following assumptions:</p>
<ol style="list-style-type: decimal">
<li>There is a true underlying time, <span class="math inline">\(\mu\)</span>, that the participant needs to press the space-bar.</li>
<li>There is some noise in this process.</li>
<li>The noise is normally distributed (this assumption is questionable given the skew but; we fix this assumption later).</li>
</ol>
<p>This means that the likelihood for each observation <span class="math inline">\(i\)</span> will be:</p>
<p><span class="math display" id="eq:rtlik">\[\begin{equation}
\begin{aligned}
rt_i \sim Normal(\mu, \sigma)
\end{aligned}
\tag{4.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(i =1 \ldots N\)</span>, and <span class="math inline">\(rt\)</span> is the dependent variable reaction times.</p>
<p>For a frequentist model that will give us the maximum likelihood estimate of the time it takes to press the space-bar, this would be enough to have a formula in <code>R</code> of the form <code>rt ~ 1</code> and plug it in <code>lm()</code> together with the data: <code>lm(rt ~ 1, data)</code>. The meaning of the 1 here is that there is no predictor associated with this parameter, and it will estimate the so-called intercept of the model, in our case <span class="math inline">\(\mu\)</span>.</p>
<p>For a Bayesian model, we will also need to define priors for the two parameters of our model. Let’s say that we know for sure that the time it takes to press a key will be positive and lower than a minute (<span class="math inline">\(60000ms\)</span>), but we don’t want to say which values are more likely. Regarding the noise, we know that the parameter <span class="math inline">\(\sigma\)</span> must be positive and we’ll assume that any value below 2000ms is equally likely. These priors are in general strongly discouraged because ????, but we’ll use them for now for pedagogical purposes.</p>
<p><span class="math display" id="eq:rtpriors">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim Uniform(0, 60000) \\
\sigma &amp;\sim Uniform(0, 2000) 
\end{aligned}
\tag{4.3}
\end{equation}\]</span></p>
<p>We’ll first load the data from <code>data/button_press.csv</code>:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1">df_noreading_data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;./data/button_press.csv&quot;</span>)</a>
<a class="sourceLine" id="cb69-2" data-line-number="2">df_noreading_data </a></code></pre></div>
<pre><code>## # A tibble: 361 x 2
##       rt trialn
##    &lt;dbl&gt;  &lt;dbl&gt;
##  1   141      1
##  2   138      2
##  3   128      3
##  4   132      4
##  5   126      5
##  6   134      6
##  7   163      7
##  8   149      8
##  9   133      9
## 10   110     10
## # … with 351 more rows</code></pre>
<p>It is a good idea to look at the distribution of the data before doing anything else. See Figure <a href="bayesian-regression-models-using-stan-brms.html#fig:m1visualize">4.2</a>.
The data looks a bit skewed, but we ignore this for the moment.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1"><span class="kw">ggplot</span>(df_noreading_data, <span class="kw">aes</span>(rt)) <span class="op">+</span></a>
<a class="sourceLine" id="cb71-2" data-line-number="2"><span class="st">    </span><span class="kw">geom_density</span>()<span class="op">+</span></a>
<a class="sourceLine" id="cb71-3" data-line-number="3"><span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Button-press data&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:m1visualize"></span>
<img src="bookdown_files/figure-html/m1visualize-1.svg" alt="Visualizing the data." width="672" />
<p class="caption">
FIGURE 4.2: Visualizing the data.
</p>
</div>
<div id="specifying-the-model-in-brms" class="section level4">
<h4><span class="header-section-number">4.2.1.1</span> Specifying the model in brms</h4>
<p>We’ll fit the model defined by equations <a href="bayesian-regression-models-using-stan-brms.html#eq:rtlik">(4.2)</a>-<a href="bayesian-regression-models-using-stan-brms.html#eq:rtlik">(4.2)</a> with brms in the following way:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1">fit_press &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb72-2" data-line-number="2">                 <span class="dt">data =</span> df_noreading_data,</a>
<a class="sourceLine" id="cb72-3" data-line-number="3">                 <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb72-4" data-line-number="4">                 <span class="dt">prior =</span> <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>,<span class="dv">60000</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb72-5" data-line-number="5">                           <span class="kw">prior</span>(<span class="kw">uniform</span>(<span class="dv">0</span>,<span class="dv">2000</span>), <span class="dt">class =</span> sigma)),</a>
<a class="sourceLine" id="cb72-6" data-line-number="6">                 <span class="dt">chains =</span> <span class="dv">4</span>,</a>
<a class="sourceLine" id="cb72-7" data-line-number="7">                 <span class="dt">iter =</span> <span class="dv">2000</span>,</a>
<a class="sourceLine" id="cb72-8" data-line-number="8">                 <span class="dt">warmup =</span> <span class="dv">1000</span>)</a></code></pre></div>
<p>The brms code has some differences from a model fit with <code>lm</code> (or <code>lmer</code> from the <code>lme4</code> package). At this beginning stage, we’ll focus on the following options.</p>
<ol style="list-style-type: decimal">
<li>The term <code>family = gaussian()</code> makes explicit that the underlying likelihood function is a normal distribution (Gaussian and normal are synonyms) that is implicit in lm(er). Other linking functions are possible, exactly as in the glm(er) function.</li>
<li>The term prior takes as argument a vector of priors. Although this specification of priors is optional, the researcher should always explicitly specify each prior. Otherwise, brms will define a prior by default, which may or may not be appropriate for the research area.</li>
<li>The term <code>chains</code> refers to the number of independent runs for sampling (by default four).</li>
<li>The term <code>iter</code> refers to the number of iterations that the sampler makes to sample from the posterior distribution of each parameter (by default 2000).</li>
<li>The term <code>warmup</code> refers to the number of iterations from the start of sampling that are eventually discarded (by default half of iter).</li>
</ol>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-brms">
<p>Bürkner, Paul-Christian. 2019. <em>Brms: Bayesian Regression Models Using ’Stan’</em>. <a href="https://CRAN.R-project.org/package=brms">https://CRAN.R-project.org/package=brms</a>.</p>
</div>
<div id="ref-lunn2000winbugs">
<p>Lunn, D.J., A. Thomas, N. Best, and D. Spiegelhalter. 2000. “WinBUGS-A Bayesian Modelling Framework: Concepts, Structure, and Extensibility.” <em>Statistics and Computing</em> 10 (4). Springer: 325–37.</p>
</div>
<div id="ref-plummer2016jags">
<p>Plummer, Martin. 2016. “JAGS Version 4.2.0 User Manual.”</p>
</div>
<div id="ref-Stan">
<p>Stan Development Team. 2019. “Stan Modeling Language Users Guide and Reference Manual, Version 2.19.2.” <a href="http://mc-stan.org/">http://mc-stan.org/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deriving-the-posterior-through-sampling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summarizing-and-evaluation-the-posterior-distribution.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/03-brms.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
