<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.1 A first linear model: Does attentional load affect pupil size? | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="4.1 A first linear model: Does attentional load affect pupil size? | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/yihui/bookdown/" />
  <meta property="og:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="rstudio/bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.1 A first linear model: Does attentional load affect pupil size? | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://bookdown.org/yihui/bookdown/images/cover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2019-12-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-regression-models.html"/>
<link rel="next" href="sec-trial.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.2</b> How to read this book</a></li>
<li class="chapter" data-level="0.3" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.3</b> Online materials</a></li>
<li class="chapter" data-level="0.4" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.4</b> Software needed</a></li>
<li class="chapter" data-level="0.5" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.5</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs.Â density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayesâ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayesâ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Deriving Bayesâ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="computational-bayesian-data-analysis.html"><a href="computational-bayesian-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using âStanâ: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="revisiting-the-button-pressing-example-with-different-priors.html"><a href="revisiting-the-button-pressing-example-with-different-priors.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#generating-prior-predictive-distributions-with-brms"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian-regression-models.html"><a href="bayesian-regression-models.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear model: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect pupil size?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#how-to-communicate-the-results-2"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayesian-hierarchical-models.html"><a href="bayesian-hierarchical-models.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal modelsâDistributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.2</b> Summary</a><ul>
<li class="chapter" data-level="5.2.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.2.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.3</b> Further reading</a></li>
<li class="chapter" data-level="5.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>6</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:pupil" class="section level2">
<h2><span class="header-section-number">4.1</span> A first linear model: Does attentional load affect pupil size?</h2>
<p>Weâll look at the effect of cognitive processing on human pupil size to illustrate the use of Bayesian linear regression models. Although pupil size is mostly related to the amount of light that reaches the retina or the distance to a perceived object, pupil sizes are also systematically influenced by cognitive processing: It has been found that increased cognitive load leads to an increase in the pupil size <span class="citation">(for a review, see Mathot <a href="#ref-mathotPupillometryPsychologyPhysiology2018">2018</a>)</span>.</p>
<p>For this example, weâll use the data of one participantâs pupil size of the control experiment of <span class="citation">Wahn et al. (<a href="#ref-wahnPupilSizesScale2016">2016</a>)</span> averaged by trial.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>
In this experiment, a participant covertly tracked between zero and five objects among several randomly moving objects on a computer screen. In this task, called multiple object tracking [or <a href="mailto:MOT;@pylyshynTrackingMultipleIndependent1988" class="email">MOT;@pylyshynTrackingMultipleIndependent1988</a>] task, several objects appear in the screen, and a subset of them
are indicated as âtargetsâ at the beginning. Then, the objects start moving randomly across the screen and become indistinguishable. After several seconds, the objects stop moving and the participant needs to indicate which objects were the targets. See also Figure <a href="sec-pupil.html#fig:mot">4.1</a>. Our research goal is to examine how the number of moving objects being tracked, that is how the attentional load, affects pupil size.</p>

<div class="figure" style="text-align: center"><span id="fig:mot"></span>
<img src="cc_figure/MOT.png" alt="Flow of events in a trial where two objects needs to be tracked. Adapted from Blumberg, Peterson, and Parasuraman (2015); licensed under CC BY 4.0." width="80%" />
<p class="caption">
FIGURE 4.1: Flow of events in a trial where two objects needs to be tracked. Adapted from <span class="citation">Blumberg, Peterson, and Parasuraman (<a href="#ref-Blumberg2015">2015</a>)</span>; licensed under CC BY 4.0.
</p>
</div>
<div id="likelihood-and-priors" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Likelihood and priors</h3>
<p>We will model pupil size as normally distributed, because we are not expecting a skew, and we have no further information available about the distribution of pupil sizes. (Notice that pupil sizes cannot be of size zero or negative, so we know for sure that this choice is not exactly right.) For simplicity, we are also going to assume a linear relationship between load and the pupil size.</p>
<p>Letâs summarize our assumptions:</p>
<ol style="list-style-type: decimal">
<li>There is some average pupil size represented by <span class="math inline">\(\alpha\)</span>.</li>
<li>The increase of attentional load has a linear relationship with pupil size, determined by <span class="math inline">\(\beta\)</span>.</li>
<li>There is some noise in this process, that is, variability around the true pupil size i.e., a scale, <span class="math inline">\(\sigma\)</span>.</li>
<li>The noise is normally distributed.</li>
</ol>
<p>Our likelihood will be as follows:</p>
<p><span class="math display">\[\begin{equation}
p\_size_n \sim Normal(\alpha + c\_load_n \cdot \beta,\sigma)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(n\)</span> indicates the observation number with <span class="math inline">\(n = 1 \ldots N\)</span></p>
<p>This means that the formula that weâll use in <code>brms</code> will be <code>p_size ~ 1 + c_load</code>, where <code>1</code> represents the intercept, <span class="math inline">\(\alpha\)</span>, which doesnât depend on a covariate or predictor, and <code>c_load</code> is our covariate that is multiplied by <span class="math inline">\(\beta\)</span>. We will generally indicate with the prefix <code>c_</code>, that a covariate (in this case load) is centered (i.e., we subtract from each value the mean of all values). If load is centered, the intercept represents the pupil size at the average load in the experiment (because at the average load, the centered load is zero, and then <span class="math inline">\(\alpha + 0 \cdot \beta\)</span>). Alternatively, if the load would not have been centered (i.e., starts with no load, then one, two, etc), then the intercept would represent the pupil size when there is no load. Although this formula would be enough to fit a frequentist model with <code>lm(p_size ~ 1 + c_load, dataset)</code>, when we fit a Bayesian model, we have to specify priors for each of the parameters.</p>
<p>For setting the priors, we need information about pupil sizes. While we might know that pupil sizes range between 2 and 5 millimeters <span class="citation">(<span class="citeproc-not-found" data-reference-id="reference"><strong>???</strong></span>)</span>, this experiment was conducted with the Eyelink-II eyetracker which measures the pupils in arbitrary units <span class="citation">(Hayes and Petrov <a href="#ref-hayesMappingCorrectingInfluence2016">2016</a>)</span>. If this is our first analysis of pupil size, before setting up the priors, weâll need to look at some measures of pupil size. (If we had analyzed this type of data before, we could also look at estimates from previous experiments). Fortunately, we have some measurements of the same participant with no attentional load for the first 100ms, each 10 ms, in <code>pupil_pilot.csv</code>: This will give us some idea about the order of magnitude of our dependent variable.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1">df_pupil_pilot &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;./data/pupil_pilot.csv&quot;</span>)</a>
<a class="sourceLine" id="cb116-2" data-line-number="2">df_pupil_pilot<span class="op">$</span>p_size <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     852     856     862     861     866     868</code></pre>
<p>With this information we can set a regularizing prior for <span class="math inline">\(\alpha\)</span>. We center the prior around 1000 to be in the right order of magnitude.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a> Since we donât know how much pupil sizes are going to vary by load yet, we include a rather wide prior by defining it as a normally distribution and setting its standard deviation as <span class="math inline">\(500\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\alpha \sim Normal(1000, 500) 
\end{equation}\]</span></p>
<p>Given that our covariate load is centered, with the prior for <span class="math inline">\(\alpha\)</span>, we are saying that we suspect that the average pupil size for the average load in the experiment will be in a 95% central interval limited by approximately <span class="math inline">\(1000 \pm 2 \cdot 500 = [20, 2000]\)</span> units. We can caclulate this in <code>R</code> using the <code>qnorm</code> function:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" data-line-number="1"><span class="kw">c</span>(<span class="kw">qnorm</span>(.<span class="dv">025</span>, <span class="dv">1000</span>,<span class="dv">500</span>), <span class="kw">qnorm</span>(.<span class="dv">975</span>, <span class="dv">1000</span>, <span class="dv">500</span>))</a></code></pre></div>
<pre><code>## [1]   20 1980</code></pre>
<p>We know that the measurements of the pilot data are strongly correlated because they were taken together just some milliseconds apart. For this reason, they wonât tell us how much the pupil size can vary. We set up a quite weak prior for <span class="math inline">\(\sigma\)</span> that encodes our lack of precise information: <span class="math inline">\(\sigma\)</span> is surely larger than zero and has to be in the order of magnitude of the pupil size with no load.</p>
<p><span class="math display">\[\begin{equation}
\sigma \sim Normal_+(0, 1000)
\end{equation}\]</span></p>
<p>With this prior for <span class="math inline">\(\sigma\)</span>, we are saying that we expect that the standard deviation of the pupil sizes should be in the following 95% central interval. (Notice that we use <code>qtnorm(..., a = 0)</code> and not <code>qnorm()</code>).</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">c</span>(<span class="kw">qtnorm</span>(.<span class="dv">025</span>, <span class="dv">0</span>, <span class="dv">1000</span>, <span class="dt">a =</span> <span class="dv">0</span>), <span class="kw">qtnorm</span>(.<span class="dv">975</span>, <span class="dv">70</span>,<span class="dv">1000</span>, <span class="dt">a =</span> <span class="dv">0</span>))</a></code></pre></div>
<pre><code>## [1]   31 2290</code></pre>
<p>We still need to set a prior for <span class="math inline">\(\beta\)</span>, the change in pupil size produced by the attentional load. Given that pupil size changes are not easily perceptible (we donât see them in our day-to-day life), we expect them to be much smaller than the pupil size, so we use the following prior:</p>
<p><span class="math display">\[\begin{equation}
\beta \sim Normal(0, 100)
\end{equation}\]</span></p>
<p>With the prior of <span class="math inline">\(\beta\)</span>, we are saying that we donât really know if the attentional load will increase or even decrease the pupil size (notice that is centered in zero), but we do know that one unit of load (that is one more object to track) will potentially change the pupil size in a way that is consistent with the following 95% central interval.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="kw">c</span>(<span class="kw">qnorm</span>(.<span class="dv">025</span>, <span class="dv">0</span>,<span class="dv">100</span>), <span class="kw">qnorm</span>(.<span class="dv">975</span>, <span class="dv">0</span>,<span class="dv">100</span>))</a></code></pre></div>
<pre><code>## [1] -196  196</code></pre>
<p>That is, we donât expect changes in size that increase or decrease the pupil size in more than 200 units.</p>

<div class="rmdnote">
to-do: maybe prior predictive distributions here??
</div>

</div>
<div id="the-brms-model" class="section level3">
<h3><span class="header-section-number">4.1.2</span> The <code>brms</code> model</h3>
<p>Before fitting the <code>brms</code> model, we load the data and center the predictor <code>load</code>:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">df_pupil_data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/pupil.csv&quot;</span>)</a>
<a class="sourceLine" id="cb124-2" data-line-number="2">df_pupil_data &lt;-<span class="st"> </span>df_pupil_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb124-3" data-line-number="3"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">c_load =</span> load <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(load))</a>
<a class="sourceLine" id="cb124-4" data-line-number="4">df_pupil_data</a></code></pre></div>
<pre><code>## # A tibble: 41 x 4
##   trial  load p_size c_load
##   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1     1     2  1021. -0.439
## 2     2     1   951. -1.44 
## 3     3     5  1064.  2.56 
## 4     4     4   913.  1.56 
## 5     5     0   603. -2.44 
## # â¦ with 36 more rows</code></pre>
<p>Now we can fit the <code>brms</code> model:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1">fit_pupil &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>c_load,</a>
<a class="sourceLine" id="cb126-2" data-line-number="2">                 <span class="dt">data =</span> df_pupil_data,</a>
<a class="sourceLine" id="cb126-3" data-line-number="3">                 <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb126-4" data-line-number="4">                 <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb126-5" data-line-number="5">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">1000</span>, <span class="dv">500</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb126-6" data-line-number="6">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb126-7" data-line-number="7">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> c_load)</a>
<a class="sourceLine" id="cb126-8" data-line-number="8">                 )) </a></code></pre></div>
<p>The only difference from our previous models is that we now have a predictor in the formula and in the priors. Priors for predictors are indicated with <code>class = b</code>, and the specific predictor with <code>coef = c_load</code>. If we want to set the same priors to different predictors we can omit the argument <code>coef</code>. We can remove the <code>1</code> of the formula, and <code>brm()</code> will fit the exact same model as when we specify <code>1</code> explicitly. If we really want to remove the intercept we indicate this with <code>0 +...</code> or <code>-1 +...</code>. See also the box <a href="sec-pupil.html#thm:intercept">4.1</a> for more details about the treatment of the intercepts by <code>brms</code>.</p>
<p>We can inspect the output of our model now:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1"><span class="kw">plot</span>(fit_pupil)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-83-1.svg" width="672" /></p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">fit_pupil</a></code></pre></div>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: p_size ~ 1 + c_load 
##    Data: df_pupil_data (Number of observations: 41) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat
## Intercept   701.57     20.36   660.74   740.48 1.00
## c_load       33.81     11.84    10.26    56.93 1.00
##           Bulk_ESS Tail_ESS
## Intercept     3326     2585
## c_load        3405     2630
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat
## sigma   128.33     15.84   101.39   163.81 1.00
##       Bulk_ESS Tail_ESS
## sigma     3572     2835
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>We discuss how we could communicate the relevant information in the next section.</p>

<div class="extra">

<div class="theorem">
<span id="thm:intercept" class="theorem"><strong>Box 4.1  </strong></span><strong>Intercepts in <code>brms</code></strong>
</div>
<p>When we set up a prior for the intercept in <code>brms</code>, we actually set a prior for an intercept given that all the predictors are centered. This did not matter in our previous examples because we centered our predictor (or we had none), but it might matter if we want to have uncentered predictors. In the design we are discussing, a non-centered predictor of load will mean that the intercept, <span class="math inline">\(\alpha\)</span>, has a straightforward interpretation (in many cases, however, an intercept with a non-centered predictor wonât have a straightforward interpretation): the pupil size when there is no attention load.</p>
<p>We might be more sure about prior values for the no load condition, and we want to set the following prior to our new <span class="math inline">\(\alpha\)</span>: <span class="math inline">\(Normal(800,200)\)</span>. In this case, we should fit the following model:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb130-1" data-line-number="1">fit_pupil_non_centered &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>intercept <span class="op">+</span><span class="st"> </span>load,</a>
<a class="sourceLine" id="cb130-2" data-line-number="2">                 <span class="dt">data =</span> df_pupil_data,</a>
<a class="sourceLine" id="cb130-3" data-line-number="3">                 <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb130-4" data-line-number="4">                 <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb130-5" data-line-number="5">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">800</span>, <span class="dv">200</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> intercept),</a>
<a class="sourceLine" id="cb130-6" data-line-number="6">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb130-7" data-line-number="7">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> load)</a>
<a class="sourceLine" id="cb130-8" data-line-number="8">                 ))</a></code></pre></div>
<p>Notice that we remove the regular centered intercept by adding <code>0</code> to the formula, and we replace it with the âactualâ intercept we want to set priors to with <code>intercept</code>âthis is a reserved word, and thus we cannot name any predictor with this name. This new parameter is also of the class <code>b</code>, so its prior needs to be defined accordingly.</p>
<p>The output below shows that, as expected, while the posterior for the intercept has changed noticeably, the posterior for the effect of load remains virtually unchanged.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_pupil_non_centered)</a></code></pre></div>
<pre><code>##             Estimate Est.Error   Q2.5 Q97.5
## b_intercept      624      34.8  557.8   696
## b_load            32      11.7    8.9    55
## sigma            129      14.9  103.5   161
## lp__            -271       1.3 -274.2  -270</code></pre>
<p>Notice the following potential pitfall. A model like the one below will fit a non-centered load predictor, but will assign a prior of <span class="math inline">\(Normal(800,200)\)</span> to the intercept of a <em>centered</em> model, <span class="math inline">\(\alpha_{centered}\)</span>, and not the current intercept, <span class="math inline">\(\alpha\)</span>.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1">fit_pupil_wrong &lt;-<span class="st"> </span><span class="kw">brm</span>(p_size <span class="op">~</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>load,</a>
<a class="sourceLine" id="cb133-2" data-line-number="2">                 <span class="dt">data =</span> df_pupil_data,</a>
<a class="sourceLine" id="cb133-3" data-line-number="3">                 <span class="dt">family =</span> <span class="kw">gaussian</span>(),</a>
<a class="sourceLine" id="cb133-4" data-line-number="4">                 <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb133-5" data-line-number="5">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">800</span>, <span class="dv">100</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb133-6" data-line-number="6">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb133-7" data-line-number="7">                     <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">100</span>), <span class="dt">class =</span> b, <span class="dt">coef =</span> load)</a>
<a class="sourceLine" id="cb133-8" data-line-number="8">                 ))</a></code></pre></div>
<p>What does it mean to set a prior to <span class="math inline">\(\alpha_{centered}\)</span> in a model that <em>doesnât</em> include <span class="math inline">\(\alpha_{centered}\)</span>?</p>
<p>Notice that the fitted values of the non-centered model and the centered one are identical, that is, the expected values of the response distribution without the residual error (when <span class="math inline">\(\sigma =0\)</span>) are identical for both models:</p>
<p><span class="math display" id="eq:fitted">\[\begin{equation}
\alpha + load_n \cdot \beta = \alpha_{centered} + (load_n - mean(load)) \cdot \beta 
\tag{4.1}
\end{equation}\]</span></p>
<p>The left side of Equation <a href="sec-pupil.html#eq:fitted">(4.1)</a> refers to the fitted values based on our current non-centered model, and the right side refers to the fitted values based on the centered model. We can re-arrange terms to understand what is the effect of a prior on <span class="math inline">\(\alpha_{centered}\)</span> in our model that <em>doesnât</em> include <span class="math inline">\(\alpha_{centered}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\alpha + load_n \cdot \beta &amp;= \alpha_{centered} + load_n\cdot \beta - mean(load) \cdot \beta\\
\alpha  &amp;= \alpha_{centered}  - mean(load) \cdot \beta\\
\alpha + mean(load) \cdot \beta  &amp;= \alpha_{centered}  
\end{aligned}
\end{equation}\]</span></p>
<p>That means that we are actually setting our prior to <span class="math inline">\(\alpha + mean(load) \cdot \beta\)</span>.
When <span class="math inline">\(\beta\)</span> is very small, and the prior for <span class="math inline">\(\alpha\)</span> is very wide, we might hardly notice the difference between setting a prior to <span class="math inline">\(\alpha_{centered}\)</span> or to our actual <span class="math inline">\(\alpha\)</span> in a non-centered model (especially if the likelihood dominates anyway). But itâs a good idea to pay attention to what are the parameters we are setting priors to.</p>
</div>

</div>
<div id="how-to-communicate-the-results" class="section level3">
<h3><span class="header-section-number">4.1.3</span> How to communicate the results?</h3>
<p>We want to answer our research question âWhat is the effect of attentional load on the participantâs pupil size?â For that weâll need to examine what happens with <span class="math inline">\(\beta\)</span>, which is <code>c_load</code> in the summary of <code>brms</code>. The summary of the posterior tells us that the most likely values of <span class="math inline">\(\beta\)</span> will be around the mean of the posterior, 33.81, and we can be 95% certain that the true value of <span class="math inline">\(\beta\)</span> <em>given the model and the data</em> lies between 10.26 and 56.93.</p>
<p>We see that as the attentional load increases, the pupil size of the participant becomes larger. If we want to determine how likely it is that the pupil size increased rather than decreased, we can examine the proportion of samples above zero. (Notice that the intercept and the slopes, are always preceded by <code>b_</code> in <code>brms</code>. One can see all the names of parameters being estimated with <code>parnames()</code>.)</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1"><span class="kw">mean</span>(<span class="kw">posterior_samples</span>(fit_pupil)<span class="op">$</span>b_c_load <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p><strong>Take into account that this probability ignores the possibility of the participant not being affected at all by the manipulation, this is because <span class="math inline">\(P(\beta=0)=0\)</span>, weâll come back to this issue in the model comparison section @ref(sec:??).</strong></p>
</div>
<div id="sec:pupiladq" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Descriptive adequacy</h3>
<p>Our model converged and we obtained a posterior distribution, there is, however, no guarantee that our model was adequate to represent our data. We can use posterior predictive checks to verify this.</p>
<p>Sometimes itâs useful to build our own posterior predictive check to visualize the fit of our model, as opposed to use the <code>pp_check</code> functions as we did before in section <a href="sec-ppd.html#sec:ppd">3.5</a>. For example, here we use <code>posterior_predict()</code> to generate 1000 posterior predictive distributions, and we convert them from an array to a long data frame.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1"><span class="co"># we start from an array of 1000 samples by 41 observations</span></a>
<a class="sourceLine" id="cb136-2" data-line-number="2">df_pupil_pred &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(fit_pupil, <span class="dt">nsamples =</span> <span class="dv">1000</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb136-3" data-line-number="3"><span class="st">    </span><span class="co"># we convert it to a list of length 1000, with 41 observations in each element:</span></a>
<a class="sourceLine" id="cb136-4" data-line-number="4"><span class="st">    </span><span class="kw">array_branch</span>(<span class="dt">margin =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb136-5" data-line-number="5"><span class="st">    </span><span class="co"># We iterate over the elements (the predicted distributions)</span></a>
<a class="sourceLine" id="cb136-6" data-line-number="6"><span class="st">    </span><span class="co"># and we convert them into a long data frame similar to the data,</span></a>
<a class="sourceLine" id="cb136-7" data-line-number="7"><span class="st">    </span><span class="co"># but with an extra column `iter` indicating from which iteration</span></a>
<a class="sourceLine" id="cb136-8" data-line-number="8"><span class="st">    </span><span class="co"># the sample is coming from.</span></a>
<a class="sourceLine" id="cb136-9" data-line-number="9"><span class="st">    </span><span class="kw">map_dfr</span>( <span class="cf">function</span>(yrep_iter) {</a>
<a class="sourceLine" id="cb136-10" data-line-number="10">        df_pupil_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb136-11" data-line-number="11"><span class="st">            </span><span class="kw">mutate</span>(<span class="dt">p_size =</span> yrep_iter)</a>
<a class="sourceLine" id="cb136-12" data-line-number="12">    }, <span class="dt">.id =</span> <span class="st">&quot;iter&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb136-13" data-line-number="13"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">iter =</span> <span class="kw">as.numeric</span>(iter))</a></code></pre></div>
<p>Then we plot 100 of the densities of the predicted distributions in blue, and the distribution of our data in black for the five levels of load in Figure <a href="sec-pupil.html#fig:postpreddens">4.2</a>. We donât have enough data to derive a strong conclusion: Notice that both the predictive distributions and our data look very wide, and it hard to tell if the distribution of the observations could have been generated by our model. For now we can say that it doesnât look too bad.</p>

<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" data-line-number="1">df_pupil_pred <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb137-2" data-line-number="2"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(p_size, <span class="dt">group=</span>iter)) <span class="op">+</span></a>
<a class="sourceLine" id="cb137-3" data-line-number="3"><span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">alpha =</span> <span class="fl">.05</span>, <span class="dt">stat=</span><span class="st">&quot;density&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb137-4" data-line-number="4"><span class="st">    </span><span class="kw">geom_density</span>(<span class="dt">data=</span>df_pupil_data, <span class="kw">aes</span>(p_size),</a>
<a class="sourceLine" id="cb137-5" data-line-number="5">                 <span class="dt">inherit.aes =</span> <span class="ot">FALSE</span>, <span class="dt">size =</span><span class="dv">1</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb137-6" data-line-number="6"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data=</span>df_pupil_data, <span class="kw">aes</span>(<span class="dt">x=</span>p_size, <span class="dt">y =</span> <span class="fl">-0.001</span>), <span class="dt">alpha =</span>.<span class="dv">5</span>,</a>
<a class="sourceLine" id="cb137-7" data-line-number="7">               <span class="dt">inherit.aes =</span> <span class="ot">FALSE</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb137-8" data-line-number="8"><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.002</span>, <span class="fl">.01</span>))<span class="op">+</span></a>
<a class="sourceLine" id="cb137-9" data-line-number="9"><span class="st">    </span><span class="kw">facet_grid</span>(load <span class="op">~</span><span class="st"> </span>.) </a></code></pre></div>
<div class="figure"><span id="fig:postpreddens"></span>
<img src="bookdown_files/figure-html/postpreddens-1.svg" alt="The plot shows 100 predicted distributions in blue density plots, the distribution of pupil size data in black density plots, and the observed pupil sizes in black dots for the five levels of attentional load." width="672" />
<p class="caption">
FIGURE 4.2: The plot shows 100 predicted distributions in blue density plots, the distribution of pupil size data in black density plots, and the observed pupil sizes in black dots for the five levels of attentional load.
</p>
</div>
<p>We can instead look at the distribution of a statistic, such as mean pupil size by load:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1"><span class="co"># predicted means:</span></a>
<a class="sourceLine" id="cb138-2" data-line-number="2">df_pupil_pred_summary &lt;-<span class="st"> </span>df_pupil_pred <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb138-3" data-line-number="3"><span class="st">    </span><span class="kw">group_by</span>(iter, load) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb138-4" data-line-number="4"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">av_p_size =</span> <span class="kw">mean</span>(p_size))</a>
<a class="sourceLine" id="cb138-5" data-line-number="5"><span class="co"># observed means:</span></a>
<a class="sourceLine" id="cb138-6" data-line-number="6">df_pupil_summary &lt;-<span class="st"> </span>df_pupil_data <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb138-7" data-line-number="7"><span class="st">    </span><span class="kw">group_by</span>(load) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb138-8" data-line-number="8"><span class="st">    </span><span class="kw">summarize</span>(<span class="dt">av_p_size =</span> <span class="kw">mean</span>(p_size))</a></code></pre></div>

<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1"><span class="kw">ggplot</span>(df_pupil_pred_summary, <span class="kw">aes</span>(av_p_size)) <span class="op">+</span></a>
<a class="sourceLine" id="cb139-2" data-line-number="2"><span class="st">    </span><span class="kw">geom_histogram</span>(<span class="dt">alpha=</span>.<span class="dv">5</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb139-3" data-line-number="3"><span class="st">    </span><span class="kw">geom_vline</span>(<span class="kw">aes</span>(<span class="dt">xintercept=</span> av_p_size),<span class="dt">data=</span> df_pupil_summary)<span class="op">+</span></a>
<a class="sourceLine" id="cb139-4" data-line-number="4"><span class="st">    </span><span class="kw">facet_grid</span>(load <span class="op">~</span><span class="st"> </span>.)</a></code></pre></div>
<div class="figure"><span id="fig:postpredmean"></span>
<img src="bookdown_files/figure-html/postpredmean-1.svg" alt="Distribution of posterior predicted means in gray and observed pupil size means in black lines by load." width="672" />
<p class="caption">
FIGURE 4.3: Distribution of posterior predicted means in gray and observed pupil size means in black lines by load.
</p>
</div>
<p>Figure <a href="sec-pupil.html#fig:postpredmean">4.3</a> shows that the observed means for no load and for a load of two are falling in the tails of the distributions. While our model predicts a monotonic increase of pupil size, the data might be indicating that the relevant difference is between (i) no load, (ii) a load between two and three, and then (iii) a load of four, and (iv) of five. However, given the uncertainty in the posterior predictive distributions and that the observed means are contained somewhere in the predicted distributions, it could be the case that we are overinterpreting noise.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Blumberg2015">
<p>Blumberg, Eric J., Matthew S. Peterson, and Raja Parasuraman. 2015. âEnhancing Multiple Object Tracking Performance with Noninvasive Brain Stimulation: A Causal Role for the Anterior Intraparietal Sulcus.â <em>Frontiers in Systems Neuroscience</em> 9: 3. <a href="https://doi.org/10.3389/fnsys.2015.00003">https://doi.org/10.3389/fnsys.2015.00003</a>.</p>
</div>
<div id="ref-hayesMappingCorrectingInfluence2016">
<p>Hayes, Taylor R., and Alexander A. Petrov. 2016. âMapping and Correcting the Influence of Gaze Position on Pupil Size Measurements.â <em>Behavior Research Methods</em> 48 (2): 510â27. <a href="https://doi.org/10.3758/s13428-015-0588-x">https://doi.org/10.3758/s13428-015-0588-x</a>.</p>
</div>
<div id="ref-mathotPupillometryPsychologyPhysiology2018">
<p>Mathot, Sebastiaan. 2018. âPupillometry: Psychology, Physiology, and Function.â <em>Journal of Cognition</em> 1 (1): 16. <a href="https://doi.org/10.5334/joc.18">https://doi.org/10.5334/joc.18</a>.</p>
</div>
<div id="ref-wahnPupilSizesScale2016">
<p>Wahn, Basil, Daniel P. Ferris, W. David Hairston, and Peter KÃ¶nig. 2016. âPupil Sizes Scale with Attentional Load and Task Experience in a Multiple Object Tracking Task.â <em>PLOS ONE</em> 11 (12): e0168087. <a href="https://doi.org/10.1371/journal.pone.0168087">https://doi.org/10.1371/journal.pone.0168087</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="8">
<li id="fn8"><p>The full dataset can be found in <a href="https://osf.io/z43dz/" class="uri">https://osf.io/z43dz/</a>. We show our preprocessing in the appendix of this chapter, section <a href="appendix-1.html#sec:preprocessingpupil">4.7.1</a>.<a href="sec-pupil.html#fnref8" class="footnote-back">â©</a></p></li>
<li id="fn9"><p>The average pupil size will probably be higher than 800, since this measurement was with no load, but, in any case, the exact number wonât matter, any mean between 500-1500 would be fine if the standard deviation is large.<a href="sec-pupil.html#fnref9" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-regression-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-trial.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/04-regressions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
