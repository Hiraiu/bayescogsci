
# Prior predictive distribution

If we are not sure whether the increase of set size could produce something between a null effect and a  relatively large effect, we can choose the prior with a standard deviation of $0.5$.

```{r priors4beta2,echo=FALSE,message = FALSE}
diff_accuracy %>%
    ggplot(aes(diffaccuracy)) +
    geom_histogram() +
    facet_grid(diffsize~prior)
```

# Priors


We settle one the following priors:

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 1.5) \\
\beta &\sim Normal(0, 0.5) 
\end{aligned}
\end{equation}


# The `brms` model

Having decided on the likelihood, the link function, and the priors, the model can now be fit using `brms`. Notice that we need to specify that the family is `bernoulli()`, and the link is `logit`. 

# The `brms` model


```{r,  echo=FALSE,message = FALSE, warning = FALSE}
df_recall_data <- read_table2("./data/PairsRSS1_all.dat",
                              col_names = c("subject", "session", "block",
                                            "trial", "set_size",
                                            "response_size_list",
                                            "response_size_new_words",
                                            "tested", "response",
                                            "response_category", "rt")) %>%
    # ignore the type of incorrect responses (the focus of the paper)
    mutate(correct = if_else(response_category ==1, 1, 0)) %>%
    # use only the data from the free recall task:
    # (when there was no list of possible responses)
    filter(response_size_list + response_size_new_words == 0) %>%
    # select one subject
    filter(subject == 10) %>%
    mutate(c_set_size = set_size - mean(set_size))
# ignore the warning from read_table

# Set sizes in the dataset:
df_recall_data$set_size %>%
    unique
# Trials by set size
df_recall_data %>%
    group_by(set_size) %>%
    count()
```


```{r}
library(brms)
fit_recall <- brm(correct ~ 1 + c_set_size,
  data = df_recall_data,
  family = bernoulli(link = logit),
  prior = c(
    prior(normal(0, 1.5), class = Intercept),
    prior(normal(0, .5), class = b, coef = c_set_size)
  )
)
```

# The posteriors

Next, look at the summary of the posteriors of each of the parameters. Keep in mind that the parameters are in log-odds space:

```{r}
posterior_summary(fit_recall)[1:2,c(1,3,4)]
```

# The posteriors

Plot the posteriors as well:

```{r}
plot(fit_recall)
```

#  How to communicate the results?

```{r, echo=FALSE, results="hide"}
alpha_samples<- posterior_samples(fit_recall)$b_Intercept
beta_samples<- posterior_samples(fit_recall)$b_c_set_size
beta_mean <- round(mean(beta_samples),5)
beta_low <- round(quantile(beta_samples,prob=0.025),5)
beta_high <- round(quantile(beta_samples,prob=0.975),5)
```

#  How to communicate the results?

Here, we are in situation analogous as before with the log-normal model. If we want to talk about the effect estimated by the model in log-odds space, we summarize the posterior of $\beta$ in the following way: $\hat\beta = `r mean(beta_samples)`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$.

#  How to communicate the results?

However, the effect might be easier to understand in proportions rather than in log-odds.  Let's look at the average accuracy for the task first:

```{r}
alpha_samples<- posterior_samples(fit_recall)$b_Intercept
av_accuracy <- plogis(alpha_samples)
c(mean = mean(av_accuracy), quantile(av_accuracy, c(.025,.975)))
```

#  How to communicate the results?

As before, to transform the effect of our manipulation to an easier to interpret scale (i.e., proportion), we need to take  into account that the scale is not linear, and that the effect of increasing the set size depends on the average accuracy, and the set size that we start from.

#  How to communicate the results?

We can do the following calculation, similar to what we did for the trial effects experiment, to find out the decrease in accuracy in proportions or probability scale:

```{r}
beta_samples<- posterior_samples(fit_recall)$b_c_set_size
effect_middle <- plogis(alpha_samples) - plogis(alpha_samples - beta_samples)
c(mean = mean(effect_middle), quantile(effect_middle, c(.025,.975)))
```

#  How to communicate the results?

Notice the interpretation here, if we increase the set size from the average set size minus one to the average set size, we get a reduction in the accuracy of recall of $`r mean(effect_middle)`$, 95% CrI = $[ `r quantile(effect_middle, .025)` , `r quantile(effect_middle, .975)` ]$. 

#  How to communicate the results?

Recall that the average set size, 5, was not presented to the subject! We could also look at the decrease in accuracy from a set size of 2 to 4:


```{r}
effect_4m2 <- plogis(alpha_samples+  (4 - mean(df_recall_data$set_size)) * beta_samples) -
    plogis(alpha_samples+  (2 - mean(df_recall_data$set_size)) * beta_samples)
c(mean = mean(effect_4m2), quantile(effect_4m2, c(.025,.975)))
```

We see that increasing the set size does have a detrimental effect in recall, as we suspected.

