<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.2 Multinomial processing tree (MPT) models | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="9.2 Multinomial processing tree (MPT) models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.2 Multinomial processing tree (MPT) models | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-07-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modeling-multiple-categorical-responses.html"/>
<link rel="next" href="further-readings.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">

 /* Uncomment this and comment the next one to show the solutions */

 /* $(document).ready(function() {
  *     $folds = $(".solution");
  *     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
  *     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
  *     $(".solution-blck").toggle();  // fold all blocks
  *     $(".solution-btn").on("click", function() {  // add onClick event
  *         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
  *         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  *     })
  * }); */

 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\"></button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         /* $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution"  */
         /* $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself. */
     })
 });

</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="3.8.1" data-path="ex-compbda.html"><a href="ex-compbda.html#a-simple-linear-model-exercises-section-refsecsimplenormal"><i class="fa fa-check"></i><b>3.8.1</b> A simple linear model exercises (Section @ref(sec:simplenormal))</a></li>
<li class="chapter" data-level="3.8.2" data-path="ex-compbda.html"><a href="ex-compbda.html#revisiting-the-button-pressing-example-with-different-priors-exercises-section-refsecrevisit"><i class="fa fa-check"></i><b>3.8.2</b> Revisiting the button-pressing example with different priors exercises (Section @ref(sec:revisit))</a></li>
<li class="chapter" data-level="3.8.3" data-path="ex-compbda.html"><a href="ex-compbda.html#posterior-predictive-distribution-and-log-normal-model-exercises-section-refsecppd"><i class="fa fa-check"></i><b>3.8.3</b> Posterior predictive distribution and log-normal model exercises (Section @ref(sec:ppd))</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#how-to-communicate-the-results-2"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a><ul>
<li class="chapter" data-level="4.6.1" data-path="exercises-2.html"><a href="exercises-2.html#a-first-linear-regression-exercises-section-refsecpupil"><i class="fa fa-check"></i><b>4.6.1</b> A first linear regression exercises (Section @ref(sec:pupil))</a></li>
<li class="chapter" data-level="4.6.2" data-path="exercises-2.html"><a href="exercises-2.html#log-normal-model-exercises-section-refsectrial"><i class="fa fa-check"></i><b>4.6.2</b> Log-normal model exercises (Section @ref(sec:trial))</a></li>
<li class="chapter" data-level="4.6.3" data-path="exercises-2.html"><a href="exercises-2.html#logistic-regression-exercises-section-refseclogistic"><i class="fa fa-check"></i><b>4.6.3</b> Logistic regression exercises (section @ref(sec:logistic))</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exercises-3.html"><a href="exercises-3.html#ex:hierarchical-logn"><i class="fa fa-check"></i><b>5.5.1</b> Hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding, interactions, etc</a></li>
<li class="chapter" data-level="7" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>7</b> Model comparison using Bayes factors</a><ul>
<li class="chapter" data-level="7.1" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>7.1</b> Summary</a></li>
<li class="chapter" data-level="7.2" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.2</b> Further reading</a></li>
<li class="chapter" data-level="7.3" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>7.3</b> Exercises</a><ul>
<li class="chapter" data-level="7.3.1" data-path="exercises-4.html"><a href="exercises-4.html#ex:bf-logn"><i class="fa fa-check"></i><b>7.3.1</b> Bayes factor for a hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>8</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="8.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>8.2</b> Meta-analysis</a><ul>
<li class="chapter" data-level="8.2.1" data-path="meta-analysis.html"><a href="meta-analysis.html#using-brms"><i class="fa fa-check"></i><b>8.2.1</b> Using brms</a></li>
<li class="chapter" data-level="8.2.2" data-path="meta-analysis.html"><a href="meta-analysis.html#using-stan"><i class="fa fa-check"></i><b>8.2.2</b> Using Stan</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>8.3</b> Measurement-error models</a><ul>
<li class="chapter" data-level="8.3.1" data-path="measurement-error-models.html"><a href="measurement-error-models.html#using-brms-1"><i class="fa fa-check"></i><b>8.3.1</b> Using brms</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>8.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cognitive-modeling-using-multinomial-processing-trees.html"><a href="cognitive-modeling-using-multinomial-processing-trees.html"><i class="fa fa-check"></i><b>9</b> Cognitive Modeling using multinomial processing trees</a><ul>
<li class="chapter" data-level="9.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>9.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="9.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>9.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>9.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>9.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="9.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>9.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="further-readings.html"><a href="further-readings.html"><i class="fa fa-check"></i><b>9.3</b> Further readings:</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>10</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multinomial-processing-tree-mpt-models" class="section level2">
<h2><span class="header-section-number">9.2</span> Multinomial processing tree (MPT) models</h2>
<p>Multinomial processing tree (MPT) modeling is a method that estimates latent variables that have a psychological interpretation given categorical data <span class="citation">(a review is provided in Batchelder and Riefer <a href="#ref-BatchelderRiefer1999">1999</a>)</span>. In other words, an MPT model is just one way to model categorical responses following a multinomial or categorical distribution. MPT models assume that the observed response categories result from a sequences of underlying cognitive events which are represented as a tree.</p>
<div id="mpts-for-modeling-picture-naming-abilities-in-aphasia" class="section level3">
<h3><span class="header-section-number">9.2.1</span> MPTs for modeling picture naming abilities in aphasia</h3>
<p><span class="citation">Walker, Hickok, and Fridriksson (<a href="#ref-WalkerEtAl2018">2018</a>)</span> created an MPT model that specifies a set of possible internal errors that lead to the various possible response types during a picture naming trial for aphasic patients. Here we’ll explore a simplification of the original model.</p>

<div class="figure"><span id="fig:MPT-tikz"></span>
<img src="bookdown_files/figure-html/MPT-tikz-1.svg" alt="Representation of a simplification of the MPT used in Walker, Hickok, and Fridriksson (2018)." width="672" />
<p class="caption">
FIGURE 9.3: Representation of a simplification of the MPT used in <span class="citation">Walker, Hickok, and Fridriksson (<a href="#ref-WalkerEtAl2018">2018</a>)</span>.
</p>
</div>
<table>
<caption><span id="tab:MPT-params">TABLE 9.2: </span> Psychological interpretation of the parameters of the MPT model.</caption>
<thead>
<tr class="header">
<th align="left">Param.</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">a</td>
<td align="left">Probability of initiating an attempt</td>
</tr>
<tr class="even">
<td align="left">t</td>
<td align="left">Probability of selecting a target word over competitors</td>
</tr>
<tr class="odd">
<td align="left">f</td>
<td align="left">Probability of retrieving correct phonemes</td>
</tr>
<tr class="even">
<td align="left">c</td>
<td align="left">Probability of a phoneme change in the target word creating a real word</td>
</tr>
</tbody>
</table>
<div id="calculation-of-the-probabilities" class="section level4">
<h4><span class="header-section-number">9.2.1.1</span> Calculation of the probabilities</h4>
<p>By navigating through the branches of the MPT (figure <a href="multinomial-processing-tree-mpt-models.html#fig:MPT-tikz">9.3</a>), we can calculate the probabilities of the four responses (the categorical outcomes), based on the underlying parameters assumed in the MPT:</p>
<ul>
<li><span class="math inline">\(P(NR| a,t,f,c)= 1-a\)</span></li>
<li><span class="math inline">\(P(Neologism| a,t,f,c)= a \cdot (1-t) \cdot (1-f) \cdot (1-c) + a \cdot t \cdot (1-f) \cdot (1-c)\)</span></li>
<li><span class="math inline">\(P(Formal| a,t,f,c)= a \cdot (1-t) \cdot (1-f) \cdot c + a \cdot t \cdot (1-f) \cdot c\)</span></li>
<li><span class="math inline">\(P(Mixed| a,t,f,c)= a \cdot (1-t) \cdot f\)</span></li>
<li><span class="math inline">\(P(Correct| a,t,f,c)= a \cdot t \cdot f\)</span></li>
</ul>
<p>Given that <span class="math inline">\(P(NR| a,t,f,c) + P(Neologism| a,t,f,c) + P(Formal| a,t,f,c) + P(Mixed| a,t,f,c) + P(Correct| a,t,f,c) = 1\)</span>, there is no need to characterize every outcome: we can always calculate the any one of the remaining responses as <span class="math inline">\(1 - other\text{ }responses\)</span>.</p>
</div>
<div id="generate-simulated-data" class="section level4">
<h4><span class="header-section-number">9.2.1.2</span> Generate simulated data</h4>
<p>First, simulate 200 trials assuming no variability between items and participants. It is necessary to define functions to compute each outcome’s probability, based on the MPT.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" data-line-number="1"><span class="co"># Probabilities of different answers</span></a>
<a class="sourceLine" id="cb309-2" data-line-number="2">Pr_NR &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb309-3" data-line-number="3">    <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>a</a>
<a class="sourceLine" id="cb309-4" data-line-number="4"></a>
<a class="sourceLine" id="cb309-5" data-line-number="5">Pr_Neologism &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb309-6" data-line-number="6">    a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>c) <span class="op">+</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>t <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>c)</a>
<a class="sourceLine" id="cb309-7" data-line-number="7"></a>
<a class="sourceLine" id="cb309-8" data-line-number="8">Pr_Formal &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb309-9" data-line-number="9">    a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>c <span class="op">+</span><span class="st">  </span>a <span class="op">*</span><span class="st"> </span>t <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>f) <span class="op">*</span><span class="st"> </span>c</a>
<a class="sourceLine" id="cb309-10" data-line-number="10"></a>
<a class="sourceLine" id="cb309-11" data-line-number="11">Pr_Mixed &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb309-12" data-line-number="12">    a <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>t) <span class="op">*</span><span class="st"> </span>f</a>
<a class="sourceLine" id="cb309-13" data-line-number="13"></a>
<a class="sourceLine" id="cb309-14" data-line-number="14">Pr_Correct &lt;-<span class="st"> </span><span class="cf">function</span>(a, t, f, c)</a>
<a class="sourceLine" id="cb309-15" data-line-number="15">    a <span class="op">*</span><span class="st"> </span>t <span class="op">*</span><span class="st"> </span>f</a>
<a class="sourceLine" id="cb309-16" data-line-number="16"></a>
<a class="sourceLine" id="cb309-17" data-line-number="17"><span class="co"># true underlying values for simulated data</span></a>
<a class="sourceLine" id="cb309-18" data-line-number="18">a_true &lt;-<span class="st"> </span><span class="fl">.75</span></a>
<a class="sourceLine" id="cb309-19" data-line-number="19">t_true &lt;-<span class="st"> </span><span class="fl">.9</span></a>
<a class="sourceLine" id="cb309-20" data-line-number="20">f_true &lt;-<span class="st"> </span><span class="fl">.8</span></a>
<a class="sourceLine" id="cb309-21" data-line-number="21">c_true &lt;-<span class="st"> </span><span class="fl">.1</span></a>
<a class="sourceLine" id="cb309-22" data-line-number="22"></a>
<a class="sourceLine" id="cb309-23" data-line-number="23"><span class="co"># Probability of the different answers:</span></a>
<a class="sourceLine" id="cb309-24" data-line-number="24">Theta &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">NR =</span> <span class="kw">Pr_NR</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb309-25" data-line-number="25">                <span class="dt">Neologism =</span> <span class="kw">Pr_Neologism</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb309-26" data-line-number="26">                <span class="dt">Formal =</span> <span class="kw">Pr_Formal</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb309-27" data-line-number="27">                <span class="dt">Mixed =</span> <span class="kw">Pr_Mixed</span>(a_true, t_true, f_true, c_true),</a>
<a class="sourceLine" id="cb309-28" data-line-number="28">                <span class="dt">Correct =</span> <span class="kw">Pr_Correct</span>(a_true, t_true, f_true, c_true))</a>
<a class="sourceLine" id="cb309-29" data-line-number="29"></a>
<a class="sourceLine" id="cb309-30" data-line-number="30">N_trials &lt;-<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb309-31" data-line-number="31"></a>
<a class="sourceLine" id="cb309-32" data-line-number="32">ans &lt;-<span class="st"> </span><span class="kw">rmultinom</span>(<span class="dv">1</span>, N_trials, <span class="kw">c</span>(Theta))</a>
<a class="sourceLine" id="cb309-33" data-line-number="33">ans</a></code></pre></div>
<pre><code>##           [,1]
## NR          56
## Neologism   20
## Formal       4
## Mixed       13
## Correct    107</code></pre>
</div>
<div id="sec:MPT-s" class="section level4">
<h4><span class="header-section-number">9.2.1.3</span> A simple MPT model in Stan</h4>
<p>The above data can be modeled in Stan as discussed below (see <code>stan_models/mpt_3.stan</code>). The probabilities of the different categories go into the <code>transformed parameters</code> section. The data are modeled as coming from a multinomial likelihood. If priors are not specified, then a beta distribution with <span class="math inline">\(a=1\)</span> and <span class="math inline">\(b=1\)</span> is assumed for the parameters <span class="math inline">\(a\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(f\)</span>, and <span class="math inline">\(c\)</span>.</p>
<pre><code>data { 
  int&lt;lower = 1&gt; N_trials;
  int&lt;lower = 0, upper = N_trials&gt; ans[5];
}
parameters {
  real&lt;lower = 0, upper = 1&gt; a;
  real&lt;lower = 0, upper = 1&gt; t;
  real&lt;lower = 0, upper = 1&gt; f;
  real&lt;lower = 0, upper = 1&gt; c;
} 
transformed parameters {
  simplex[5] theta;
  theta[1] = 1 - a; //Pr_NR
  theta[2] = a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c); //Pr_Neologism
  theta[3] = a * (1 - t) * (1 - f) * c +  a * t * (1 - f) * c;  //Pr_Formal
  theta[4] = a * (1 - t) * f; //Pr_Mixed
  theta[5] = a * t * f; //Pr_Correct
}
model {
  target += beta_lpdf(a | 2, 2);
  target += beta_lpdf(t | 2, 2);
  target += beta_lpdf(f | 2, 2);
  target += beta_lpdf(c | 2, 2);
  target += multinomial_lpmf(ans | theta);
}
generated quantities{
    int pred_ans[5];
  pred_ans = multinomial_rng(theta, 5);
}</code></pre>
<p>Fit the model:</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb312-1" data-line-number="1">data_sMPT &lt;-<span class="st">  </span><span class="kw">list</span>(<span class="dt">N_trials =</span> N_trials,</a>
<a class="sourceLine" id="cb312-2" data-line-number="2">               <span class="dt">ans =</span> <span class="kw">c</span>(ans)) </a>
<a class="sourceLine" id="cb312-3" data-line-number="3">fit_sMPT &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mpt_3.stan&#39;</span>, <span class="dt">data =</span> data_sMPT)  </a></code></pre></div>
<p>Print out a summary of the posterior of the parameter of interest:</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" data-line-number="1"><span class="kw">print</span>(fit_sMPT, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;t&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;c&quot;</span>), </a>
<a class="sourceLine" id="cb313-2" data-line-number="2">      <span class="dt">probs =</span> <span class="kw">c</span>(.<span class="dv">0275</span>,.<span class="dv">5</span>,.<span class="dv">975</span>))</a></code></pre></div>
<pre><code>## Inference for Stan model: mpt_3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##   mean se_mean   sd 2.8%  50%  98% n_eff Rhat
## a 0.72       0 0.03 0.66 0.72 0.77  4786    1
## t 0.88       0 0.03 0.82 0.88 0.93  4754    1
## f 0.82       0 0.03 0.76 0.83 0.88  4164    1
## c 0.21       0 0.08 0.09 0.21 0.38  4560    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jul  7 13:44:00 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>Evaluate whether the model recovers the true parameters that generated the data; see Figure <a href="multinomial-processing-tree-mpt-models.html#fig:sMPT-posterior">9.4</a>.</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" data-line-number="1"><span class="kw">mcmc_hist</span>(<span class="kw">as.array</span>(fit_sMPT), </a>
<a class="sourceLine" id="cb315-2" data-line-number="2">          <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;t&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;c&quot;</span>)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb315-3" data-line-number="3"><span class="st">        </span><span class="kw">geom_vline</span>(<span class="dt">data =</span> <span class="kw">tibble</span>(<span class="dt">Parameter =</span> <span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;t&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;c&quot;</span>), </a>
<a class="sourceLine" id="cb315-4" data-line-number="4">                                 <span class="dt">value =</span> <span class="kw">c</span>(a_true, t_true, f_true, c_true)),</a>
<a class="sourceLine" id="cb315-5" data-line-number="5">                   <span class="kw">aes</span>(<span class="dt">xintercept =</span> value)) <span class="op">+</span></a>
<a class="sourceLine" id="cb315-6" data-line-number="6"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</a></code></pre></div>
<div class="figure"><span id="fig:sMPT-posterior"></span>
<img src="bookdown_files/figure-html/sMPT-posterior-1.svg" alt="Posterior distributions and true values of the parameters of the simple MPT model (mpt_3.stan)." width="672" />
<p class="caption">
FIGURE 9.4: Posterior distributions and true values of the parameters of the simple MPT model (mpt_3.stan).
</p>
</div>
<p>The posteriors of the <span class="math inline">\(\theta\)</span> parameters can also be summarized:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb316-1" data-line-number="1"><span class="kw">print</span>(fit_sMPT, <span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;theta&quot;</span>), <span class="dt">probs =</span> <span class="kw">c</span>(.<span class="dv">0275</span>,.<span class="dv">5</span>,.<span class="dv">975</span>))</a></code></pre></div>
<pre><code>## Inference for Stan model: mpt_3.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##          mean se_mean   sd 2.8%  50%  98% n_eff Rhat
## theta[1] 0.28       0 0.03 0.23 0.28 0.35  4786    1
## theta[2] 0.10       0 0.02 0.06 0.10 0.14  4253    1
## theta[3] 0.03       0 0.01 0.01 0.03 0.05  4429    1
## theta[4] 0.07       0 0.02 0.04 0.07 0.11  4746    1
## theta[5] 0.52       0 0.03 0.45 0.52 0.59  4692    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jul  7 13:44:00 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
</div>
<div id="sec:MPT-reg" class="section level4">
<h4><span class="header-section-number">9.2.1.4</span> An MPT assuming by-item variability</h4>
<p>The use of aggregated data implies the assumption that the estimated parameters do not vary too much between subjects and items. If this assumption is incorrect, the analysis of aggregated data may lead to erroneous conclusions: reliance on aggregated data in the presence of parameter heterogeneity may lead to biased parameter estimates and the underestimation of credible intervals.</p>
<p>For example, if it is known that <span class="math inline">\(f\)</span> is affected by the phonological complexity of the individual word (e.g., <em>cat</em> is easier to produce than <em>umbrella</em>), the previous model does not have a way to include that information.</p>
<p>Simulated data can be generated taking into account the <code>complexity</code> of the items. Assume here for simplicity that the complexity of items is scaled and centered; i.e., mean complexity is represented by 0, and the standard deviation is assumed to be 1. Thus, the predictor complexity can be represented as a Normal(0,1). We will assume a regression model that determines the probability to be a function of complexity, determined by some property of the item.</p>
<p>One important detail is that <code>f</code> is a probability and needs to be bounded between 0 and 1. To make sure that this property is met, the computation of <code>f</code> for each item will be converted to probability space using the logistic function. This is achieved as follows.</p>
<p>Suppose that <span class="math inline">\(f\)</span> is a linear function of complexity. For example, two parameters <span class="math inline">\(\alpha_f\)</span> and <span class="math inline">\(\beta_f\)</span> could determine how <span class="math inline">\(f\)</span> is affected by complexity:</p>
<p><span class="math inline">\(f&#39;_j=\alpha_f + complexity_j\cdot \beta_f\)</span>.</p>
<p>The parameters <span class="math inline">\(\alpha_f\)</span> and <span class="math inline">\(\beta_f\)</span> are defined in an unconstrained log-odds space (they can be any real number). The model that is fit then yields an <span class="math inline">\(f&#39;_j\)</span> value for each item <span class="math inline">\(j\)</span> in log-odds space. The log-odds value <span class="math inline">\(f&#39;_j\)</span> can be converted to a probability value <span class="math inline">\(f_{true}\)</span> by applying the logistic function (or the inverse logit, <span class="math inline">\(logit^{-1}\)</span>) to <span class="math inline">\(f&#39;\)</span>. Recall from the generalized linear model discussed earlier that if we have a model in log-odds space:</p>
<p><span class="math display">\[\begin{equation}
\log \left(\frac{p_j}{(1-p_j)}\right) = \alpha + \beta\cdot x_j = \mu_j
\end{equation}\]</span></p>
<p>Then we can recover the probability <span class="math inline">\(p_j\)</span> by solving for <span class="math inline">\(p_j\)</span>:</p>
<p><span class="math display">\[\begin{equation}
p_j = \frac{\exp(\mu_j)}{1+\exp(\mu_j)}
\end{equation}\]</span></p>
<p>The above is the logistic or inverse logit function: it takes as input <span class="math inline">\(\mu_j\)</span> and returns the corresponding probability <span class="math inline">\(p_j\)</span>. The plogis function in R carries out the calculation shown above.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" data-line-number="1">N_obs &lt;-<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb318-2" data-line-number="2">complexity &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_obs, <span class="dv">0</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb318-3" data-line-number="3"></a>
<a class="sourceLine" id="cb318-4" data-line-number="4"><span class="co">## choose some hypothetical values:</span></a>
<a class="sourceLine" id="cb318-5" data-line-number="5">f_alpha &lt;-<span class="st"> </span><span class="fl">.3</span></a>
<a class="sourceLine" id="cb318-6" data-line-number="6">f_beta &lt;-<span class="st"> </span><span class="fl">.05</span></a>
<a class="sourceLine" id="cb318-7" data-line-number="7"></a>
<a class="sourceLine" id="cb318-8" data-line-number="8"><span class="co">## get probabilities f for each item:</span></a>
<a class="sourceLine" id="cb318-9" data-line-number="9">f_true &lt;-<span class="st"> </span><span class="kw">plogis</span>(f_alpha <span class="op">+</span><span class="st"> </span>complexity <span class="op">*</span><span class="st"> </span>f_beta)</a></code></pre></div>
<p>This change in our assumptions entails that the probability of each response changes with the item. The parameters <code>theta</code> now have to be a matrix (this is in R, in Stan, we’ll code it as an array of simplexes).<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1">theta_NR_v &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">Pr_NR</span>(a_true, t_true, f_true, c_true), N_obs)  </a>
<a class="sourceLine" id="cb319-2" data-line-number="2">theta_Neologism_v &lt;-<span class="st"> </span><span class="kw">Pr_Neologism</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb319-3" data-line-number="3">theta_Formal_v &lt;-<span class="st"> </span><span class="kw">Pr_Formal</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb319-4" data-line-number="4">theta_Mixed_v &lt;-<span class="st"> </span><span class="kw">Pr_Mixed</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb319-5" data-line-number="5">theta_Correct_v &lt;-<span class="st"> </span><span class="kw">Pr_Correct</span>(a_true, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb319-6" data-line-number="6"></a>
<a class="sourceLine" id="cb319-7" data-line-number="7">theta_item &lt;-<span class="st"> </span><span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb319-8" data-line-number="8">                  <span class="kw">c</span>(theta_NR_v,</a>
<a class="sourceLine" id="cb319-9" data-line-number="9">                  theta_Neologism_v,</a>
<a class="sourceLine" id="cb319-10" data-line-number="10">                  theta_Formal_v,</a>
<a class="sourceLine" id="cb319-11" data-line-number="11">                  theta_Mixed_v,</a>
<a class="sourceLine" id="cb319-12" data-line-number="12">                  theta_Correct_v),<span class="dt">ncol=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb319-13" data-line-number="13"></a>
<a class="sourceLine" id="cb319-14" data-line-number="14"><span class="kw">head</span>(theta_item)</a></code></pre></div>
<pre><code>##      [,1] [,2]  [,3]  [,4] [,5]
## [1,] 0.25 0.27 0.030 0.045 0.40
## [2,] 0.25 0.31 0.035 0.040 0.36
## [3,] 0.25 0.31 0.035 0.040 0.36
## [4,] 0.25 0.28 0.031 0.044 0.40
## [5,] 0.25 0.30 0.034 0.041 0.37
## [6,] 0.25 0.29 0.033 0.042 0.38</code></pre>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="kw">str</span>(theta_item)</a></code></pre></div>
<pre><code>##  num [1:50, 1:5] 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25 ...</code></pre>
<p>Generate data:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" data-line-number="1">sim_data_cx &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">item =</span> <span class="dv">1</span><span class="op">:</span>N_obs,</a>
<a class="sourceLine" id="cb323-2" data-line-number="2">       <span class="dt">complexity =</span>  complexity,</a>
<a class="sourceLine" id="cb323-3" data-line-number="3">       <span class="dt">w_ans =</span> <span class="kw">c</span>(extraDistr<span class="op">::</span><span class="kw">rcat</span>(N_obs,theta_item)))</a>
<a class="sourceLine" id="cb323-4" data-line-number="4"></a>
<a class="sourceLine" id="cb323-5" data-line-number="5"><span class="kw">head</span>(sim_data_cx)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##    item complexity w_ans
##   &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1     1      1.88      1
## 2     2     -3.19      2
## 3     3     -3.24      5
## 4     4      0.999     3
## 5     5     -1.99      1
## # … with 1 more row</code></pre>
<p>The following model (saved in <code>stan_models/mpt_4.stan</code>) is essentially doing the same as the previous model but instead of fitting a multinomial to the summary of all the trials, it is fitting a categorical distribution to each individual observation. (This is analogous to the difference between the Bernoulli and Binomial distributions). It is not the appropriate model for the generative process that we are assuming in this section, but it is a good start.</p>
<pre><code>data {
  int&lt;lower=1&gt; N_obs;
  int&lt;lower=1,upper=5&gt; w_ans[N_obs];
}
parameters {
  real&lt;lower=0,upper=1&gt; a;
  real&lt;lower=0,upper=1&gt; t;
  real&lt;lower=0,upper=1&gt; f;
  real&lt;lower=0,upper=1&gt; c;
}
transformed parameters {
  simplex[5] theta[N_obs];

  for(n in 1:N_obs){
    //Pr_NR:
    theta[n, 1] = 1 - a;
    //Pr_Neologism:
    theta[n, 2] = a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c);
    //Pr_Formal:
    theta[n, 3] = a * (1 - t) * (1 - f) * c +  a * t * (1 - f) * c;
    //Pr_Mixed:
    theta[n, 4] = a * (1 - t) * f;
    //Pr_Correct:
    theta[n, 5] = a * t * f;
  }
}
model {
  target += beta_lpdf(a | 2, 2);
  target += beta_lpdf(t | 2, 2);
  target += beta_lpdf(f | 2, 2);
  target += beta_lpdf(c | 2, 2);
  for(n in 1:N_obs)
    target += categorical_lpmf(w_ans[n] | theta[n]);
}
generated quantities{
    int pred_w_ans[N_obs];
  for(n in 1:N_obs)
    pred_w_ans[n] = categorical_rng(theta[n]);
}</code></pre>
</div>
<div id="sec:MPT-h" class="section level4">
<h4><span class="header-section-number">9.2.1.5</span> A hierarchical MPT in Stan</h4>
<p>The previous model doesn’t take into account that subjects might vary and that items might vary beyond what is accounted by complexity. Let’s focus on taking into account the differences between subjects.</p>
<p>Different subjects might be differently motivated to the task. This can be accounted by adding a hierarchical structure to the parameter <code>a</code>. Begin by simulating some data that incorporates by-subject variability.</p>
<p>First, define the number of items and subjects, and the number of observations:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb326-1" data-line-number="1"><span class="co"># Data:</span></a>
<a class="sourceLine" id="cb326-2" data-line-number="2">N_item &lt;-<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb326-3" data-line-number="3">N_subj &lt;-<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb326-4" data-line-number="4">N_obs &lt;-<span class="st"> </span>N_item <span class="op">*</span><span class="st"> </span>N_subj </a></code></pre></div>
<p>Then, generate a vector for subjects and for items. Assume here that each subject sees each item.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1">subject &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_subj, <span class="dt">each =</span> N_item)</a>
<a class="sourceLine" id="cb327-2" data-line-number="2">item &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_item, <span class="dt">time =</span> N_subj)</a></code></pre></div>
<p>A vector representing complexity is created for the number of items we have, and this vector is repeated as many times as there are subjects:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" data-line-number="1">complexity &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">rnorm</span>(N_item, <span class="dv">0</span>, <span class="dv">2</span>),N_subj)</a></code></pre></div>
<p>Next, create a data-frame with all the above information:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" data-line-number="1">exp_sim &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">subject =</span> subject,</a>
<a class="sourceLine" id="cb329-2" data-line-number="2">                  <span class="dt">item =</span> item,</a>
<a class="sourceLine" id="cb329-3" data-line-number="3">                  <span class="dt">complexity =</span> complexity)</a>
<a class="sourceLine" id="cb329-4" data-line-number="4"><span class="kw">head</span>(exp_sim)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##   subject  item complexity
##     &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;
## 1       1     1     1.86  
## 2       1     2    -2.20  
## 3       1     3     0.0404
## 4       1     4    -0.625 
## 5       1     5    -1.28  
## # … with 1 more row</code></pre>
<p>To create subject-level variability in the data, a between-subject standard deviation needs to be defined. This standard deviation represents the deviations of subjects about the grand mean alpha value. It is important to note here that we are defining this adjustment in log-odds space.</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1"><span class="co"># New parameters, in log-odds space:</span></a>
<a class="sourceLine" id="cb331-2" data-line-number="2">sigma_a_alpha_subject &lt;-<span class="st"> </span><span class="fl">1.1</span></a>
<a class="sourceLine" id="cb331-3" data-line-number="3"><span class="co">## generate subject adjustments in log-odds space:</span></a>
<a class="sourceLine" id="cb331-4" data-line-number="4">a_alpha_subject &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_subj, <span class="dv">0</span>, sigma_a_alpha_subject)</a>
<a class="sourceLine" id="cb331-5" data-line-number="5">a_alpha_subject</a></code></pre></div>
<pre><code>##  [1]  0.2076 -0.9511 -0.1193  1.1182 -0.1636  1.3971
##  [7]  0.9247 -0.4636 -1.1772  0.2230 -1.2250  2.4934
## [13] -1.5872 -0.8833  2.7603  0.1967  0.9152  1.8145
## [19] -3.3830 -1.4159  2.7633  0.6643  0.4972  0.8532
## [25] -2.0145  1.3032  1.1163  0.0857 -0.0092  1.0810</code></pre>
<p>Given the fixed <code>a_true</code> probability value of 0.75, the subject-level values for individual <code>a_true</code> can be derived by (a) first converting the overall <code>a_true</code> value to log-odds space, (b) adding the by-subject adjustment to this converted overall value, and (c) then convert back to probability space using the logistic or inverse logit (plogis) function.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" data-line-number="1"><span class="co">## convert to log-odds space:</span></a>
<a class="sourceLine" id="cb333-2" data-line-number="2">alpha_a&lt;-<span class="kw">qlogis</span>(a_true)</a>
<a class="sourceLine" id="cb333-3" data-line-number="3">a_true_h &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_a <span class="op">+</span><span class="st"> </span>a_alpha_subject[subject])</a></code></pre></div>
<p>What this achieves mathematically is adding varying intercepts by subjects to <code>alpha_a</code>, and then the values adjusted by subject are saved in probability space.</p>
<p>As before, f_true is computed as a function of complexity:</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1">f_true &lt;-<span class="st"> </span><span class="kw">plogis</span>(f_alpha <span class="op">+</span><span class="st"> </span>complexity <span class="op">*</span><span class="st"> </span>f_beta)</a></code></pre></div>
<p>Now, we can define the probabilities of different outcomes:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" data-line-number="1"><span class="co"># Aux. parameters that define the probabilities:</span></a>
<a class="sourceLine" id="cb335-2" data-line-number="2">theta_NR_v_h &lt;-<span class="st"> </span><span class="kw">Pr_NR</span>(a_true_h, t_true, f_true, c_true) </a>
<a class="sourceLine" id="cb335-3" data-line-number="3">theta_Neologism_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Neologism</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb335-4" data-line-number="4">theta_Formal_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Formal</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb335-5" data-line-number="5">theta_Mixed_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Mixed</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb335-6" data-line-number="6">theta_Correct_v_h &lt;-<span class="st"> </span><span class="kw">Pr_Correct</span>(a_true_h, t_true, f_true, c_true)</a>
<a class="sourceLine" id="cb335-7" data-line-number="7">theta_h &lt;-<span class="st"> </span><span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb335-8" data-line-number="8">          <span class="kw">c</span>(theta_NR_v_h,</a>
<a class="sourceLine" id="cb335-9" data-line-number="9">          theta_Neologism_v_h,</a>
<a class="sourceLine" id="cb335-10" data-line-number="10">          theta_Formal_v_h,</a>
<a class="sourceLine" id="cb335-11" data-line-number="11">          theta_Mixed_v_h,</a>
<a class="sourceLine" id="cb335-12" data-line-number="12">          theta_Correct_v_h),<span class="dt">ncol=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb335-13" data-line-number="13"><span class="kw">head</span>(theta_h)</a></code></pre></div>
<pre><code>##      [,1] [,2]  [,3]  [,4] [,5]
## [1,] 0.21 0.29 0.032 0.047 0.42
## [2,] 0.21 0.32 0.036 0.043 0.39
## [3,] 0.21 0.30 0.033 0.045 0.41
## [4,] 0.21 0.31 0.034 0.045 0.40
## [5,] 0.21 0.31 0.035 0.044 0.40
## [6,] 0.21 0.31 0.035 0.044 0.40</code></pre>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1"><span class="kw">str</span>(theta_h)</a></code></pre></div>
<pre><code>##  num [1:600, 1:5] 0.213 0.213 0.213 0.213 0.213 ...</code></pre>
<p>The probability specifications shown above can now generate the simulated data:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="co"># simulated data:</span></a>
<a class="sourceLine" id="cb339-2" data-line-number="2">sim_data_h &lt;-<span class="st"> </span><span class="kw">mutate</span>(exp_sim,</a>
<a class="sourceLine" id="cb339-3" data-line-number="3">                 <span class="dt">w_ans =</span> extraDistr<span class="op">::</span><span class="kw">rcat</span>(N_obs,theta_h))</a>
<a class="sourceLine" id="cb339-4" data-line-number="4"><span class="kw">head</span>(sim_data_h)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 4
##   subject  item complexity w_ans
##     &lt;int&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1       1     1     1.86       5
## 2       1     2    -2.20       2
## 3       1     3     0.0404     5
## 4       1     4    -0.625      1
## 5       1     5    -1.28       5
## # … with 1 more row</code></pre>
<p>The appropriate model <code>mpt_5.stan</code> will look like this:</p>
<pre><code>data {
  int&lt;lower = 1&gt; N_obs;
  int&lt;lower = 1,upper = 5&gt; w_ans[N_obs];
  real complexity[N_obs];
  int&lt;lower = 1&gt; N_subject;
  int&lt;lower = 1, upper = N_subject&gt; subject[N_obs];
}
parameters {
  real&lt;lower = 0, upper = 1&gt; t;
  real&lt;lower = 0, upper = 1&gt; c;
  real a_alpha;
  real&lt;lower = 0&gt; sigma_a_alpha_subject;
  vector[N_subject] a_alpha_subject_tilde;
  real f_alpha;
  real f_beta;
}
transformed parameters {
  simplex[5] theta[N_obs];
  for (n in 1:N_obs){
    real a = inv_logit(a_alpha + a_alpha_subject_tilde[subject[n]]);
    real f = inv_logit(f_alpha + complexity[n] * f_beta);
    theta[n, 1] = 1 - a; //Pr_NR
    theta[n, 2] = a * (1 - t) * (1 - f) * (1 - c) + a * t * (1 - f) * (1 - c); //Pr_Neologism
    theta[n, 3] = a * (1 - t) * (1 - f) * c +  a * t * (1 - f) * c;  //Pr_Formal
    theta[n, 4] = a * (1 - t) * f; //Pr_Mixed
    theta[n, 5] = a * t * f; //Pr_Correct
  }
}
model {
  target += beta_lpdf(t | 2, 2);
  target += beta_lpdf(c | 2, 2);
  target += normal_lpdf(f_alpha | 0, 2);
  target += normal_lpdf(f_beta | 0, 2);
  target += normal_lpdf(a_alpha | 0, 2);
  target += normal_lpdf(a_alpha_subject_tilde | 0, sigma_a_alpha_subject);
  target += normal_lpdf(sigma_a_alpha_subject | 0, 1);

  for(n in 1:N_obs)
    target +=  categorical_lpmf(w_ans[n] | theta[n]);
}
generated quantities{
  int&lt;lower = 1, upper = 5&gt; pred_w_ans[N_obs];
  for(n in 1:N_obs)
    pred_w_ans[n] = categorical_rng(theta[n]);
}</code></pre>
<p>Next, fit the model to the simulated data. The data are defined as a list:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1">sim_list_h &lt;-<span class="st">  </span><span class="kw">list</span>(<span class="dt">N_obs =</span> <span class="kw">nrow</span>(sim_data_h),</a>
<a class="sourceLine" id="cb342-2" data-line-number="2">         <span class="dt">w_ans =</span> sim_data_h<span class="op">$</span>w_ans,</a>
<a class="sourceLine" id="cb342-3" data-line-number="3">         <span class="dt">N_subject =</span> <span class="kw">max</span>(sim_data_h<span class="op">$</span>subject),</a>
<a class="sourceLine" id="cb342-4" data-line-number="4">         <span class="dt">subject =</span> sim_data_h<span class="op">$</span>subject,</a>
<a class="sourceLine" id="cb342-5" data-line-number="5">         <span class="dt">complexity =</span> sim_data_h<span class="op">$</span>complexity)</a>
<a class="sourceLine" id="cb342-6" data-line-number="6"></a>
<a class="sourceLine" id="cb342-7" data-line-number="7">fit_h &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mpt_5.stan&#39;</span>, </a>
<a class="sourceLine" id="cb342-8" data-line-number="8">              <span class="dt">data =</span> sim_list_h, </a>
<a class="sourceLine" id="cb342-9" data-line-number="9">              <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">.9</span>))  </a></code></pre></div>
<pre><code>## hash mismatch so recompiling; make sure Stan code ends with a blank line</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>Print out a summary of the posterior; also see figure <a href="multinomial-processing-tree-mpt-models.html#fig:mpt-h">9.5</a></p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">summary</span>(fit_h,<span class="dt">pars=</span><span class="kw">c</span>(<span class="st">&quot;t&quot;</span>, <span class="st">&quot;c&quot;</span>, </a>
<a class="sourceLine" id="cb345-2" data-line-number="2">                     <span class="st">&quot;sigma_a_alpha_subject&quot;</span>,</a>
<a class="sourceLine" id="cb345-3" data-line-number="3">                     <span class="st">&quot;a_alpha&quot;</span>, <span class="st">&quot;f_alpha&quot;</span>,</a>
<a class="sourceLine" id="cb345-4" data-line-number="4">                     <span class="st">&quot;f_beta&quot;</span>))<span class="op">$</span>summary[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">8</span>)],<span class="dv">2</span>)</a></code></pre></div>
<pre><code>##                        mean  2.5%  98%
## t                      0.88  0.83 0.91
## c                      0.08  0.05 0.12
## sigma_a_alpha_subject  1.45  1.05 2.01
## a_alpha                1.24  0.66 1.86
## f_alpha                0.18 -0.01 0.40
## f_beta                -0.05 -0.17 0.06</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1">true_values &lt;-</a>
<a class="sourceLine" id="cb347-2" data-line-number="2"><span class="st">    </span><span class="kw">tibble</span>(<span class="dt">Parameter =</span></a>
<a class="sourceLine" id="cb347-3" data-line-number="3">               <span class="kw">c</span>(<span class="st">&quot;sigma_a_alpha_subject&quot;</span>,<span class="st">&quot;a_alpha&quot;</span>, <span class="st">&quot;t&quot;</span>,<span class="st">&quot;f_alpha&quot;</span>,<span class="st">&quot;f_beta&quot;</span>,<span class="st">&quot;c&quot;</span>),</a>
<a class="sourceLine" id="cb347-4" data-line-number="4">           <span class="dt">value =</span></a>
<a class="sourceLine" id="cb347-5" data-line-number="5">               <span class="kw">c</span>(sigma_a_alpha_subject, <span class="kw">qlogis</span>(a_true),  t_true, f_alpha, f_beta, c_true))</a>
<a class="sourceLine" id="cb347-6" data-line-number="6"><span class="kw">mcmc_hist</span>(<span class="kw">as.array</span>(fit_h),</a>
<a class="sourceLine" id="cb347-7" data-line-number="7">          <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;sigma_a_alpha_subject&quot;</span>,<span class="st">&quot;a_alpha&quot;</span>, <span class="st">&quot;t&quot;</span>,<span class="st">&quot;f_alpha&quot;</span>,<span class="st">&quot;f_beta&quot;</span>,<span class="st">&quot;c&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb347-8" data-line-number="8"><span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">data =</span> true_values, <span class="kw">aes</span>(<span class="dt">xintercept =</span> value)) <span class="op">+</span></a>
<a class="sourceLine" id="cb347-9" data-line-number="9"><span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2</span>))</a></code></pre></div>
<div class="figure"><span id="fig:mpt-h"></span>
<img src="bookdown_files/figure-html/mpt-h-1.svg" alt="Posterior of the hierarchical MPT with true values as vertical lines (model `mpt_5.stan`)." width="672" />
<p class="caption">
FIGURE 9.5: Posterior of the hierarchical MPT with true values as vertical lines (model <code>mpt_5.stan</code>).
</p>
</div>
<p>If everything is correctly defined in the model, we should be able to generate posterior predictive data based on our estimates that looks quite similar to the simulated data; see figure <a href="multinomial-processing-tree-mpt-models.html#fig:aggMPT-h">9.6</a>.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb348-1" data-line-number="1">gen_data &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">extract</span>(fit_h)<span class="op">$</span>pred_w_ans</a>
<a class="sourceLine" id="cb348-2" data-line-number="2"><span class="kw">ppc_bars</span>(sim_list_h<span class="op">$</span>w_ans, gen_data) <span class="op">+</span></a>
<a class="sourceLine" id="cb348-3" data-line-number="3"><span class="st">    </span><span class="kw">ggtitle</span> (<span class="st">&quot;Hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:aggMPT-h"></span>
<img src="bookdown_files/figure-html/aggMPT-h-1.svg" alt="Posterior predictive check for aggregated data in the hierarchical MPT model" width="672" />
<p class="caption">
FIGURE 9.6: Posterior predictive check for aggregated data in the hierarchical MPT model
</p>
</div>
<p>It is also useful to look at the individual subjects’ posteriors in figure <a href="multinomial-processing-tree-mpt-models.html#fig:pMPT-h">9.7</a>.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1"><span class="kw">ppc_bars_grouped</span>(sim_list_h<span class="op">$</span>w_ans, </a>
<a class="sourceLine" id="cb349-2" data-line-number="2">                 gen_data, <span class="dt">group =</span> subject) <span class="op">+</span></a>
<a class="sourceLine" id="cb349-3" data-line-number="3"><span class="st">    </span><span class="kw">ggtitle</span> (<span class="st">&quot;By subject plot for the hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:pMPT-h"></span>
<img src="bookdown_files/figure-html/pMPT-h-1.svg" alt="Individual subjects in the hierarchical MPT model." width="672" />
<p class="caption">
FIGURE 9.7: Individual subjects in the hierarchical MPT model.
</p>
</div>
<p>But what about the first <em>non-hierarchical</em> MPT model (<code>mpt_4.stan</code>)?:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb350-1" data-line-number="1">fit_sh &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&quot;stan_models/mpt_4.stan&quot;</span>, <span class="dt">data =</span> sim_list_h)  </a></code></pre></div>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>The aggregated data looks great (Figure <a href="multinomial-processing-tree-mpt-models.html#fig:aggMPT-nh">9.8</a>).</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb352-1" data-line-number="1">gen_data_sMPT &lt;-<span class="st"> </span>rstan<span class="op">::</span><span class="kw">extract</span>(fit_sh)<span class="op">$</span>pred_w_ans</a>
<a class="sourceLine" id="cb352-2" data-line-number="2"><span class="kw">ppc_bars</span>(sim_list_h<span class="op">$</span>w_ans, gen_data_sMPT) <span class="op">+</span></a>
<a class="sourceLine" id="cb352-3" data-line-number="3"><span class="st">    </span><span class="kw">ggtitle</span> (<span class="st">&quot;Non-hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:aggMPT-nh"></span>
<img src="bookdown_files/figure-html/aggMPT-nh-1.svg" alt="Posterior predictive check for aggregated data in a non-hierarchical MPT model (mpt_4.stan)." width="672" />
<p class="caption">
FIGURE 9.8: Posterior predictive check for aggregated data in a non-hierarchical MPT model (mpt_4.stan).
</p>
</div>
<p>However, the fit to individual subjects looks less good (Figure <a href="multinomial-processing-tree-mpt-models.html#fig:pMPT">9.9</a>).</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1"><span class="kw">ppc_bars_grouped</span>(sim_list_h<span class="op">$</span>w_ans, gen_data_sMPT, <span class="dt">group =</span> subject) <span class="op">+</span></a>
<a class="sourceLine" id="cb353-2" data-line-number="2"><span class="st">    </span><span class="kw">ggtitle</span> (<span class="st">&quot;By subject plot for the non-hierarchical model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:pMPT"></span>
<img src="bookdown_files/figure-html/pMPT-1.svg" alt="Individual subjects in the non-hierarchical MPT model (mpt_4.stan)." width="672" />
<p class="caption">
FIGURE 9.9: Individual subjects in the non-hierarchical MPT model (mpt_4.stan).
</p>
</div>
<p>The hierachical does a better job of modeling individual-level variability.</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BatchelderRiefer1999">
<p>Batchelder, William H, and David M Riefer. 1999. “Theoretical and Empirical Review of Multinomial Process Tree Modeling.” <em>Psychonomic Bulletin &amp; Review</em> 6 (1). Springer: 57–86.</p>
</div>
<div id="ref-WalkerEtAl2018">
<p>Walker, Grant M, Gregory Hickok, and Julius Fridriksson. 2018. “A Cognitive Psychometric Model for Assessment of Picture Naming Abilities in Aphasia.” <em>Psychological Assessment</em>. American Psychological Association.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>Notice below that because the equations depend on <code>f</code> and <code>f</code> is a vector now, the outcome is automatically a vector. But this is not the case for <code>theta_NR_v</code>, and thus we need to repeat the value.<a href="multinomial-processing-tree-mpt-models.html#fnref16" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-multiple-categorical-responses.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-readings.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/25-MPT.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
