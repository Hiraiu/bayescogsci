<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.2 A hierarchical log-normal model: The Stroop effect | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="5.2 A hierarchical log-normal model: The Stroop effect | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.2 A hierarchical log-normal model: The Stroop effect | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-07-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="a-hierarchical-normal-model-the-n400-effect.html"/>
<link rel="next" href="summary-2.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">

 /* Uncomment this and comment the next one to show the solutions */

 /* $(document).ready(function() {
  *     $folds = $(".solution");
  *     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
  *     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
  *     $(".solution-blck").toggle();  // fold all blocks
  *     $(".solution-btn").on("click", function() {  // add onClick event
  *         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
  *         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  *     })
  * }); */

 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\"></button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         /* $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution"  */
         /* $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself. */
     })
 });

</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="3.8.1" data-path="ex-compbda.html"><a href="ex-compbda.html#a-simple-linear-model-exercises-section-refsecsimplenormal"><i class="fa fa-check"></i><b>3.8.1</b> A simple linear model exercises (Section @ref(sec:simplenormal))</a></li>
<li class="chapter" data-level="3.8.2" data-path="ex-compbda.html"><a href="ex-compbda.html#revisiting-the-button-pressing-example-with-different-priors-exercises-section-refsecrevisit"><i class="fa fa-check"></i><b>3.8.2</b> Revisiting the button-pressing example with different priors exercises (Section @ref(sec:revisit))</a></li>
<li class="chapter" data-level="3.8.3" data-path="ex-compbda.html"><a href="ex-compbda.html#posterior-predictive-distribution-and-log-normal-model-exercises-section-refsecppd"><i class="fa fa-check"></i><b>3.8.3</b> Posterior predictive distribution and log-normal model exercises (Section @ref(sec:ppd))</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#how-to-communicate-the-results-2"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a><ul>
<li class="chapter" data-level="4.6.1" data-path="exercises-2.html"><a href="exercises-2.html#a-first-linear-regression-exercises-section-refsecpupil"><i class="fa fa-check"></i><b>4.6.1</b> A first linear regression exercises (Section @ref(sec:pupil))</a></li>
<li class="chapter" data-level="4.6.2" data-path="exercises-2.html"><a href="exercises-2.html#log-normal-model-exercises-section-refsectrial"><i class="fa fa-check"></i><b>4.6.2</b> Log-normal model exercises (Section @ref(sec:trial))</a></li>
<li class="chapter" data-level="4.6.3" data-path="exercises-2.html"><a href="exercises-2.html#logistic-regression-exercises-section-refseclogistic"><i class="fa fa-check"></i><b>4.6.3</b> Logistic regression exercises (section @ref(sec:logistic))</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exercises-3.html"><a href="exercises-3.html#ex:hierarchical-logn"><i class="fa fa-check"></i><b>5.5.1</b> Hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding, interactions, etc</a></li>
<li class="chapter" data-level="7" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>7</b> Model comparison using Bayes factors</a><ul>
<li class="chapter" data-level="7.1" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>7.1</b> Summary</a></li>
<li class="chapter" data-level="7.2" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.2</b> Further reading</a></li>
<li class="chapter" data-level="7.3" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>7.3</b> Exercises</a><ul>
<li class="chapter" data-level="7.3.1" data-path="exercises-4.html"><a href="exercises-4.html#ex:bf-logn"><i class="fa fa-check"></i><b>7.3.1</b> Bayes factor for a hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>8</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="8.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>8.2</b> Meta-analysis</a><ul>
<li class="chapter" data-level="8.2.1" data-path="meta-analysis.html"><a href="meta-analysis.html#using-brms"><i class="fa fa-check"></i><b>8.2.1</b> Using brms</a></li>
<li class="chapter" data-level="8.2.2" data-path="meta-analysis.html"><a href="meta-analysis.html#using-stan"><i class="fa fa-check"></i><b>8.2.2</b> Using Stan</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>8.3</b> Measurement-error models</a><ul>
<li class="chapter" data-level="8.3.1" data-path="measurement-error-models.html"><a href="measurement-error-models.html#using-brms-1"><i class="fa fa-check"></i><b>8.3.1</b> Using brms</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>8.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="cognitive-modeling-using-multinomial-processing-trees.html"><a href="cognitive-modeling-using-multinomial-processing-trees.html"><i class="fa fa-check"></i><b>9</b> Cognitive Modeling using multinomial processing trees</a><ul>
<li class="chapter" data-level="9.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>9.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="9.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>9.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>9.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>9.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="9.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>9.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="further-readings.html"><a href="further-readings.html"><i class="fa fa-check"></i><b>9.3</b> Further readings:</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>10</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:stroop" class="section level2">
<h2><span class="header-section-number">5.2</span> A hierarchical log-normal model: The Stroop effect</h2>
<p>We will illustrate the issues that arise with a log-normal likelihood in a hierarchical model using data from a Stroop task <span class="citation">(Stroop <a href="#ref-stroop1935studies">1935</a>; for a review, see MacLeod <a href="#ref-macleod1991half">1991</a>)</span>. We will analyze a subset of the data of 3337 participants that undertook one variant of the Stroop task as part of the battery of tasks run in <span class="citation">Ebersole et al. (<a href="#ref-ManyLabs3">2016</a>)</span>.</p>
<p>For this variant of the Stroop task, participants were presented with one word at the center of the screen, which was either “red”, “blue”, and “green” (<em>word</em>) written in either red, blue, or green font (<em>color</em>). In one third of the trials the <em>word</em> matched the <em>color</em> of the text (“congruent” condition) and in the rest of the trials it did not match (“incongruent” condition). Participants were instructed to only pay attention to the color, and press <code>1</code> if the color of the word was red, <code>2</code> if it was blue, and <code>3</code> if it was green. The complete procedure can be found in <a href="https://osf.io/5ykuj/" class="uri">https://osf.io/5ykuj/</a> and the dataset can be found in <a href="https://osf.io/n8xa7/" class="uri">https://osf.io/n8xa7/</a>. The Stroop effect, that is, the difficulty in identifying the color when it mismatches the word in the incongruent condition (e.g., green in color blue) in comparison to a baseline condition, here, the congruent condition (e.g., green in color green) is extremely robust across variations of the task.</p>
<p>While this task yields two measures: the accuracy of the decision made and the time it took to respond. For the Stroop task, accuracy is usually almost at ceiling level, and to simplify the model, we will ignore it. <span class="citation">(See Nicenboim <a href="#ref-Nicenboim2018StanCon">2018</a> for a cognitive model that incorporates accuracy and reaction times in the same model to analyze the same Stroop data)</span>.</p>
<div id="a-correlated-varying-intercept-varying-slopes-log-normal-model" class="section level3">
<h3><span class="header-section-number">5.2.1</span> A correlated varying intercept varying slopes log-normal model</h3>
<p>If our theory only focuses on the difference between the reaction times for the “congruent” vs. “incongruent” condition, we can ignore the actual color presented and what was written and focus in whether there was a match or not between the two. We will need a predictor that indicates whether each trial is congruent or incongruent (<code>c_cond</code>). For simplicity, we will also assume that all participants share the same variance (as we saw in section <a href="a-hierarchical-normal-model-the-n400-effect.html#sec:distrmodel">5.1.6</a> changing this assumption leads to distributional regression models). This means that we are going to fit the data with the following likelihood (identical to the likelihood that we fit in section <a href="a-hierarchical-normal-model-the-n400-effect.html#sec:mcvivs">5.1.4</a> except that here the location and scale are embedded in a log-normal likelihood rather than a normal one). This likelihood indicates that we are dealing with a hierarchical model with by-subjects varying intercept and varying slopes model:</p>
<p><span class="math display">\[\begin{equation}
  rt_n \sim LogNormal(\alpha + u_{i[n],0}  + c\_cond_n \cdot  (\beta + u_{i[n],1}), \sigma)
\end{equation}\]</span></p>
<p>We will discuss how to deal with the coding of conditions, such as <code>c_cond</code>, with more details in chapter <a href="ch-contr.html#ch:contr">6</a> <span class="citation">(and see also Schad et al. <a href="#ref-schadHowCapitalizePriori2020">2020</a> for the mathematics underlying different kinds of contrasts)</span>, but for now it will suffice to say that we assign a <code>1</code> to <code>c_cond</code> for the “incongruent” condition and a <code>-1</code> for the “congruent” one (i.e., a sum coding contrast). This will mean that if <span class="math inline">\(\beta\)</span> turns out to be positive, the incongruent condition will be slower than the congruent one. This is because on average the location of the log-normal likelihood for each condition will be as follows. (We could have chosen to do the opposite assignments, and get to the opposite conclusion without any change in the underlying model).</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
  \mu_{incongruent} &amp;= \alpha + 1 \cdot  \beta \\
  \mu_{congruent} &amp;= \alpha + -1 \cdot  \beta
  \end{aligned}
\end{equation}\]</span></p>
<p>As always, we need priors for all the parameters in our model. For the population-level parameters (or fixed effects), we use the same priors as we did when we were fitting a regression with a log-normal likelihood in section <a href="sec-ppd.html#sec:lognormal">3.5.3</a>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
   \alpha &amp; \sim Normal(6, 1.5) \\
   \beta  &amp; \sim Normal(0, .01) \\
    \sigma  &amp;\sim Normal_+(0, 1)
 \end{aligned}
 \end{equation}\]</span></p>
<p>Here, <span class="math inline">\(\beta\)</span> represents the effect of the experimental manipulation in log-scale: how much we increase or decrease the location of the log-normal in comparison to the intercept, <span class="math inline">\(\alpha\)</span>. For this model, <span class="math inline">\(\beta\)</span> will probably be longer than for the model that examined the difference in pressing the spacebar for two consecutive trials in section <a href="sec-ppd.html#sec:lognormal">3.5.3</a>. We might need to examine the prior for <span class="math inline">\(\beta\)</span> with predictive distributions, but we will delay this for now.</p>
<p>In contrast to our previous models, the intercept, <span class="math inline">\(\alpha\)</span>, is not the grand mean of the location because the conditions were not balanced in the experiment (one third of the conditions were congruent and two thirds incongruent). The intercept could be interpreted here as the time (in log-scale) it takes to answer if we cancel out the experimental manipulation.</p>
<p>We focus now on the priors for the group-level parameters (or random effects). If we assume a possible correlation between by-participant intercept and slope, our model will have the following structure, which requires us to assign priors to <span class="math inline">\(\Sigma_u\)</span>.</p>
<p><span class="math display">\[\begin{equation}
 \begin{aligned}
    {\begin{pmatrix}
    u_{i,0} \\
    u_{i,1}
    \end{pmatrix}}
   &amp;\sim {\mathcal {N}}
    \left(
   {\begin{pmatrix} 
    0\\
    0
   \end{pmatrix}}
 ,\boldsymbol{\Sigma_u} \right) 
 \end{aligned}
 \end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
 \boldsymbol{\Sigma_u} &amp; = 
{\begin{pmatrix} 
\tau_{u_0}^2 &amp; \rho_u \tau_{u_0} \tau_{u_1} \\ 
\rho_u \tau_{u_0} \tau_{u_1} &amp; \tau_{u_1}^2
\end{pmatrix}}
\end{aligned}
\end{equation}\]</span></p>
<p>In practice this means that we need priors for the by-participant variances and correlations. For the variance components (which, confusingly enough, are actually standard deviations in our prior specification), we will set a similar prior than for <span class="math inline">\(\sigma\)</span>. We don’t expect the by-group adjustments to the intercept and slope to have more variance than the overall observations, so this prior will be quite conservative (keeping a big deal of uncertainty). We assign the same prior for the correlations as we did in <a href="a-hierarchical-normal-model-the-n400-effect.html#sec:sih">5.1.5</a>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\tau_{u_0} &amp;\sim Normal_+(0,1)\\
\tau_{u_1} &amp;\sim Normal_+(0,1)\\
\rho_u &amp;\sim LKJcorr(2) 
\end{aligned}
\end{equation}\]</span></p>
<p>We are ready to fit a model now. To speed up computation, we subset 50 participants of the original dataset. (In a real setting, we would obviously not subset the participants.)
We restrict ourselves to the correct trials only and we add a <code>c_cond</code> predictor with the sum coded variable.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1">df_stroop_data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/StroopCleanSet.csv&quot;</span>)  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb230-2" data-line-number="2"><span class="st">  </span><span class="co"># rename unintuitive column names:</span></a>
<a class="sourceLine" id="cb230-3" data-line-number="3"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">correct=</span> trial_error, </a>
<a class="sourceLine" id="cb230-4" data-line-number="4">         <span class="dt">RT =</span> trial_latency,</a>
<a class="sourceLine" id="cb230-5" data-line-number="5">         <span class="dt">condition =</span> congruent) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb230-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">subject =</span> session_id <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb230-7" data-line-number="7"><span class="st">           </span><span class="kw">as.factor</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb230-8" data-line-number="8"><span class="st">           </span><span class="kw">as.numeric</span>(),</a>
<a class="sourceLine" id="cb230-9" data-line-number="9">         <span class="dt">c_cond =</span> <span class="kw">if_else</span>(condition <span class="op">==</span><span class="st"> &quot;Incongruent&quot;</span>, <span class="dv">1</span>, <span class="dv">-1</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb230-10" data-line-number="10"><span class="st">  </span><span class="kw">filter</span>(correct <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, subject <span class="op">&lt;=</span><span class="st"> </span><span class="dv">50</span>)</a></code></pre></div>
<p>We fit the model with 4000 iterations rather than with the default of 2000 iterations by chain. This is because if we run the model with the default number of iterations, the following warning that appears: <code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and media Running the chains for more iterations may help. See http://mc-stan.org/misc/warnings.html#bulk-ess</code>.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb231-1" data-line-number="1">fit_stroop_data &lt;-<span class="st"> </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subject),</a>
<a class="sourceLine" id="cb231-2" data-line-number="2">                   <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb231-3" data-line-number="3">                  <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb231-4" data-line-number="4">                      <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb231-5" data-line-number="5">                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="fl">.01</span>), <span class="dt">class =</span> b),</a>
<a class="sourceLine" id="cb231-6" data-line-number="6">                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb231-7" data-line-number="7">                        <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb231-8" data-line-number="8">                        <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</a>
<a class="sourceLine" id="cb231-9" data-line-number="9">                  <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb231-10" data-line-number="10">                  <span class="dt">data =</span> df_stroop_data)</a></code></pre></div>
<p>We will focus on <span class="math inline">\(\beta\)</span> (but you can verify that there is nothing surprising in <code>fit_stroop_data</code> ):</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1"><span class="kw">posterior_summary</span>(fit_stroop_data, <span class="dt">pars =</span> <span class="st">&quot;b_c_cond&quot;</span>)</a></code></pre></div>
<pre><code>##          Estimate Est.Error  Q2.5 Q97.5
## b_c_cond    0.027    0.0056 0.016 0.038</code></pre>
<p>After seeing the posterior for <span class="math inline">\(\beta\)</span>, we suspect that the prior might have been too restrictive. If we overlay density plots for prior and posterior distributions this is more evident:</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" data-line-number="1">sample_b_post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_stroop_data)<span class="op">$</span>b_c_cond</a>
<a class="sourceLine" id="cb234-2" data-line-number="2"><span class="co"># We generate samples from the prior as well:</span></a>
<a class="sourceLine" id="cb234-3" data-line-number="3">N &lt;-<span class="st"> </span><span class="kw">length</span>(sample_b_post)</a>
<a class="sourceLine" id="cb234-4" data-line-number="4">sample_b_prior &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">.01</span>)</a>
<a class="sourceLine" id="cb234-5" data-line-number="5">samples &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">sample =</span> <span class="kw">c</span>(sample_b_post, sample_b_prior),</a>
<a class="sourceLine" id="cb234-6" data-line-number="6">                  <span class="dt">distribution =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;posterior&quot;</span>,N), <span class="kw">rep</span>(<span class="st">&quot;prior&quot;</span>, N)))</a>
<a class="sourceLine" id="cb234-7" data-line-number="7"><span class="kw">ggplot</span>(samples, <span class="kw">aes</span>(<span class="dt">x =</span> sample, <span class="dt">fill =</span> distribution)) <span class="op">+</span></a>
<a class="sourceLine" id="cb234-8" data-line-number="8"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha =</span> <span class="fl">.5</span>)</a></code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-146-1.svg" width="672" /></p>
<div id="a-sensitivity-analysis" class="section level4">
<h4><span class="header-section-number">5.2.1.1</span> A sensitivity analysis</h4>
<p>We see that the posterior distribution covers values much larger than the ones that are in the bulk of the prior distribution. Is our posterior overly biased by the prior distribution? We can investigate this with a sensitivity analysis. We will examine what happens for the following priors for <span class="math inline">\(\beta\)</span>:</p>
<ul>
<li><span class="math inline">\(\beta \sim Normal(0,.05)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0,.1)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0,1)\)</span></li>
<li><span class="math inline">\(\beta \sim Normal(0,2)\)</span></li>
</ul>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb235-1" data-line-number="1">sds &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>,<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb235-2" data-line-number="2">df_beta_stroop &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds, <span class="cf">function</span>(sd){</a>
<a class="sourceLine" id="cb235-3" data-line-number="3">  priorb &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;normal(0, &quot;</span>,sd ,<span class="st">&quot;)&quot;</span>)</a>
<a class="sourceLine" id="cb235-4" data-line-number="4">  fit &lt;-<span class="st">  </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subject),</a>
<a class="sourceLine" id="cb235-5" data-line-number="5">              <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb235-6" data-line-number="6">              <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb235-7" data-line-number="7">                <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb235-8" data-line-number="8">                  <span class="kw">prior_string</span>(priorb, <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb235-9" data-line-number="9">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb235-10" data-line-number="10">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb235-11" data-line-number="11">                  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</a>
<a class="sourceLine" id="cb235-12" data-line-number="12">              <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb235-13" data-line-number="13">              <span class="dt">data =</span> df_stroop_data)</a>
<a class="sourceLine" id="cb235-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb235-15" data-line-number="15">  <span class="kw">posterior_summary</span>(fit, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b_c_cond&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb235-16" data-line-number="16"><span class="st">    </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb235-17" data-line-number="17"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">prior =</span> priorb)</a>
<a class="sourceLine" id="cb235-18" data-line-number="18">})</a></code></pre></div>
<p>We can summarize the estimates of <span class="math inline">\(\beta\)</span> given different priors in the following way:</p>
<table>
<thead>
<tr class="header">
<th align="right">Estimate</th>
<th align="right">Q2.5</th>
<th align="right">Q97.5</th>
<th align="left">prior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="left">normal(0, 0.001)</td>
</tr>
<tr class="even">
<td align="right">0.03</td>
<td align="right">0.02</td>
<td align="right">0.04</td>
<td align="left">normal(0, 0.01)</td>
</tr>
<tr class="odd">
<td align="right">0.04</td>
<td align="right">0.02</td>
<td align="right">0.05</td>
<td align="left">normal(0, 0.05)</td>
</tr>
<tr class="even">
<td align="right">0.04</td>
<td align="right">0.03</td>
<td align="right">0.05</td>
<td align="left">normal(0, 0.1)</td>
</tr>
<tr class="odd">
<td align="right">0.04</td>
<td align="right">0.03</td>
<td align="right">0.05</td>
<td align="left">normal(0, 1)</td>
</tr>
<tr class="even">
<td align="right">0.04</td>
<td align="right">0.03</td>
<td align="right">0.05</td>
<td align="left">normal(0, 2)</td>
</tr>
</tbody>
</table>
<p>It might be easier to see how much the posterior difference between conditions changes depending on the prior. In order to answer this question, we need to remember that the median difference between conditions can be calculated as the difference between the exponentiation of each condition:</p>
<p><span class="math display" id="eq:medianrt">\[\begin{equation}
\begin{aligned}
MedianRT_{diff} &amp;= MedianRT_{incongruent} - MedianRT_{congruent}\\
MedianRT_{diff} &amp;= exp(\alpha + \beta) - \exp(\alpha - \beta)
\end{aligned}
\tag{5.2}
\end{equation}\]</span></p>
<p>This means that we need to re-run the models to extract samples from the intercept.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" data-line-number="1">sds &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>,<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb236-2" data-line-number="2">df_diffs_stroop &lt;-<span class="st"> </span><span class="kw">map_dfr</span>(sds, <span class="cf">function</span>(sd){</a>
<a class="sourceLine" id="cb236-3" data-line-number="3">  priorb &lt;-<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;normal(0, &quot;</span>,sd ,<span class="st">&quot;)&quot;</span>)</a>
<a class="sourceLine" id="cb236-4" data-line-number="4">  fit &lt;-<span class="st">  </span><span class="kw">brm</span>(RT <span class="op">~</span><span class="st"> </span>c_cond <span class="op">+</span><span class="st"> </span>(c_cond <span class="op">|</span><span class="st"> </span>subject),</a>
<a class="sourceLine" id="cb236-5" data-line-number="5">              <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb236-6" data-line-number="6">              <span class="dt">prior =</span></a>
<a class="sourceLine" id="cb236-7" data-line-number="7">                <span class="kw">c</span>(<span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb236-8" data-line-number="8">                  <span class="kw">prior_string</span>(priorb, <span class="dt">class =</span> <span class="st">&quot;b&quot;</span>),</a>
<a class="sourceLine" id="cb236-9" data-line-number="9">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma),</a>
<a class="sourceLine" id="cb236-10" data-line-number="10">                  <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sd),</a>
<a class="sourceLine" id="cb236-11" data-line-number="11">                  <span class="kw">prior</span>(<span class="kw">lkj</span>(<span class="dv">2</span>), <span class="dt">class =</span> cor)),</a>
<a class="sourceLine" id="cb236-12" data-line-number="12">              <span class="dt">iter =</span> <span class="dv">4000</span>,</a>
<a class="sourceLine" id="cb236-13" data-line-number="13">              <span class="dt">data =</span> df_stroop_data)</a>
<a class="sourceLine" id="cb236-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb236-15" data-line-number="15">  sample_a_post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit)<span class="op">$</span>b_Intercept</a>
<a class="sourceLine" id="cb236-16" data-line-number="16">  sample_b_post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit)<span class="op">$</span>b_c_cond</a>
<a class="sourceLine" id="cb236-17" data-line-number="17">  RT_diff =<span class="st"> </span><span class="kw">exp</span>(sample_a_post <span class="op">+</span><span class="st"> </span>sample_b_post) <span class="op">-</span></a>
<a class="sourceLine" id="cb236-18" data-line-number="18"><span class="st">    </span><span class="kw">exp</span>(sample_a_post <span class="op">-</span><span class="st"> </span>sample_b_post)</a>
<a class="sourceLine" id="cb236-19" data-line-number="19">  <span class="kw">tibble</span>(<span class="st">`</span><span class="dt">mean diff (ms)</span><span class="st">`</span> =<span class="st"> </span><span class="kw">mean</span>(RT_diff),</a>
<a class="sourceLine" id="cb236-20" data-line-number="20">         <span class="dt">Q2.5 =</span> <span class="kw">quantile</span>(RT_diff, <span class="fl">.025</span>),</a>
<a class="sourceLine" id="cb236-21" data-line-number="21">         <span class="dt">Q97.5 =</span> <span class="kw">quantile</span>(RT_diff, <span class="fl">.975</span>),</a>
<a class="sourceLine" id="cb236-22" data-line-number="22">         <span class="dt">prior =</span> priorb)</a>
<a class="sourceLine" id="cb236-23" data-line-number="23">})</a></code></pre></div>
<p>We get the <em>posterior distributions</em> of the median difference between conditions for different models by using equation <a href="sec-stroop.html#eq:medianrt">(5.2)</a>. We calculate the median difference rather than the mean difference because the mean depends on the parameter <span class="math inline">\(\sigma\)</span>, but the median doesn’t; see <a href="sec-ppd.html#sec:lognormal">3.5.3</a>. In the following table, we use <em>means</em>–this is orthogonal to our use of median before, we could have summarized the distribution with its median–, and 95% quantiles to summarize these distributions.</p>
<table>
<thead>
<tr class="header">
<th align="right">mean diff (ms)</th>
<th align="right">Q2.5</th>
<th align="right">Q97.5</th>
<th align="left">prior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.67</td>
<td align="right">-1.5</td>
<td align="right">2.8</td>
<td align="left">normal(0, 0.001)</td>
</tr>
<tr class="even">
<td align="right">30.08</td>
<td align="right">17.7</td>
<td align="right">42.0</td>
<td align="left">normal(0, 0.01)</td>
</tr>
<tr class="odd">
<td align="right">41.41</td>
<td align="right">27.7</td>
<td align="right">55.0</td>
<td align="left">normal(0, 0.05)</td>
</tr>
<tr class="even">
<td align="right">41.93</td>
<td align="right">28.3</td>
<td align="right">56.3</td>
<td align="left">normal(0, 0.1)</td>
</tr>
<tr class="odd">
<td align="right">42.08</td>
<td align="right">28.0</td>
<td align="right">56.1</td>
<td align="left">normal(0, 1)</td>
</tr>
<tr class="even">
<td align="right">42.14</td>
<td align="right">28.2</td>
<td align="right">55.8</td>
<td align="left">normal(0, 2)</td>
</tr>
</tbody>
</table>
<p>This shows us that the posterior changes substantially when we include wider priors in our model. It seems that the posterior is relatively unaffected for priors with a standard deviation larger than .05, but if we assume a priori that the effect of the manipulation <em>must</em> be small, we will end up finding that. When we include less information about the possible effect sizes with a more diffuse prior–we assume that they can be small but also large–, we allow the data to influence more the posterior. (We can safely ignore a difference of a couple milliseconds in <span class="math inline">\(\approx 40\)</span> milliseconds.)</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-ManyLabs3">
<p>Ebersole, Charles R., Olivia E. Atherton, Aimee L. Belanger, Hayley M. Skulborstad, Jill M. Allen, Jonathan B. Banks, Erica Baranski, et al. 2016. “Many Labs 3: Evaluating Participant Pool Quality Across the Academic Semester via Replication.” <em>Journal of Experimental Social Psychology</em> 67: 68–82. <a href="https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012">https://doi.org/https://doi.org/10.1016/j.jesp.2015.10.012</a>.</p>
</div>
<div id="ref-macleod1991half">
<p>MacLeod, Colin M. 1991. “Half a Century of Research on the Stroop Effect: An Integrative Review.” <em>Psychological Bulletin</em> 109 (2). American Psychological Association: 163.</p>
</div>
<div id="ref-Nicenboim2018StanCon">
<p>Nicenboim, Bruno. 2018. “The Implementation of a Model of Choice: The (Truncated) Linear Ballistic Accumulator.” In <em>StanCon</em>. Aalto University, Helsinki, Finland. <a href="https://doi.org/10.5281/zenodo.1465990">https://doi.org/10.5281/zenodo.1465990</a>.</p>
</div>
<div id="ref-schadHowCapitalizePriori2020">
<p>Schad, Daniel J., Shravan Vasishth, Sven Hohenstein, and Reinhold Kliegl. 2020. “How to Capitalize on a Priori Contrasts in Linear (Mixed) Models: A Tutorial.” <em>Journal of Memory and Language</em> 110 (February): 104038. <a href="https://doi.org/10/gf9tjp">https://doi.org/10/gf9tjp</a>.</p>
</div>
<div id="ref-stroop1935studies">
<p>Stroop, J Ridley. 1935. “Studies of Interference in Serial Verbal Reactions.” <em>Journal of Experimental Psychology</em> 18 (6). Psychological Review Company: 643.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-hierarchical-normal-model-the-n400-effect.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/05-hierarchical.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
