<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.5 Posterior predictive distribution | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="3.5 Posterior predictive distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.5 Posterior predictive distribution | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-07-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-revisit.html"/>
<link rel="next" href="summary.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">

 /* Uncomment this and comment the next one to show the solutions */

 /* $(document).ready(function() {
  *     $folds = $(".solution");
  *     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
  *     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
  *     $(".solution-blck").toggle();  // fold all blocks
  *     $(".solution-btn").on("click", function() {  // add onClick event
  *         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
  *         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  *     })
  * }); */

 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\"></button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         /* $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution"  */
         /* $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself. */
     })
 });

</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.3</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.3.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.3.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.4</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.4.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><a href="an-important-concept-the-marginal-likelihood-integrating-out-a-parameter.html"><i class="fa fa-check"></i><b>1.5</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.6" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.7" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.7</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.8" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.8</b> Further reading</a></li>
<li class="chapter" data-level="1.9" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.9</b> Exercises</a><ul>
<li class="chapter" data-level="1.9.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.9.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.9.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.9.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.9.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.9.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.9.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.9.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.9.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.9.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes’ rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes’ rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes’ rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Regression models</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="deriving-the-posterior-through-sampling.html"><a href="deriving-the-posterior-through-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using ‘Stan’: brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a><ul>
<li class="chapter" data-level="3.8.1" data-path="ex-compbda.html"><a href="ex-compbda.html#a-simple-linear-model-exercises-section-refsecsimplenormal"><i class="fa fa-check"></i><b>3.8.1</b> A simple linear model exercises (Section @ref(sec:simplenormal))</a></li>
<li class="chapter" data-level="3.8.2" data-path="ex-compbda.html"><a href="ex-compbda.html#revisiting-the-button-pressing-example-with-different-priors-exercises-section-refsecrevisit"><i class="fa fa-check"></i><b>3.8.2</b> Revisiting the button-pressing example with different priors exercises (Section @ref(sec:revisit))</a></li>
<li class="chapter" data-level="3.8.3" data-path="ex-compbda.html"><a href="ex-compbda.html#posterior-predictive-distribution-and-log-normal-model-exercises-section-refsecppd"><i class="fa fa-check"></i><b>3.8.3</b> Posterior predictive distribution and log-normal model exercises (Section @ref(sec:ppd))</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#how-to-communicate-the-results-2"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a><ul>
<li class="chapter" data-level="4.6.1" data-path="exercises-2.html"><a href="exercises-2.html#a-first-linear-regression-exercises-section-refsecpupil"><i class="fa fa-check"></i><b>4.6.1</b> A first linear regression exercises (Section @ref(sec:pupil))</a></li>
<li class="chapter" data-level="4.6.2" data-path="exercises-2.html"><a href="exercises-2.html#log-normal-model-exercises-section-refsectrial"><i class="fa fa-check"></i><b>4.6.2</b> Log-normal model exercises (Section @ref(sec:trial))</a></li>
<li class="chapter" data-level="4.6.3" data-path="exercises-2.html"><a href="exercises-2.html#logistic-regression-exercises-section-refseclogistic"><i class="fa fa-check"></i><b>4.6.3</b> Logistic regression exercises (section @ref(sec:logistic))</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="a-hierarchical-normal-model-the-n400-effect.html"><a href="a-hierarchical-normal-model-the-n400-effect.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models–Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exercises-3.html"><a href="exercises-3.html#ex:hierarchical-logn"><i class="fa fa-check"></i><b>5.5.1</b> Hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding, interactions, etc</a></li>
<li class="chapter" data-level="7" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>7</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-1.html"><a href="introduction-1.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>7.2</b> Meta-analysis</a><ul>
<li class="chapter" data-level="7.2.1" data-path="meta-analysis.html"><a href="meta-analysis.html#using-brms"><i class="fa fa-check"></i><b>7.2.1</b> Using brms</a></li>
<li class="chapter" data-level="7.2.2" data-path="meta-analysis.html"><a href="meta-analysis.html#using-stan"><i class="fa fa-check"></i><b>7.2.2</b> Using Stan</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>7.3</b> Measurement-error models</a><ul>
<li class="chapter" data-level="7.3.1" data-path="measurement-error-models.html"><a href="measurement-error-models.html#using-brms-1"><i class="fa fa-check"></i><b>7.3.1</b> Using brms</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.4</b> Further reading</a></li>
</ul></li>
<li class="part"><span><b>III Model comparison</b></span></li>
<li class="chapter" data-level="8" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>8</b> Model comparison</a></li>
<li class="chapter" data-level="9" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>9</b> Bayes factors</a><ul>
<li class="chapter" data-level="9.1" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>9.1</b> Summary</a></li>
<li class="chapter" data-level="9.2" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>9.2</b> Further reading</a></li>
<li class="chapter" data-level="9.3" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>9.3</b> Exercises</a><ul>
<li class="chapter" data-level="9.3.1" data-path="exercises-4.html"><a href="exercises-4.html#ex:bf-logn"><i class="fa fa-check"></i><b>9.3.1</b> Bayes factor for a hierarchical model with a lognormal likelihood.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>10</b> Cross validation</a><ul>
<li class="chapter" data-level="10.1" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>10.1</b> Summary</a></li>
<li class="chapter" data-level="10.2" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>10.2</b> Further reading</a></li>
<li class="chapter" data-level="10.3" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>10.3</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Advanced models with Stan</b></span></li>
<li class="chapter" data-level="11" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>11</b> Introduction to Stan probabilistic language</a></li>
<li class="chapter" data-level="12" data-path="cognitive-modeling-using-multinomial-processing-trees.html"><a href="cognitive-modeling-using-multinomial-processing-trees.html"><i class="fa fa-check"></i><b>12</b> Cognitive Modeling using multinomial processing trees</a><ul>
<li class="chapter" data-level="12.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>12.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="12.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>12.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="12.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>12.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>12.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>12.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="further-readings.html"><a href="further-readings.html"><i class="fa fa-check"></i><b>12.3</b> Further readings:</a></li>
</ul></li>
<li class="part"><span><b>V Appendix</b></span></li>
<li class="chapter" data-level="13" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>13</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:ppd" class="section level2">
<h2><span class="header-section-number">3.5</span> Posterior predictive distribution</h2>
<p>The prior predictive distribution is a collection of datasets generated from the model (the likelihood and the priors). After we have seen the data and obtained the posterior distributions of the parameters, we can now use the <em>posterior distributions</em> to generate future data from the model. In other words, given the posterior distributions of the parameters of the model, the posterior predictive distribution shows how future data might look like.</p>
<p>Once we have the posterior distribution <span class="math inline">\(p(\Theta\mid y)\)</span>, we can derive the predictions based on this distribution:</p>
<p><span class="math display">\[\begin{equation}
p(D_{pred}\mid y ) = \int_\Theta p(D_{pred}, \Theta\mid y)\, d\Theta= \int_\Theta 
p(D_{pred}\mid \Theta,y)p(\Theta\mid y)\, d\Theta
\end{equation}\]</span></p>
<p>Assuming that past and future observations are conditionally independent given <span class="math inline">\(\Theta\)</span>, i.e., <span class="math inline">\(p(D_{pred}\mid \Theta,y)= p(D_{pred}\mid \Theta)\)</span>, we can write:</p>
<p><span class="math display">\[\begin{equation}
p(D_{pred}\mid y )=\int_\Theta p(D_{pred}\mid \Theta) p(\Theta\mid y)\, d\Theta
\end{equation}\]</span></p>
<p>Note that we are conditioning <span class="math inline">\(D_{pred}\)</span> only on <span class="math inline">\(y\)</span>, we do not condition on what we don’t know (<span class="math inline">\(\Theta\)</span>); we integrate out the unknown parameters. This posterior predictive distribution is different from the frequentist approach, which gives only a predictive distribution of <span class="math inline">\(D_{pred}\)</span> given our maximum likelihood estimate of <span class="math inline">\(\theta\)</span> (a point value). As with the prior predictive distribution, we can avoid performing the integration explicitly by generating samples from the posterior predictive distribution. We can use the same function that we created before, <code>normal_predictive_distribution_fast</code>, with the only difference in that instead of sampling <code>mu</code> and <code>sigma</code> from the priors, we use samples from the posterior.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1">N_obs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_noreading_data)</a>
<a class="sourceLine" id="cb99-2" data-line-number="2">mu_samples &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_press)<span class="op">$</span>b_Intercept</a>
<a class="sourceLine" id="cb99-3" data-line-number="3">sigma_samples &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit_press)<span class="op">$</span>sigma</a>
<a class="sourceLine" id="cb99-4" data-line-number="4"><span class="kw">normal_predictive_distribution_fast</span>(</a>
<a class="sourceLine" id="cb99-5" data-line-number="5">  <span class="dt">mu_samples =</span> mu_samples,</a>
<a class="sourceLine" id="cb99-6" data-line-number="6">  <span class="dt">sigma_samples =</span> sigma_samples,</a>
<a class="sourceLine" id="cb99-7" data-line-number="7">  N_obs</a>
<a class="sourceLine" id="cb99-8" data-line-number="8">)</a></code></pre></div>
<pre><code>## # A tibble: 1,444,000 x 3
##    iter trialn rt_pred
##   &lt;dbl&gt;  &lt;int&gt;   &lt;dbl&gt;
## 1     1      1    137.
## 2     1      2    181.
## 3     1      3    171.
## 4     1      4    162.
## 5     1      5    171.
## # … with 1,443,995 more rows</code></pre>
<p>There is a built-in function provided in <code>brms</code>, that will give us the posterior predictive distribution: <code>posterior_predict(fit_press)</code> provides the predicted reaction times in a matrix, with the number of samples as rows and the number of observations (data-points) as columns.</p>
<p>We can use the posterior predictive distribution to examine the “descriptive adequacy” of our models <span class="citation">(Gelman et al. <a href="#ref-Gelman14">2014</a>, Chapter 6; Shiffrin et al. <a href="#ref-shiffrinSurveyModelEvaluation2008">2008</a>)</span>; these are called posterior predictive checks, and what we want to establish here is that the posterior predictive data look more or less similar to the observed data. Achieving descriptive adequacy means that the current data could have been generated by the model. While passing a test of descriptive adequacy is not strong evidence in favor of a model, a major failure in descriptive adequacy can be interpreted as strong evidence against a model <span class="citation">(Shiffrin et al. <a href="#ref-shiffrinSurveyModelEvaluation2008">2008</a>)</span>. Thus, posterior predictive checking is an important sanity check to assess whether the model behavior is reasonable.</p>
<p>In many cases, we can simply use the plot functions from <code>brms</code> and <code>bayesplot</code> that take the model as an argument for the visualization of posterior predictive checks. For example, we can use <code>pp_check</code> to investigate how well the observed distribution of reaction times fit our model based on some number (11 and 100) of samples of the posterior predictive distributions; see figures <a href="sec-ppd.html#fig:normalppc">3.7</a> and <a href="sec-ppd.html#fig:normalppc2">3.8</a>.</p>

<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press, <span class="dt">nsamples =</span> <span class="dv">11</span>, <span class="dt">type =</span> <span class="st">&quot;hist&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:normalppc"></span>
<img src="bookdown_files/figure-html/normalppc-1.svg" alt="Eleven samples from the posterior predictive distribution of the model fit_press." width="672" />
<p class="caption">
FIGURE 3.7: Eleven samples from the posterior predictive distribution of the model <code>fit_press</code>.
</p>
</div>

<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press, <span class="dt">nsamples =</span> <span class="dv">100</span>)</a></code></pre></div>
<div class="figure"><span id="fig:normalppc2"></span>
<img src="bookdown_files/figure-html/normalppc2-1.svg" alt="Posterior predictive check that shows the fit of the model fit_press in comparison to datasets from the posterior predictive distribution." width="672" />
<p class="caption">
FIGURE 3.8: Posterior predictive check that shows the fit of the model <code>fit_press</code> in comparison to datasets from the posterior predictive distribution.
</p>
</div>
<p>Notice that the real data is slightly skewed and has no values shorter than 100 ms, while the predictive distributions are centered and symmetrical; see figures <a href="sec-ppd.html#fig:normalppc">3.7</a> and <a href="sec-ppd.html#fig:normalppc2">3.8</a>. This posterior predictive check shows a slight mismatch between the observed and predicted data. Can we build a better model? We’ll come back to this issue in the next section.</p>
<div id="comparing-different-likelihoods" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Comparing different likelihoods</h3>
<p>Since we know that the reaction times shouldn’t be normally distributed, we can choose a more realistic distribution for the likelihood. A good candidate is the log-normal distribution since a variable (such as time) that is log-normally distributed takes only positive real values and is right skewed.</p>
</div>
<div id="sec:lnfirst" class="section level3">
<h3><span class="header-section-number">3.5.2</span> The log-normal likelihood</h3>
<p>If <span class="math inline">\(y\)</span> is log-normally distributed, this means that <span class="math inline">\(\log(y)\)</span> is normally distributed.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> Something important to notice is that the log-normal distribution is again defined using <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, but these correspond to the mean and standard deviation of the normally distributed logarithm of the data <span class="math inline">\(y\)</span>: <span class="math inline">\(\log(y)\)</span>. Thus, when we model some data <span class="math inline">\(y\)</span> using the log-normal likelihood, the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are on a different scale than the data <span class="math inline">\(y\)</span>.</p>
<p>We can create a log-normal distribution by exponentiating the samples of a normal distribution. See Figure <a href="sec-ppd.html#fig:logndemo">3.9</a>.</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\log(y) &amp;\sim Normal( \mu, \sigma)\\
y &amp;\sim \exp(Normal( \mu, \sigma)) \\
y &amp;\sim LogNormal( \mu, \sigma)
\end{aligned}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1">mu &lt;-<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb103-2" data-line-number="2">sigma &lt;-<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb103-3" data-line-number="3">N &lt;-<span class="st"> </span><span class="dv">500000</span></a>
<a class="sourceLine" id="cb103-4" data-line-number="4"><span class="co"># Generate N random samples from a log-normal distribution</span></a>
<a class="sourceLine" id="cb103-5" data-line-number="5">sl &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(N, mu, sigma)</a>
<a class="sourceLine" id="cb103-6" data-line-number="6"><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">samples =</span> sl), <span class="kw">aes</span>(samples)) <span class="op">+</span></a>
<a class="sourceLine" id="cb103-7" data-line-number="7"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">50</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb103-8" data-line-number="8"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Log-normal distribution</span><span class="ch">\n</span><span class="st">&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb103-9" data-line-number="9"><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">70000</span>), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>))</a>
<a class="sourceLine" id="cb103-10" data-line-number="10"><span class="co"># Generate N random samples from a normal distribution,</span></a>
<a class="sourceLine" id="cb103-11" data-line-number="11"><span class="co"># and then exponentiate them</span></a>
<a class="sourceLine" id="cb103-12" data-line-number="12">sn &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">rnorm</span>(N, mu, sigma))</a>
<a class="sourceLine" id="cb103-13" data-line-number="13"><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">samples =</span> sn), <span class="kw">aes</span>(samples)) <span class="op">+</span></a>
<a class="sourceLine" id="cb103-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">50</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb103-15" data-line-number="15"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Exponentiated samples of</span><span class="ch">\n</span><span class="st">a normal distribution&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb103-16" data-line-number="16"><span class="st">    </span><span class="kw">coord_cartesian</span>(<span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">70000</span>), <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">2000</span>))</a></code></pre></div>
<div class="figure"><span id="fig:logndemo"></span>
<img src="bookdown_files/figure-html/logndemo-1.svg" alt="Two log-normal distributions with the same parameters generated by either generating samples from a log-normal distribution or exponentiating samples from a normal distribution." width="48%" /><img src="bookdown_files/figure-html/logndemo-2.svg" alt="Two log-normal distributions with the same parameters generated by either generating samples from a log-normal distribution or exponentiating samples from a normal distribution." width="48%" />
<p class="caption">
FIGURE 3.9: Two log-normal distributions with the same parameters generated by either generating samples from a log-normal distribution or exponentiating samples from a normal distribution.
</p>
</div>
</div>
<div id="sec:lognormal" class="section level3">
<h3><span class="header-section-number">3.5.3</span> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</h3>
<p>If we assume that reaction times are log-normally distributed, we’ll need to change our likelihood function as follows:</p>
<p><span class="math display">\[\begin{equation}
rt_n \sim LogNormal(\mu,\sigma)
\end{equation}\]</span></p>
<p>But now the scale of our priors needs to change! We’ll continue with the uniform priors for ease of exposition, even though, as we mentioned earlier, these are not recommended.</p>
<p><span class="math display" id="eq:logpriorsunif">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim Uniform(0, 8) \\
\sigma &amp;\sim Uniform(0, 1) \\
\end{aligned}
\tag{3.7}
\end{equation}\]</span></p>
<p>Because the parameters are in a different scale than the dependent variable, their interpretation changes and it is more complex than if we were dealing with a linear model that assumes a normal likelihood (location and scale do not coincide with the mean and standard deviation of the log-normal):</p>
<ul>
<li><em>The location, <span class="math inline">\(\mu\)</span></em>: In our previous linear model, <span class="math inline">\(\mu\)</span> represented the grand mean (or the grand median, or grand mode, since in a normal distribution the three coincide). But now, the grand mean needs to be calculated in the following way, <span class="math inline">\(\exp(\mu +\sigma ^{2}/2)\)</span>. Interestingly, the grand median will just be <span class="math inline">\(\exp(\mu)\)</span>. We could assume that the grand median, <span class="math inline">\(\exp(\mu)\)</span>, represents the underlying time it takes to press the space bar if there would be no noise, that is, if <span class="math inline">\(\sigma\)</span> would be 0. This also means that the prior of <span class="math inline">\(\mu\)</span> is not in milliseconds, but in log(milliseconds).</li>
<li><em>The scale, <span class="math inline">\(\sigma\)</span></em>: This is the standard deviation of the normal distribution of <span class="math inline">\(\log(y)\)</span>. The standard deviation of a log-normal distribution with <em>location</em> <span class="math inline">\(\mu\)</span> and <em>shape</em> <span class="math inline">\(\sigma\)</span> will be <span class="math inline">\(\exp(\mu +\sigma ^{2}/2)\times \sqrt(\exp(\sigma^2)- 1)\)</span>. It’s important to notice that, unlike the normal distribution, the spread of the log-normal distribution depends on both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</li>
</ul>
<p>To understand the meaning of our priors in the millisecond scale, we need to take into account both the priors and the likelihood. We can do this by generating a prior predictive distribution. Notice that we can just exponentiate the samples produced by <code>normal_predictive_distribution_fast()</code> (or, alternatively, we could have edited the function and replaced <code>rnorm</code> for <code>rlnorm</code>).</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1">N_samples &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb104-2" data-line-number="2">N_obs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_noreading_data)</a>
<a class="sourceLine" id="cb104-3" data-line-number="3">mu_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(N_samples, <span class="dv">0</span>, <span class="dv">8</span>)</a>
<a class="sourceLine" id="cb104-4" data-line-number="4">sigma_samples &lt;-<span class="st"> </span><span class="kw">runif</span>(N_samples, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb104-5" data-line-number="5">prior_pred_ln &lt;-<span class="st"> </span><span class="kw">normal_predictive_distribution_fast</span>(</a>
<a class="sourceLine" id="cb104-6" data-line-number="6">  <span class="dt">mu_samples =</span> mu_samples,</a>
<a class="sourceLine" id="cb104-7" data-line-number="7">  <span class="dt">sigma_samples =</span> sigma_samples,</a>
<a class="sourceLine" id="cb104-8" data-line-number="8">  N_obs</a>
<a class="sourceLine" id="cb104-9" data-line-number="9">) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb104-10" data-line-number="10"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rt_pred =</span> <span class="kw">exp</span>(rt_pred))</a></code></pre></div>
<p>And then we plot the distribution of some representative statistics:</p>

<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" data-line-number="1">prior_pred_ln <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb105-2" data-line-number="2"><span class="st">  </span><span class="kw">group_by</span>(iter) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb105-3" data-line-number="3"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb105-4" data-line-number="4">    <span class="dt">min_rt =</span> <span class="kw">min</span>(rt_pred),</a>
<a class="sourceLine" id="cb105-5" data-line-number="5">    <span class="dt">max_rt =</span> <span class="kw">max</span>(rt_pred),</a>
<a class="sourceLine" id="cb105-6" data-line-number="6">    <span class="dt">average_rt =</span> <span class="kw">mean</span>(rt_pred),</a>
<a class="sourceLine" id="cb105-7" data-line-number="7">    <span class="dt">median_rt =</span> <span class="kw">median</span>(rt_pred)</a>
<a class="sourceLine" id="cb105-8" data-line-number="8">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb105-9" data-line-number="9"><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="dt">cols =</span> <span class="kw">ends_with</span>(<span class="st">&quot;rt&quot;</span>), <span class="dt">names_to =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">values_to =</span> <span class="st">&quot;rt&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb105-10" data-line-number="10"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(rt)) <span class="op">+</span></a>
<a class="sourceLine" id="cb105-11" data-line-number="11"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Reaction times in ms&quot;</span>,</a>
<a class="sourceLine" id="cb105-12" data-line-number="12">    <span class="dt">trans =</span> <span class="st">&quot;log&quot;</span>, <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>)</a>
<a class="sourceLine" id="cb105-13" data-line-number="13">  )<span class="op">+</span></a>
<a class="sourceLine" id="cb105-14" data-line-number="14"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb105-15" data-line-number="15"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>stat, <span class="dt">ncol =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure"><span id="fig:priorpredlogunif"></span>
<img src="bookdown_files/figure-html/priorpredlogunif-1.svg" alt="Prior predictive distribution of averages, maximum, and minimum value of the log-normal model with priors defined in (3.7). Notice that the x-axis is log-transformed." width="672" />
<p class="caption">
FIGURE 3.10: Prior predictive distribution of averages, maximum, and minimum value of the log-normal model with priors defined in <a href="sec-ppd.html#eq:logpriorsunif">(3.7)</a>. Notice that the x-axis is log-transformed.
</p>
</div>
<p>While we cannot generate negative values anymore, since <span class="math inline">\(\exp(\)</span>any number<span class="math inline">\() &gt; 0\)</span>, and these priors might work, we can choose better regularizing priors for our model, such as the following:</p>
<p><span class="math display" id="eq:logpriorsnorm">\[\begin{equation}
\begin{aligned}
\mu &amp;\sim Normal(6, 1.5) \\
\sigma &amp;\sim Normal_+(0, 1) \\
\end{aligned}
\tag{3.8}
\end{equation}\]</span></p>
<p>Notice that while <span class="math inline">\(\mu\)</span> can be negative, the dependent variable won’t, since the exponent of a negative value, <span class="math inline">\(\exp(\)</span>some negative value<span class="math inline">\()\)</span>, is always greater than <span class="math inline">\(0\)</span>. Even before generating the prior predictive distributions, we can calculate the values within which we are 95% sure that the expected median of the observations will lie. We do this by looking at what happens at two standard deviations away from the mean of the <em>prior</em>, <span class="math inline">\(\mu\)</span>, that is <span class="math inline">\(6 - 2\times 1.5\)</span> and <span class="math inline">\(6 + 2\times 1.5\)</span>, and exponentiating these values:</p>

<div class="rmdnote">
to-do: We should explain more the meaning of location of the prior of sigma being 0. Maybe using extraDistr::rtnorm(0,1,a=0)
</div>

<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1"><span class="kw">c</span>(<span class="dt">lower =</span> <span class="kw">exp</span>(<span class="dv">6</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="fl">1.5</span>),</a>
<a class="sourceLine" id="cb106-2" data-line-number="2">  <span class="dt">higher =</span> <span class="kw">exp</span>(<span class="dv">6</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="fl">1.5</span>))</a></code></pre></div>
<pre><code>##  lower higher 
##     20   8103</code></pre>
<p>This means that our prior for <span class="math inline">\(\mu\)</span> is still not too informative (these are medians; the actual values generated by the distribution can be much more spread out). We plot the distribution of some representative statistics in Figure <a href="sec-ppd.html#fig:priorpredlognorm">3.11</a>.</p>

<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1">N_samples &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb108-2" data-line-number="2">N_obs &lt;-<span class="st"> </span><span class="kw">nrow</span>(df_noreading_data)</a>
<a class="sourceLine" id="cb108-3" data-line-number="3">mu_samples &lt;-<span class="st"> </span><span class="kw">rnorm</span>(N_samples, <span class="dv">6</span>, <span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb108-4" data-line-number="4">sigma_samples &lt;-<span class="st"> </span><span class="kw">rtnorm</span>(N_samples, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">a =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb108-5" data-line-number="5">prior_pred_ln_better &lt;-<span class="st"> </span><span class="kw">normal_predictive_distribution_fast</span>(</a>
<a class="sourceLine" id="cb108-6" data-line-number="6">  <span class="dt">mu_samples =</span> mu_samples,</a>
<a class="sourceLine" id="cb108-7" data-line-number="7">  <span class="dt">sigma_samples =</span> sigma_samples,</a>
<a class="sourceLine" id="cb108-8" data-line-number="8">  N_obs) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb108-9" data-line-number="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">rt_pred =</span> <span class="kw">exp</span>(rt_pred))</a>
<a class="sourceLine" id="cb108-10" data-line-number="10">  </a>
<a class="sourceLine" id="cb108-11" data-line-number="11"></a>
<a class="sourceLine" id="cb108-12" data-line-number="12">prior_pred_ln_better <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb108-13" data-line-number="13"><span class="st">  </span><span class="kw">group_by</span>(iter) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb108-14" data-line-number="14"><span class="st">  </span><span class="kw">summarize</span>(</a>
<a class="sourceLine" id="cb108-15" data-line-number="15">    <span class="dt">min_rt =</span> <span class="kw">min</span>(rt_pred),</a>
<a class="sourceLine" id="cb108-16" data-line-number="16">    <span class="dt">max_rt =</span> <span class="kw">max</span>(rt_pred),</a>
<a class="sourceLine" id="cb108-17" data-line-number="17">    <span class="dt">average_rt =</span> <span class="kw">mean</span>(rt_pred),</a>
<a class="sourceLine" id="cb108-18" data-line-number="18">    <span class="dt">median_rt =</span> <span class="kw">median</span>(rt_pred)</a>
<a class="sourceLine" id="cb108-19" data-line-number="19">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb108-20" data-line-number="20"><span class="st"> </span><span class="kw">pivot_longer</span>(<span class="dt">cols =</span> <span class="kw">ends_with</span>(<span class="st">&quot;rt&quot;</span>),</a>
<a class="sourceLine" id="cb108-21" data-line-number="21">              <span class="dt">names_to =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">values_to =</span> <span class="st">&quot;rt&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb108-22" data-line-number="22"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(rt)) <span class="op">+</span></a>
<a class="sourceLine" id="cb108-23" data-line-number="23"><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">trans =</span> <span class="st">&quot;log&quot;</span>, <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">100000</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb108-24" data-line-number="24"><span class="st">  </span><span class="kw">geom_histogram</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb108-25" data-line-number="25"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>stat, <span class="dt">ncol =</span> <span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb108-26" data-line-number="26"><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="fl">0.001</span>, <span class="dv">300000</span>))</a></code></pre></div>
<div class="figure"><span id="fig:priorpredlognorm"></span>
<img src="bookdown_files/figure-html/priorpredlognorm-1.svg" alt="Prior predictive distribution of averages, maximum, and minimum value of the log-normal model with priors defined in (3.8). Notice that the x-axis is log-transformed." width="672" />
<p class="caption">
FIGURE 3.11: Prior predictive distribution of averages, maximum, and minimum value of the log-normal model with priors defined in <a href="sec-ppd.html#eq:logpriorsnorm">(3.8)</a>. Notice that the x-axis is log-transformed.
</p>
</div>
<p>We see that the priors that we are using are still too uninformative. The tails of the prior predictive distributions that correspond to our normal priors shown in Figure <a href="sec-ppd.html#fig:priorpredlognorm">3.11</a> are even further to the right reaching more extreme values than for the predictive distributions generated by uniform priors shown in Figure <a href="sec-ppd.html#fig:priorpredlogunif">3.10</a>. Our new priors are still far from perfectly encoding our prior knowledge. We could do more iterations of choosing priors and generating prior predictive distributions until we have priors that generate realistic data. However, for most cases, priors that generate data whose statistics (mean, median, min, max, etc.) lie roughly in the correct order of magnitude are going to be acceptable.</p>
<p>We can fit the model now, but notice that we need to specify that the family is <code>lognormal()</code>. In our first example, we had used the family <code>gaussian()</code>.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb109-1" data-line-number="1">fit_press_ln &lt;-<span class="st"> </span><span class="kw">brm</span>(rt <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb109-2" data-line-number="2">  <span class="dt">data =</span> df_noreading_data,</a>
<a class="sourceLine" id="cb109-3" data-line-number="3">  <span class="dt">family =</span> <span class="kw">lognormal</span>(),</a>
<a class="sourceLine" id="cb109-4" data-line-number="4">  <span class="dt">prior =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb109-5" data-line-number="5">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">6</span>, <span class="fl">1.5</span>), <span class="dt">class =</span> Intercept),</a>
<a class="sourceLine" id="cb109-6" data-line-number="6">    <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> sigma)</a>
<a class="sourceLine" id="cb109-7" data-line-number="7">  )</a>
<a class="sourceLine" id="cb109-8" data-line-number="8">)</a></code></pre></div>
<p>When we look at the summary of the posterior, the parameters are in log-scale:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1">fit_press_ln</a></code></pre></div>
<pre><code>##  Family: lognormal 
##   Links: mu = identity; sigma = identity 
## Formula: rt ~ 1 
##    Data: df_noreading_data (Number of observations: 361) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat
## Intercept     5.12      0.01     5.10     5.13 1.00
##           Bulk_ESS Tail_ESS
## Intercept     3714     2514
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat
## sigma     0.13      0.01     0.12     0.15 1.00
##       Bulk_ESS Tail_ESS
## sigma     2994     2371
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>If we want to know how long does it take to press the space bar in milliseconds, we need to transform the <span class="math inline">\(\mu\)</span> (or <code>Intercept</code> in the model) to milliseconds. Since we know that the median of the log-normal distribution is <span class="math inline">\(exp(\mu)\)</span>, we do the following to calculate an estimate in milliseconds:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1">estimate_ms &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">posterior_samples</span>(fit_press_ln)<span class="op">$</span>b_Intercept)</a></code></pre></div>
<p>If we want to know the mean and 95% credible interval, we do the following:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="kw">c</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(estimate_ms), <span class="kw">quantile</span>(estimate_ms, <span class="dt">probs =</span> <span class="kw">c</span>(.<span class="dv">025</span>, <span class="fl">.975</span>)))</a></code></pre></div>
<pre><code>## mean 2.5%  98% 
##  167  165  169</code></pre>
<p>We can now verify whether our predicted datasets look similar to
the real dataset. See Figure <a href="sec-ppd.html#fig:lognppc">3.12</a>; compare this with the earlier Figure <a href="sec-ppd.html#fig:normalppc2">3.8</a>.</p>

<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press_ln, <span class="dt">nsamples =</span> <span class="dv">100</span>)</a></code></pre></div>
<div class="figure"><span id="fig:lognppc"></span>
<img src="bookdown_files/figure-html/lognppc-1.svg" alt="Posterior predictive distribution of fit_noreading_ln" width="672" />
<p class="caption">
FIGURE 3.12: Posterior predictive distribution of <code>fit_noreading_ln</code>
</p>
</div>
<p><em>Are the posterior predicted data now more similar to the real data, compared to the case where we had a Normal likelihood?</em></p>
<p>It seems so, but it’s not easy to tell. Another way to examine this would be to look at the distribution of summary statistics. We compare the distribution of representative summary statistics for the datasets generated by different models and compare them to the observed statistics. We suspect that the normal distribution would generate reaction times that are too fast (since it’s symmetrical) and that the log-normal distribution may capture the long tail better than the normal model. Based on our hunch, we compute the distribution of minimum and maximum values for the posterior predictive distributions, and we compare them with the minimum and maximum value respectively in the data.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press, <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">stat =</span> <span class="st">&quot;min&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Normal model&quot;</span>)</a>
<a class="sourceLine" id="cb116-2" data-line-number="2"><span class="kw">pp_check</span>(fit_press_ln, <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">stat =</span> <span class="st">&quot;min&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Log-normal model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:ppcheckmin"></span>
<img src="bookdown_files/figure-html/ppcheckmin-1.svg" alt="Distribution of minimum values in a posterior predictive check. The minimum in the data is 110 ms." width="45%" /><img src="bookdown_files/figure-html/ppcheckmin-2.svg" alt="Distribution of minimum values in a posterior predictive check. The minimum in the data is 110 ms." width="45%" />
<p class="caption">
FIGURE 3.13: Distribution of minimum values in a posterior predictive check. The minimum in the data is 110 ms.
</p>
</div>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">pp_check</span>(fit_press, <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">stat =</span> <span class="st">&quot;max&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Normal model&quot;</span>)</a>
<a class="sourceLine" id="cb117-2" data-line-number="2"><span class="kw">pp_check</span>(fit_press_ln, <span class="dt">type =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">stat =</span> <span class="st">&quot;max&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Log-normal model&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:ppcheckmax"></span>
<img src="bookdown_files/figure-html/ppcheckmax-1.svg" alt="Distribution of maximum values in a posterior predictive check. The maximum in the data is 409 ms." width="45%" /><img src="bookdown_files/figure-html/ppcheckmax-2.svg" alt="Distribution of maximum values in a posterior predictive check. The maximum in the data is 409 ms." width="45%" />
<p class="caption">
FIGURE 3.14: Distribution of maximum values in a posterior predictive check. The maximum in the data is 409 ms.
</p>
</div>
<p>Figure <a href="sec-ppd.html#fig:ppcheckmin">3.13</a> shows that the log-normal likelihood does a slightly better job since the minimum value is contained in the bulk of the log-normal distribution and in the tail of the normal one. Figure <a href="sec-ppd.html#fig:ppcheckmax">3.14</a> shows that both models are unable to capture the maximum value of the observed data. One explanation for this is that the log-normal-ish observations in our data are being generated by the task of pressing as fast as possible, while the observations with long reaction times are being generated by lapses of attention.
This would mean that two probability distributions are mixed here; modeling this process involves more complex tools that we will treat in chapter <a href="#ch:mix"><strong>??</strong></a>.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Gelman14">
<p>Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2014. <em>Bayesian Data Analysis</em>. Third. Boca Raton, FL: Chapman; Hall/CRC.</p>
</div>
<div id="ref-shiffrinSurveyModelEvaluation2008">
<p>Shiffrin, Richard, Michael Lee, Woojae Kim, and Eric-Jan Wagenmakers. 2008. “A Survey of Model Evaluation Approaches with a Tutorial on Hierarchical Bayesian Methods.” <em>Cognitive Science: A Multidisciplinary Journal</em> 32 (8): 1248–84. <a href="https://doi.org/10.1080/03640210802414826">https://doi.org/10.1080/03640210802414826</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>In fact, <span class="math inline">\(\log_e(y)\)</span> or <span class="math inline">\(\ln(y)\)</span>, but we’ll write it as just <span class="math inline">\(log()\)</span><a href="sec-ppd.html#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-revisit.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/03-compbayes.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
