<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>15.1 A mixture model of the speed-accuracy trade-off | An Introduction to Bayesian Data Analysis for Cognitive Science</title>
  <meta name="description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="generator" content="bookdown 0.20.2 and GitBook 2.6.7" />

  <meta property="og:title" content="15.1 A mixture model of the speed-accuracy trade-off | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://vasishth.github.io/Bayes_CogSci/" />
  <meta property="og:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />
  <meta property="og:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="github-repo" content="https://github.com/vasishth/Bayes_CogSci" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="15.1 A mixture model of the speed-accuracy trade-off | An Introduction to Bayesian Data Analysis for Cognitive Science" />
  
  <meta name="twitter:description" content="An introduction to Bayesian data analysis for Cognitive Science." />
  <meta name="twitter:image" content="https://vasishth.github.io/Bayes_CogSci/images/temporarycover.jpg" />

<meta name="author" content="Bruno Nicenboim, Daniel Schad, and Shravan Vasishth" />


<meta name="date" content="2020-09-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-mixture.html"/>
<link rel="next" href="summary-8.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script type="text/javascript">
 $(document).ready(function() {
     $folds = $(".solution");
     $folds.wrapInner("<div class=\"solution-blck\">"); // wrap a div container around content
     $folds.prepend("<button class=\"solution-btn\">Show solution</button>");  // add a button
     $(".solution-blck").toggle();  // fold all blocks
     $(".solution-btn").on("click", function() {  // add onClick event
         $(this).text($(this).text() === "Hide solution" ? "Show solution" : "Hide solution");  // if the text equals "Hide solution", change it to "Show solution"or else to "Hide solution" 
         $(this).next(".solution-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
     })
 });
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis for Cognitive Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>0.1</b> Prerequisites</a></li>
<li class="chapter" data-level="0.2" data-path="developing-the-right-mindset-for-this-book.html"><a href="developing-the-right-mindset-for-this-book.html"><i class="fa fa-check"></i><b>0.2</b> Developing the right mindset for this book</a></li>
<li class="chapter" data-level="0.3" data-path="how-to-read-this-book.html"><a href="how-to-read-this-book.html"><i class="fa fa-check"></i><b>0.3</b> How to read this book</a></li>
<li class="chapter" data-level="0.4" data-path="online-materials.html"><a href="online-materials.html"><i class="fa fa-check"></i><b>0.4</b> Online materials</a></li>
<li class="chapter" data-level="0.5" data-path="software-needed.html"><a href="software-needed.html"><i class="fa fa-check"></i><b>0.5</b> Software needed</a></li>
<li class="chapter" data-level="0.6" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>0.6</b> Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="part"><span><b>I Foundational ideas</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introprob.html"><a href="introprob.html"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="conditional-probability.html"><a href="conditional-probability.html"><i class="fa fa-check"></i><b>1.2</b> Conditional probability</a></li>
<li class="chapter" data-level="1.3" data-path="the-law-of-total-probability.html"><a href="the-law-of-total-probability.html"><i class="fa fa-check"></i><b>1.3</b> The law of total probability</a></li>
<li class="chapter" data-level="1.4" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html"><i class="fa fa-check"></i><b>1.4</b> Discrete random variables: An example using the Binomial distribution</a><ul>
<li class="chapter" data-level="1.4.1" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#the-mean-and-variance-of-the-binomial-distribution"><i class="fa fa-check"></i><b>1.4.1</b> The mean and variance of the Binomial distribution</a></li>
<li class="chapter" data-level="1.4.2" data-path="sec-binomialcloze.html"><a href="sec-binomialcloze.html#what-information-does-a-probability-distribution-provide"><i class="fa fa-check"></i><b>1.4.2</b> What information does a probability distribution provide?</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html"><i class="fa fa-check"></i><b>1.5</b> Continuous random variables: An example using the Normal distribution</a><ul>
<li class="chapter" data-level="1.5.1" data-path="continuous-random-variables-an-example-using-the-normal-distribution.html"><a href="continuous-random-variables-an-example-using-the-normal-distribution.html#an-important-distinction-probability-vs.-density-in-a-continuous-random-variable"><i class="fa fa-check"></i><b>1.5.1</b> An important distinction: probability vs. density in a continuous random variable</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html"><i class="fa fa-check"></i><b>1.6</b> Bivariate and multivariate distributions</a><ul>
<li class="chapter" data-level="1.6.1" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-1-discrete-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.1</b> Example 1: Discrete bivariate distributions</a></li>
<li class="chapter" data-level="1.6.2" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#example-2-continuous-bivariate-distributions"><i class="fa fa-check"></i><b>1.6.2</b> Example 2: Continuous bivariate distributions</a></li>
<li class="chapter" data-level="1.6.3" data-path="bivariate-and-multivariate-distributions.html"><a href="bivariate-and-multivariate-distributions.html#generate-simulated-bivariate-multivariate-data"><i class="fa fa-check"></i><b>1.6.3</b> Generate simulated bivariate (multivariate) data</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="sec-marginal.html"><a href="sec-marginal.html"><i class="fa fa-check"></i><b>1.7</b> An important concept: The marginal likelihood (integrating out a parameter)</a></li>
<li class="chapter" data-level="1.8" data-path="summary-of-useful-r-functions-relating-to-distributions.html"><a href="summary-of-useful-r-functions-relating-to-distributions.html"><i class="fa fa-check"></i><b>1.8</b> Summary of useful R functions relating to distributions</a></li>
<li class="chapter" data-level="1.9" data-path="summary-of-concepts-introduced-in-this-chapter.html"><a href="summary-of-concepts-introduced-in-this-chapter.html"><i class="fa fa-check"></i><b>1.9</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="1.10" data-path="further-reading.html"><a href="further-reading.html"><i class="fa fa-check"></i><b>1.10</b> Further reading</a></li>
<li class="chapter" data-level="1.11" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>1.11</b> Exercises</a><ul>
<li class="chapter" data-level="1.11.1" data-path="exercises.html"><a href="exercises.html#practice-using-the-pnorm-function"><i class="fa fa-check"></i><b>1.11.1</b> Practice using the <code>pnorm</code> function</a></li>
<li class="chapter" data-level="1.11.2" data-path="exercises.html"><a href="exercises.html#practice-using-the-qnorm-function"><i class="fa fa-check"></i><b>1.11.2</b> Practice using the <code>qnorm</code> function</a></li>
<li class="chapter" data-level="1.11.3" data-path="exercises.html"><a href="exercises.html#practice-using-qt"><i class="fa fa-check"></i><b>1.11.3</b> Practice using <code>qt</code></a></li>
<li class="chapter" data-level="1.11.4" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.11.4</b> Maximum likelihood estimation 1</a></li>
<li class="chapter" data-level="1.11.5" data-path="exercises.html"><a href="exercises.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>1.11.5</b> Maximum likelihood estimation 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introBDA.html"><a href="introBDA.html"><i class="fa fa-check"></i><b>2</b> Introduction to Bayesian data analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="sec-analytical.html"><a href="sec-analytical.html"><i class="fa fa-check"></i><b>2.1</b> Deriving the posterior using Bayes' rule: An analytical example</a><ul>
<li class="chapter" data-level="2.1.1" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.1.1</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.1.2" data-path="sec-analytical.html"><a href="sec-analytical.html#choosing-a-prior-for-theta"><i class="fa fa-check"></i><b>2.1.2</b> Choosing a prior for <span class="math inline">\(\theta\)</span></a></li>
<li class="chapter" data-level="2.1.3" data-path="sec-analytical.html"><a href="sec-analytical.html#using-bayes-rule-to-compute-the-posterior-pthetank"><i class="fa fa-check"></i><b>2.1.3</b> Using Bayes' rule to compute the posterior <span class="math inline">\(p(\theta|n,k)\)</span></a></li>
<li class="chapter" data-level="2.1.4" data-path="sec-analytical.html"><a href="sec-analytical.html#summary-of-the-procedure"><i class="fa fa-check"></i><b>2.1.4</b> Summary of the procedure</a></li>
<li class="chapter" data-level="2.1.5" data-path="sec-analytical.html"><a href="sec-analytical.html#visualizing-the-prior-likelihood-and-the-posterior"><i class="fa fa-check"></i><b>2.1.5</b> Visualizing the prior, likelihood, and the posterior</a></li>
<li class="chapter" data-level="2.1.6" data-path="sec-analytical.html"><a href="sec-analytical.html#the-posterior-distribution-is-a-compromise-between-the-prior-and-the-likelihood"><i class="fa fa-check"></i><b>2.1.6</b> The posterior distribution is a compromise between the prior and the likelihood</a></li>
<li class="chapter" data-level="2.1.7" data-path="sec-analytical.html"><a href="sec-analytical.html#incremental-knowledge-gain-using-prior-knowledge"><i class="fa fa-check"></i><b>2.1.7</b> Incremental knowledge gain using prior knowledge</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="summary-of-concepts-introduced-in-this-chapter-1.html"><a href="summary-of-concepts-introduced-in-this-chapter-1.html"><i class="fa fa-check"></i><b>2.2</b> Summary of concepts introduced in this chapter</a></li>
<li class="chapter" data-level="2.3" data-path="further-reading-1.html"><a href="further-reading-1.html"><i class="fa fa-check"></i><b>2.3</b> Further reading</a></li>
<li class="chapter" data-level="2.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>2.4</b> Exercises</a><ul>
<li class="chapter" data-level="2.4.1" data-path="exercises-1.html"><a href="exercises-1.html#exercise-deriving-bayes-rule"><i class="fa fa-check"></i><b>2.4.1</b> Exercise: Deriving Bayes' rule</a></li>
<li class="chapter" data-level="2.4.2" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-1"><i class="fa fa-check"></i><b>2.4.2</b> Exercise: Conjugate forms 1</a></li>
<li class="chapter" data-level="2.4.3" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-2"><i class="fa fa-check"></i><b>2.4.3</b> Exercise: Conjugate forms 2</a></li>
<li class="chapter" data-level="2.4.4" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-3"><i class="fa fa-check"></i><b>2.4.4</b> Exercise: Conjugate forms 3</a></li>
<li class="chapter" data-level="2.4.5" data-path="exercises-1.html"><a href="exercises-1.html#exercise-conjugate-forms-4"><i class="fa fa-check"></i><b>2.4.5</b> Exercise: Conjugate forms 4</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Regression models</b></span></li>
<li class="chapter" data-level="3" data-path="ch-compbda.html"><a href="ch-compbda.html"><i class="fa fa-check"></i><b>3</b> Computational Bayesian data analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-sampling.html"><a href="sec-sampling.html"><i class="fa fa-check"></i><b>3.1</b> Deriving the posterior through sampling</a><ul>
<li class="chapter" data-level="3.1.1" data-path="sec-sampling.html"><a href="sec-sampling.html#bayesian-regression-models-using-stan-brms"><i class="fa fa-check"></i><b>3.1.1</b> Bayesian Regression Models using 'Stan': brms</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="sec-priorpred.html"><a href="sec-priorpred.html"><i class="fa fa-check"></i><b>3.2</b> Prior predictive distribution</a></li>
<li class="chapter" data-level="3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html"><i class="fa fa-check"></i><b>3.3</b> The influence of priors: sensitivity analysis</a><ul>
<li class="chapter" data-level="3.3.1" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#flat-uninformative-priors"><i class="fa fa-check"></i><b>3.3.1</b> Flat uninformative priors</a></li>
<li class="chapter" data-level="3.3.2" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#regularizing-priors"><i class="fa fa-check"></i><b>3.3.2</b> Regularizing priors</a></li>
<li class="chapter" data-level="3.3.3" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#principled-priors"><i class="fa fa-check"></i><b>3.3.3</b> Principled priors</a></li>
<li class="chapter" data-level="3.3.4" data-path="sec-sensitivity.html"><a href="sec-sensitivity.html#informative-priors"><i class="fa fa-check"></i><b>3.3.4</b> Informative priors</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="sec-revisit.html"><a href="sec-revisit.html"><i class="fa fa-check"></i><b>3.4</b> Revisiting the button-pressing example with different priors</a></li>
<li class="chapter" data-level="3.5" data-path="sec-ppd.html"><a href="sec-ppd.html"><i class="fa fa-check"></i><b>3.5</b> Posterior predictive distribution</a><ul>
<li class="chapter" data-level="3.5.1" data-path="sec-ppd.html"><a href="sec-ppd.html#comparing-different-likelihoods"><i class="fa fa-check"></i><b>3.5.1</b> Comparing different likelihoods</a></li>
<li class="chapter" data-level="3.5.2" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lnfirst"><i class="fa fa-check"></i><b>3.5.2</b> The log-normal likelihood</a></li>
<li class="chapter" data-level="3.5.3" data-path="sec-ppd.html"><a href="sec-ppd.html#sec:lognormal"><i class="fa fa-check"></i><b>3.5.3</b> Re-fitting a single participant pressing a button repeatedly with a log-normal likelihood</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="further-reading-2.html"><a href="further-reading-2.html"><i class="fa fa-check"></i><b>3.7</b> Further reading</a></li>
<li class="chapter" data-level="3.8" data-path="ex-compbda.html"><a href="ex-compbda.html"><i class="fa fa-check"></i><b>3.8</b> Exercises</a></li>
<li class="chapter" data-level="3.9" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>3.9</b> Appendix</a><ul>
<li class="chapter" data-level="3.9.1" data-path="appendix.html"><a href="appendix.html#app:pp"><i class="fa fa-check"></i><b>3.9.1</b> Generating prior predictive distributions with <code>brms</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-reg.html"><a href="ch-reg.html"><i class="fa fa-check"></i><b>4</b> Bayesian regression models</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-pupil.html"><a href="sec-pupil.html"><i class="fa fa-check"></i><b>4.1</b> A first linear regression: Does attentional load affect pupil size?</a><ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pupil.html"><a href="sec-pupil.html#likelihood-and-priors"><i class="fa fa-check"></i><b>4.1.1</b> Likelihood and priors</a></li>
<li class="chapter" data-level="4.1.2" data-path="sec-pupil.html"><a href="sec-pupil.html#the-brms-model"><i class="fa fa-check"></i><b>4.1.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.1.3" data-path="sec-pupil.html"><a href="sec-pupil.html#how-to-communicate-the-results"><i class="fa fa-check"></i><b>4.1.3</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.1.4" data-path="sec-pupil.html"><a href="sec-pupil.html#sec:pupiladq"><i class="fa fa-check"></i><b>4.1.4</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-trial.html"><a href="sec-trial.html"><i class="fa fa-check"></i><b>4.2</b> Log-normal model: Does trial affect reaction times?</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sec-trial.html"><a href="sec-trial.html#likelihood-and-priors-for-the-log-normal-model"><i class="fa fa-check"></i><b>4.2.1</b> Likelihood and priors for the log-normal model</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-trial.html"><a href="sec-trial.html#the-brms-model-1"><i class="fa fa-check"></i><b>4.2.2</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-trial.html"><a href="sec-trial.html#how-to-communicate-the-results-1"><i class="fa fa-check"></i><b>4.2.3</b> How to communicate the results?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sec-logistic.html"><a href="sec-logistic.html"><i class="fa fa-check"></i><b>4.3</b> Logistic regression: Does set size affect free recall?</a><ul>
<li class="chapter" data-level="4.3.1" data-path="sec-logistic.html"><a href="sec-logistic.html#the-likelihood-for-the-logistic-regression-model"><i class="fa fa-check"></i><b>4.3.1</b> The likelihood for the logistic regression model</a></li>
<li class="chapter" data-level="4.3.2" data-path="sec-logistic.html"><a href="sec-logistic.html#priors-for-the-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Priors for the logistic regression</a></li>
<li class="chapter" data-level="4.3.3" data-path="sec-logistic.html"><a href="sec-logistic.html#the-brms-model-2"><i class="fa fa-check"></i><b>4.3.3</b> The <code>brms</code> model</a></li>
<li class="chapter" data-level="4.3.4" data-path="sec-logistic.html"><a href="sec-logistic.html#sec:comlogis"><i class="fa fa-check"></i><b>4.3.4</b> How to communicate the results?</a></li>
<li class="chapter" data-level="4.3.5" data-path="sec-logistic.html"><a href="sec-logistic.html#descriptive-adequacy"><i class="fa fa-check"></i><b>4.3.5</b> Descriptive adequacy</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="further-reading-3.html"><a href="further-reading-3.html"><i class="fa fa-check"></i><b>4.5</b> Further reading</a></li>
<li class="chapter" data-level="4.6" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
<li class="chapter" data-level="4.7" data-path="appendix-1.html"><a href="appendix-1.html"><i class="fa fa-check"></i><b>4.7</b> Appendix</a><ul>
<li class="chapter" data-level="4.7.1" data-path="appendix-1.html"><a href="appendix-1.html#sec:preprocessingpupil"><i class="fa fa-check"></i><b>4.7.1</b> Preparation of the pupil size data (section @ref(sec:pupil))</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-hierarchical.html"><a href="ch-hierarchical.html"><i class="fa fa-check"></i><b>5</b> Bayesian hierarchical models</a><ul>
<li class="chapter" data-level="5.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html"><i class="fa fa-check"></i><b>5.1</b> A hierarchical normal model: The N400 effect</a><ul>
<li class="chapter" data-level="5.1.1" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#complete-pooling-model-m_cp"><i class="fa fa-check"></i><b>5.1.1</b> Complete-pooling model (<span class="math inline">\(M_{cp}\)</span>)</a></li>
<li class="chapter" data-level="5.1.2" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#no-pooling-model-m_np"><i class="fa fa-check"></i><b>5.1.2</b> No-pooling model (<span class="math inline">\(M_{np}\)</span>)</a></li>
<li class="chapter" data-level="5.1.3" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:uncorrelated"><i class="fa fa-check"></i><b>5.1.3</b> Varying intercept and varying slopes model (<span class="math inline">\(M_{v}\)</span>)</a></li>
<li class="chapter" data-level="5.1.4" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:mcvivs"><i class="fa fa-check"></i><b>5.1.4</b> Correlated varying intercept varying slopes model (<span class="math inline">\(M_{h}\)</span>)</a></li>
<li class="chapter" data-level="5.1.5" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:sih"><i class="fa fa-check"></i><b>5.1.5</b> By-subjects and by-items correlated varying intercept varying slopes model (<span class="math inline">\(M_{sih}\)</span>)</a></li>
<li class="chapter" data-level="5.1.6" data-path="sec-N400hierarchical.html"><a href="sec-N400hierarchical.html#sec:distrmodel"><i class="fa fa-check"></i><b>5.1.6</b> Beyond the so-called maximal models--Distributional regression models</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="sec-stroop.html"><a href="sec-stroop.html"><i class="fa fa-check"></i><b>5.2</b> A hierarchical log-normal model: The Stroop effect</a><ul>
<li class="chapter" data-level="5.2.1" data-path="sec-stroop.html"><a href="sec-stroop.html#a-correlated-varying-intercept-varying-slopes-log-normal-model"><i class="fa fa-check"></i><b>5.2.1</b> A correlated varying intercept varying slopes log-normal model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>5.3</b> Summary</a><ul>
<li class="chapter" data-level="5.3.1" data-path="summary-2.html"><a href="summary-2.html#why-should-we-take-the-trouble-of-fitting-a-bayesian-hierarchical-model"><i class="fa fa-check"></i><b>5.3.1</b> Why should we take the trouble of fitting a Bayesian hierarchical model?</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="further-reading-4.html"><a href="further-reading-4.html"><i class="fa fa-check"></i><b>5.4</b> Further reading</a></li>
<li class="chapter" data-level="5.5" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-contr.html"><a href="ch-contr.html"><i class="fa fa-check"></i><b>6</b> Contrast coding</a><ul>
<li class="chapter" data-level="6.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html"><i class="fa fa-check"></i><b>6.1</b> Basic concepts illustrated using a two-level factor</a><ul>
<li class="chapter" data-level="6.1.1" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#treatmentcontrasts"><i class="fa fa-check"></i><b>6.1.1</b> Default contrast coding: Treatment contrasts</a></li>
<li class="chapter" data-level="6.1.2" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#inverseMatrix"><i class="fa fa-check"></i><b>6.1.2</b> Defining hypotheses</a></li>
<li class="chapter" data-level="6.1.3" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#effectcoding"><i class="fa fa-check"></i><b>6.1.3</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.1.4" data-path="basic-concepts-illustrated-using-a-two-level-factor.html"><a href="basic-concepts-illustrated-using-a-two-level-factor.html#cell-means-parameterization-and-posterior-comparisons"><i class="fa fa-check"></i><b>6.1.4</b> Cell means parameterization and posterior comparisons</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><i class="fa fa-check"></i><b>6.2</b> The hypothesis matrix illustrated with a three-level factor</a><ul>
<li class="chapter" data-level="6.2.1" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#sumcontrasts"><i class="fa fa-check"></i><b>6.2.1</b> Sum contrasts</a></li>
<li class="chapter" data-level="6.2.2" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#the-hypothesis-matrix"><i class="fa fa-check"></i><b>6.2.2</b> The hypothesis matrix</a></li>
<li class="chapter" data-level="6.2.3" data-path="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html"><a href="the-hypothesis-matrix-illustrated-with-a-three-level-factor.html#generating-contrasts-the-hypr-package"><i class="fa fa-check"></i><b>6.2.3</b> Generating contrasts: The <code>hypr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><i class="fa fa-check"></i><b>6.3</b> Further examples of contrasts illustrated with a factor with four levels</a><ul>
<li class="chapter" data-level="6.3.1" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#repeatedcontrasts"><i class="fa fa-check"></i><b>6.3.1</b> Repeated contrasts</a></li>
<li class="chapter" data-level="6.3.2" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#contrasts-in-linear-regression-analysis-the-design-or-model-matrix"><i class="fa fa-check"></i><b>6.3.2</b> Contrasts in linear regression analysis: The design or model matrix</a></li>
<li class="chapter" data-level="6.3.3" data-path="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html"><a href="further-examples-of-contrasts-illustrated-with-a-factor-with-four-levels.html#polynomialContrasts"><i class="fa fa-check"></i><b>6.3.3</b> Polynomial contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html"><i class="fa fa-check"></i><b>6.4</b> What makes a good set of contrasts?</a><ul>
<li class="chapter" data-level="6.4.1" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#centered-contrasts"><i class="fa fa-check"></i><b>6.4.1</b> Centered contrasts</a></li>
<li class="chapter" data-level="6.4.2" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#orthogonal-contrasts"><i class="fa fa-check"></i><b>6.4.2</b> Orthogonal contrasts</a></li>
<li class="chapter" data-level="6.4.3" data-path="nonOrthogonal.html"><a href="nonOrthogonal.html#the-role-of-the-intercept-in-non-centered-contrasts"><i class="fa fa-check"></i><b>6.4.3</b> The role of the intercept in non-centered contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html"><i class="fa fa-check"></i><b>6.5</b> Examples of contrast coding in a factorial design with two factors</a><ul>
<li class="chapter" data-level="6.5.1" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#the-difference-between-an-anova-and-a-multiple-regression"><i class="fa fa-check"></i><b>6.5.1</b> The difference between an ANOVA and a multiple regression</a></li>
<li class="chapter" data-level="6.5.2" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#nestedEffects"><i class="fa fa-check"></i><b>6.5.2</b> Nested effects</a></li>
<li class="chapter" data-level="6.5.3" data-path="MR-ANOVA.html"><a href="MR-ANOVA.html#interactions-between-contrasts"><i class="fa fa-check"></i><b>6.5.3</b> Interactions between contrasts</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-remame.html"><a href="ch-remame.html"><i class="fa fa-check"></i><b>7</b> Meta-analysis and measurement error models</a><ul>
<li class="chapter" data-level="7.1" data-path="meta-analysis.html"><a href="meta-analysis.html"><i class="fa fa-check"></i><b>7.1</b> Meta-analysis</a></li>
<li class="chapter" data-level="7.2" data-path="measurement-error-models.html"><a href="measurement-error-models.html"><i class="fa fa-check"></i><b>7.2</b> Measurement-error models</a></li>
<li class="chapter" data-level="7.3" data-path="further-reading-5.html"><a href="further-reading-5.html"><i class="fa fa-check"></i><b>7.3</b> Further reading</a></li>
<li class="chapter" data-level="7.4" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>7.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>III Model comparison</b></span></li>
<li class="chapter" data-level="8" data-path="ch-comparison.html"><a href="ch-comparison.html"><i class="fa fa-check"></i><b>8</b> Introduction to model comparison</a></li>
<li class="chapter" data-level="9" data-path="ch-bf.html"><a href="ch-bf.html"><i class="fa fa-check"></i><b>9</b> Bayes factors</a><ul>
<li class="chapter" data-level="9.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html"><i class="fa fa-check"></i><b>9.1</b> Hypothesis testing using the Bayes factor</a><ul>
<li class="chapter" data-level="9.1.1" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#marginal-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Marginal likelihood</a></li>
<li class="chapter" data-level="9.1.2" data-path="hypothesis-testing-using-the-bayes-factor.html"><a href="hypothesis-testing-using-the-bayes-factor.html#bayes-factor"><i class="fa fa-check"></i><b>9.1.2</b> Bayes factor</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="sec-N400BF.html"><a href="sec-N400BF.html"><i class="fa fa-check"></i><b>9.2</b> Testing the N400 effect using null hypothesis testing</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-N400BF.html"><a href="sec-N400BF.html#sec:BFnonnested"><i class="fa fa-check"></i><b>9.2.1</b> Non-nested models</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html"><i class="fa fa-check"></i><b>9.3</b> Understanding the (in-)stability of Bayes factors</a><ul>
<li class="chapter" data-level="9.3.1" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html#instability-due-to-the-number-of-iterations-of-the-posterior-sampler"><i class="fa fa-check"></i><b>9.3.1</b> Instability due to the number of iterations of the posterior sampler</a></li>
<li class="chapter" data-level="9.3.2" data-path="understanding-the-in-stability-of-bayes-factors.html"><a href="understanding-the-in-stability-of-bayes-factors.html#instability-due-to-posterior-uncertainty-and-noise-associated-with-subjects-items-and-residual-variability"><i class="fa fa-check"></i><b>9.3.2</b> Instability due to posterior uncertainty and noise associated with subjects, items, and residual variability</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="summary-4.html"><a href="summary-4.html"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
<li class="chapter" data-level="9.5" data-path="further-reading-6.html"><a href="further-reading-6.html"><i class="fa fa-check"></i><b>9.5</b> Further reading</a></li>
<li class="chapter" data-level="9.6" data-path="exercises-5.html"><a href="exercises-5.html"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ch-cv.html"><a href="ch-cv.html"><i class="fa fa-check"></i><b>10</b> Cross validation</a><ul>
<li class="chapter" data-level="10.1" data-path="expected-log-predictive-density-of-a-model.html"><a href="expected-log-predictive-density-of-a-model.html"><i class="fa fa-check"></i><b>10.1</b> Expected log predictive density of a model</a></li>
<li class="chapter" data-level="10.2" data-path="k-fold-and-leave-one-out-cross-validation.html"><a href="k-fold-and-leave-one-out-cross-validation.html"><i class="fa fa-check"></i><b>10.2</b> K-fold and leave-one-out cross-validation</a></li>
<li class="chapter" data-level="10.3" data-path="testing-the-n400-effect-using-cross-validation.html"><a href="testing-the-n400-effect-using-cross-validation.html"><i class="fa fa-check"></i><b>10.3</b> Testing the N400 effect using cross-validation</a></li>
<li class="chapter" data-level="10.4" data-path="summary-5.html"><a href="summary-5.html"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
<li class="chapter" data-level="10.5" data-path="further-reading-7.html"><a href="further-reading-7.html"><i class="fa fa-check"></i><b>10.5</b> Further reading</a></li>
<li class="chapter" data-level="10.6" data-path="exercises-6.html"><a href="exercises-6.html"><i class="fa fa-check"></i><b>10.6</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>IV Advanced models with Stan</b></span></li>
<li class="chapter" data-level="11" data-path="ch-introstan.html"><a href="ch-introstan.html"><i class="fa fa-check"></i><b>11</b> Introduction to the probabilistic programming language Stan</a><ul>
<li class="chapter" data-level="11.1" data-path="stan-syntax.html"><a href="stan-syntax.html"><i class="fa fa-check"></i><b>11.1</b> Stan syntax</a></li>
<li class="chapter" data-level="11.2" data-path="sec-firststan.html"><a href="sec-firststan.html"><i class="fa fa-check"></i><b>11.2</b> A first simple example with Stan: Normal likelihood</a></li>
<li class="chapter" data-level="11.3" data-path="sec-clozestan.html"><a href="sec-clozestan.html"><i class="fa fa-check"></i><b>11.3</b> Another simple example: Cloze probability with Stan: Binomial likelihood</a></li>
<li class="chapter" data-level="11.4" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html"><i class="fa fa-check"></i><b>11.4</b> Regression models in Stan</a><ul>
<li class="chapter" data-level="11.4.1" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:pupilstan"><i class="fa fa-check"></i><b>11.4.1</b> A first linear regression in Stan: Does attentional load affect pupil size?</a></li>
<li class="chapter" data-level="11.4.2" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:interstan"><i class="fa fa-check"></i><b>11.4.2</b> Interactions in Stan: Does attentional load interact with trial number affecting pupil size?</a></li>
<li class="chapter" data-level="11.4.3" data-path="regression-models-in-stan.html"><a href="regression-models-in-stan.html#sec:logisticstan"><i class="fa fa-check"></i><b>11.4.3</b> Logistic regression in Stan: Does set size and trial affect free recall?</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html"><i class="fa fa-check"></i><b>11.5</b> Model comparison in Stan</a><ul>
<li class="chapter" data-level="11.5.1" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#bayes-factor-in-stan"><i class="fa fa-check"></i><b>11.5.1</b> Bayes factor in Stan</a></li>
<li class="chapter" data-level="11.5.2" data-path="model-comparison-in-stan.html"><a href="model-comparison-in-stan.html#cross-validation-in-stan"><i class="fa fa-check"></i><b>11.5.2</b> Cross validation in Stan</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="summary-6.html"><a href="summary-6.html"><i class="fa fa-check"></i><b>11.6</b> Summary</a></li>
<li class="chapter" data-level="11.7" data-path="further-reading-8.html"><a href="further-reading-8.html"><i class="fa fa-check"></i><b>11.7</b> Further reading</a></li>
<li class="chapter" data-level="11.8" data-path="exercises-7.html"><a href="exercises-7.html"><i class="fa fa-check"></i><b>11.8</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ch-complexstan.html"><a href="ch-complexstan.html"><i class="fa fa-check"></i><b>12</b> Complex models and reparametrization</a><ul>
<li class="chapter" data-level="12.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html"><i class="fa fa-check"></i><b>12.1</b> Hierarchical models with Stan</a><ul>
<li class="chapter" data-level="12.1.1" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#varying-intercept-model-with-stan"><i class="fa fa-check"></i><b>12.1.1</b> Varying intercept model with Stan</a></li>
<li class="chapter" data-level="12.1.2" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:uncorrstan"><i class="fa fa-check"></i><b>12.1.2</b> Uncorrelated varying intercept and slopes model with Stan</a></li>
<li class="chapter" data-level="12.1.3" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:corrstan"><i class="fa fa-check"></i><b>12.1.3</b> Correlated varying intercept varying slopes model</a></li>
<li class="chapter" data-level="12.1.4" data-path="hierarchical-models-with-stan.html"><a href="hierarchical-models-with-stan.html#sec:crosscorrstan"><i class="fa fa-check"></i><b>12.1.4</b> By-participant and by-items correlated varying intercept varying slopes model</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="summary-7.html"><a href="summary-7.html"><i class="fa fa-check"></i><b>12.2</b> Summary</a></li>
<li class="chapter" data-level="12.3" data-path="further-reading-9.html"><a href="further-reading-9.html"><i class="fa fa-check"></i><b>12.3</b> Further reading</a></li>
<li class="chapter" data-level="12.4" data-path="exercises-8.html"><a href="exercises-8.html"><i class="fa fa-check"></i><b>12.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>V Computational cognitive modeling</b></span></li>
<li class="chapter" data-level="13" data-path="introduction-to-computational-cognitive-modeling.html"><a href="introduction-to-computational-cognitive-modeling.html"><i class="fa fa-check"></i><b>13</b> Introduction to computational cognitive modeling</a><ul>
<li class="chapter" data-level="13.1" data-path="further-reading-10.html"><a href="further-reading-10.html"><i class="fa fa-check"></i><b>13.1</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ch-MPT.html"><a href="ch-MPT.html"><i class="fa fa-check"></i><b>14</b> Multinomial processing trees</a><ul>
<li class="chapter" data-level="14.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html"><i class="fa fa-check"></i><b>14.1</b> Modeling multiple categorical responses</a><ul>
<li class="chapter" data-level="14.1.1" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:mult"><i class="fa fa-check"></i><b>14.1.1</b> A model for multiple responses using the multinomial likelihood</a></li>
<li class="chapter" data-level="14.1.2" data-path="modeling-multiple-categorical-responses.html"><a href="modeling-multiple-categorical-responses.html#sec:cat"><i class="fa fa-check"></i><b>14.1.2</b> A model for multiple responses using the categorical distribution</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html"><i class="fa fa-check"></i><b>14.2</b> Multinomial processing tree (MPT) models</a><ul>
<li class="chapter" data-level="14.2.1" data-path="multinomial-processing-tree-mpt-models.html"><a href="multinomial-processing-tree-mpt-models.html#mpts-for-modeling-picture-naming-abilities-in-aphasia"><i class="fa fa-check"></i><b>14.2.1</b> MPTs for modeling picture naming abilities in aphasia</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="further-reading-11.html"><a href="further-reading-11.html"><i class="fa fa-check"></i><b>14.3</b> Further reading</a></li>
<li class="chapter" data-level="14.4" data-path="exercises-9.html"><a href="exercises-9.html"><i class="fa fa-check"></i><b>14.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ch-mixture.html"><a href="ch-mixture.html"><i class="fa fa-check"></i><b>15</b> Mixture models</a><ul>
<li class="chapter" data-level="15.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html"><i class="fa fa-check"></i><b>15.1</b> A mixture model of the speed-accuracy trade-off</a><ul>
<li class="chapter" data-level="15.1.1" data-path="a-mixture-model-of-the-speed-accuracy-trade-off.html"><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#a-fast-guess-model-account-of-the-global-motion-detection-task"><i class="fa fa-check"></i><b>15.1.1</b> A fast guess model account of the global motion detection task</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="summary-8.html"><a href="summary-8.html"><i class="fa fa-check"></i><b>15.2</b> Summary</a></li>
<li class="chapter" data-level="15.3" data-path="further-reading-12.html"><a href="further-reading-12.html"><i class="fa fa-check"></i><b>15.3</b> Further reading</a></li>
<li class="chapter" data-level="15.4" data-path="exercises-10.html"><a href="exercises-10.html"><i class="fa fa-check"></i><b>15.4</b> Exercises</a></li>
</ul></li>
<li class="part"><span><b>VI Appendix</b></span></li>
<li class="chapter" data-level="16" data-path="important-distributions.html"><a href="important-distributions.html"><i class="fa fa-check"></i><b>16</b> Important distributions</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Data Analysis for Cognitive Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-mixture-model-of-the-speed-accuracy-trade-off" class="section level2">
<h2><span class="header-section-number">15.1</span> A mixture model of the speed-accuracy trade-off</h2>
<p>It has been long noticed that when we are faced with multiple choices that require an immediate decision, we can speed up the decision at the expense of accuracy and become more accurate at the expense of speed; this is the so-called speed-accuracy trade-off <span class="citation">(Wickelgren <a href="#ref-wickelgren1977speed">1977</a>)</span> <!-- (Bogacz, Wagenmakers, Forstmann, & Nieuwenhuis, 2010; Schouten & Bekker, 1967; Wickelgren, 1977) -->. The most popular class of models that can incorporate both response times and accuracy and give an account for the speed-accuracy trade-off is the class of sequential sampling models, which include the drift diffusion model <span class="citation">(Ratcliff <a href="#ref-Ratcliff1978">1978</a>)</span>, the linear ballistic accumulator <span class="citation">(Brown and Heathcote <a href="#ref-brownSimplestCompleteModel2008">2008</a>)</span>, and others; for a review see <span class="citation">Ratcliff et al. (<a href="#ref-Ratcliff2016">2016</a>)</span>.</p>
<p>However, an alternative model that has been proposed in the past is Ollman's simple fast guess model <span class="citation">(Ollman <a href="#ref-Ollman1966">1966</a>)</span>. Although it has mostly fallen out of favor <span class="citation">(but see Dutilh et al. <a href="#ref-DutilhEtAl2011">2011</a> for a more modern variant of this model)</span>, it presents a very simple framework using finite mixture modeling that can also account for the speed-accuracy trade-off. In the next section, we'll use this model to exemplify the use of finite mixtures to represent different cognitive processes.</p>
<div id="a-fast-guess-model-account-of-the-global-motion-detection-task" class="section level3">
<h3><span class="header-section-number">15.1.1</span> A fast guess model account of the global motion detection task</h3>
<p>One way to examine the behavior of human and primate subjects when faced with two-alternative forced choices is the detection of the global motion of a random dot kinematogram <span class="citation">(Britten et al. <a href="#ref-britten_shadlen_newsome_movshon_1993">1993</a>)</span>. In this task, a participant sees a number of random dots on the screen from which a proportion of them move in a single direction (e.g., up) and the rest move in random directions. The participant's goal is to estimate the overall direction of the movement. One of the reasons for the popularity of this task is that it permits the fine-tuning of the difficulty of trials <span class="citation">(Dutilh et al. <a href="#ref-Dutilh2019quality">2019</a>)</span>: The task is harder when the proportion of dots that move coherently (the level of <em>coherence</em>) is lower; see Fig. <a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#fig:globalmotionpng">15.1</a>.</p>

<div class="figure" style="text-align: center"><span id="fig:globalmotionpng"></span>
<img src="cc_figure/globalmotion.PNG" alt="Three levels of difficulty of the global motion detection task. The figures show a consistent upward movement with three levels of coherence (10%, 50%, and 100%). The participants see the dots moving in the direction indicated by the arrows. The participants do not see the arrows and all the dots look identical in the actual task. Adapted from Han et al. (2018); licensed under CC BY 4.0." width="100%" />
<p class="caption">
FIGURE 15.1: Three levels of difficulty of the global motion detection task. The figures show a consistent upward movement with three levels of coherence (10%, 50%, and 100%). The participants see the dots moving in the direction indicated by the arrows. The participants do not see the arrows and all the dots look identical in the actual task. Adapted from <span class="citation">Han et al. (<a href="#ref-DingEtAl">2018</a>)</span>; licensed under CC BY 4.0.
</p>
</div>
<p>Ollman's <span class="citation">(<a href="#ref-Ollman1966">1966</a>)</span> fast guess model assumes that the behavior in this task (and in any other choice task) is governed by two distinct cognitive processes: (i) a guess mode, and (ii) a task-engaged mode. In the guess mode, responses are fast and accuracy is at chance level. In the task-engaged mode, responses are slower and accuracy approaches 100%. This means that intermediate values of response times and accuracy can only be achieved by mixing responses from the two modes. Further assumptions of this model are that response times depend on the difficulty of the choice, and that the probability of being on one of the two states depend on the speed incentives during the instructions. (To simplify matters, we'll ignore the possibility of the accuracy of the choice being also affected by the difficulty of the choice.)</p>
<div id="dataset" class="section level4">
<h4><span class="header-section-number">15.1.1.1</span> Dataset</h4>
<p>We implement the assumptions behind Ollman's fast guess model and examine its fit to data of a global motion detection task from <span class="citation">Dutilh et al. (<a href="#ref-Dutilh2019quality">2019</a>)</span>.<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a></p>
<p>The dataset from <span class="citation">Dutilh et al. (<a href="#ref-Dutilh2019quality">2019</a>)</span> contains ~2800 trials of each of the 20 subjects participating in a global motion detection task. There were two level of coherence yielding hard and easy trials (<code>diff</code>), and the trials where done under instructions that emphasized either accuracy or speed (<code>emphasis</code>). (The description of the other columns can be found in <a href="https://osf.io/utkjf/" class="uri">https://osf.io/utkjf/</a>.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(df_dots_data &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/global_motion.csv&quot;</span>))</code></pre></div>
<pre><code>## # A tibble: 56,097 x 11
##   subject diff  emphasis    rt   acc fix.dur stim  resp  trial block
##     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1       1 easy  speed      482     1   0.738 R     R         1     6
## 2       1 hard  speed      602     1   0.784 R     R         2     6
## 3       1 hard  speed      381     1   0.651 R     R         3     6
## 4       1 hard  speed      584     1   0.781 R     R         4     6
## 5       1 hard  speed      464     0   0.585 L     R         5     6
##   block.trial
##         &lt;dbl&gt;
## 1           1
## 2           2
## 3           3
## 4           4
## 5           5
## # … with 56,092 more rows</code></pre>
<p>We could imagine that if the fast guesses model would be true, we would see a bimodal distribution, when we plot a histogram of the data. Unfortunately, when two similar distributions are mixed, we won't see any apparent bimodality:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_data, <span class="kw">aes</span>(rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-400-1.svg" width="672" /></p>
<p>However, another plot reveals that incorrect responses are generally faster, and this is especially true when the instructions emphasized accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_data, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(acc), <span class="dt">y =</span> rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">.4</span>, <span class="dt">height =</span> <span class="dv">0</span>),
             <span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(diff <span class="op">~</span><span class="st"> </span>emphasis) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-401-1.svg" width="672" /></p>
<!-- using a plotting style -->
<!-- that has become important in RT research, -->
<!-- called a “latency-probability” plot (LP plot: -->
<!-- Audley & Pike, 1965). Latency probability -->
<!-- plots show mean RT as a function of the -->
<!-- probability of a response. Points on the left -->
<!-- of the graph represent the lower probability -->
<!-- (error) responses and complementary points -->
<!-- on the right of the graph represent the higher -->
<!-- probability (correct) responses from the same -->
<!-- experimental conditions. Sometimes, LP -->
<!-- plots are expanded to show more than just -->
<!-- the mean RT, by plotting several quantiles -->
<!-- of the RT distributions–these are called -->
<!-- “quantile-probability,” or QP, plots. chapter -->
</div>
<div id="a-very-simple-implementation-of-the-fast-guess-model" class="section level4">
<h4><span class="header-section-number">15.1.1.2</span> A very simple implementation of the fast guess model</h4>
<p>The description of the model makes clear that an ideal participant that never guesses has a response time that depends on the difficulty of the trial. As we did in previous chapters, we assume that response times are log-normally distributed, and for simplicity we start by modeling the behavior of a single subject:</p>
<span class="math display">\[\begin{equation}
rt_n \sim LogNormal(\alpha + \beta \cdot x_n, \sigma)
\end{equation}\]</span>
<p>In the previous equation, <span class="math inline">\(x\)</span> is larger for difficult trials. If we center <span class="math inline">\(x\)</span>, <span class="math inline">\(\alpha\)</span> represents the average logarithmic transformed response times for a participant engaged in the task, and <span class="math inline">\(\beta\)</span> is the effect of trial difficulty on log-response time. We assume a non-deterministic process, with a noise parameter <span class="math inline">\(\sigma\)</span>. See also Box <a href="sec-trial.html#thm:lognormal">4.3</a> for more information about log-normally distributed response times.</p>
<p>Alternatively, a participant that guesses in every trial would show a response time that is independent of the difficulty of the trial:</p>
<span class="math display">\[\begin{equation}
rt_n \sim LogNormal(\gamma, \sigma_2)
\end{equation}\]</span>
<p>Here <span class="math inline">\(\gamma\)</span> represents the the average logarithmic transformed response time when a participant only guesses. We assume that responses from the guess mode might have a different noise component than from a task-engaged mode.</p>
<p>The fast guess model makes the assumption that during a task, a single participant would behave in these two ways: They would be engaged in the task a proportion of the trials and would guess on the rest of the trials. This means that for a single participant, there is an underlying probability of being engaged in the task, <span class="math inline">\(p_{task}\)</span>, that determines whether they are actually choosing (<span class="math inline">\(z=1\)</span>) or guessing (<span class="math inline">\(z=0\)</span>):</p>
<span class="math display">\[\begin{equation}
z_n \sim Bernoulli(p_{task})
\end{equation}\]</span>
<p>The value of the parameter <span class="math inline">\(z\)</span> in every trial determines the behavior of the participant. This means that the distribution that we observe is a mixture of the two distributions presented before:</p>
<span class="math display" id="eq:dismix">\[\begin{equation}
rt_n \sim 
\begin{cases}
LogNormal(\alpha + \beta \cdot x_n, \sigma), &amp; \text{ if } z_n =1 \\
LogNormal(\gamma, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\tag{15.1}
\end{equation}\]</span>
<p>In order to have a Bayesian implementation, we also need to define some priors. We use priors that encode what we know about reaction time experiments; see also <a href="sec-trial.html#sec:trial">4.2</a>.</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\alpha &amp;\sim Normal(6, 1)\\
\beta &amp;\sim Normal(0, .1)\\
\sigma &amp;\sim Normal_+(.5, .2)
\end{aligned}
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
\begin{aligned}
\gamma &amp;\sim Normal(6, 1)\\
\sigma_2 &amp;\sim Normal_+(.5, .2)
\end{aligned}
\end{equation}\]</span>
<p>For now, we don't commit to any value for the probability of having an engaged response by setting the following prior to <span class="math inline">\(p_{task}\)</span>:</p>
<span class="math display">\[\begin{equation}
p_{task} \sim Beta(1, 1)
\end{equation}\]</span>
<p>This represents a flat prior over probabilities, <span class="math inline">\(p_{task}\)</span> is equally likely to be any number between 0 and 1.</p>
<p>Before we fit our model to the real data, we generate synthetic data to make sure that our model is working as expected. We follow <span class="citation">Cook, Gelman, and Rubin (<a href="#ref-Cooketal2006">2006</a>)</span>, and for now we are going to verify that our model is roughly correct <span class="citation">(a more thorough approach is presented in Talts et al. <a href="#ref-talts2018validating">2018</a>; and Schad, Betancourt, and Vasishth <a href="#ref-schad2020toward">2020</a>)</span>. We are going to generate 1000 observations, where we know the true values of the parameters.</p>
<p>We first define the number of observations, predictors, and true values. We assume 1000 observations and two levels of difficulty -.5 and .5. The values of the parameters are relatively realistic (based on our previous experience on reaction time experiments). Notice that while in the priors we try to encode the range of possible values for the parameters, in this simulation we assume only one instance of this possible range:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">1000</span>
x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="op">-</span>.<span class="dv">5</span>, N<span class="op">/</span><span class="dv">2</span>), <span class="kw">rep</span>(.<span class="dv">5</span>, N<span class="op">/</span><span class="dv">2</span>))
<span class="co"># Parameters true values</span>
alpha &lt;-<span class="st"> </span><span class="fl">5.8</span>
beta &lt;-<span class="st"> </span><span class="fl">0.05</span>
sigma &lt;-<span class="st"> </span><span class="fl">.4</span>
sigma2 &lt;-<span class="st"> </span><span class="fl">.5</span>
gamma &lt;-<span class="st"> </span><span class="fl">5.2</span>
p_task &lt;-<span class="st"> </span><span class="fl">.8</span>
<span class="co"># Median time</span>
<span class="kw">c</span>(<span class="st">&quot;engaged&quot;</span> =<span class="st"> </span><span class="kw">exp</span>(alpha), <span class="st">&quot;guessing&quot;</span> =<span class="st"> </span><span class="kw">exp</span>(gamma))</code></pre></div>
<pre><code>##  engaged guessing 
##      330      181</code></pre>
<p>Generate response times:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z &lt;-<span class="st"> </span>extraDistr<span class="op">::</span><span class="kw">rbern</span>(<span class="dt">n =</span> N, <span class="dt">prob =</span> p_task)
rt &lt;-<span class="st"> </span><span class="kw">if_else</span>(z <span class="op">==</span><span class="st"> </span><span class="dv">1</span>,
             <span class="kw">rlnorm</span>(<span class="dt">n =</span> N, <span class="dt">meanlog =</span> alpha <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>x, <span class="dt">sdlog =</span> sigma),
             <span class="kw">rlnorm</span>(<span class="dt">n =</span> N, <span class="dt">meanlog =</span> gamma, <span class="dt">sdlog =</span> sigma2))
df_dots_simdata1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">trial =</span> <span class="dv">1</span><span class="op">:</span>N, <span class="dt">x =</span> x, <span class="dt">rt =</span> rt)</code></pre></div>
<p>We verify that our simulated data is realistic, that is, it's on the same range as the original data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_simdata1, <span class="kw">aes</span>(rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-405-1.svg" width="672" /></p>
<p>To implement the mixture model defined in Eq. <a href="sec-ppd.html#eq:logpriorsunif">(3.8)</a> in Stan, the discrete parameter <span class="math inline">\(z\)</span> needs to be marginalized:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
p(rt_n | \Theta) &amp;= p_{task} \cdot LogNormal(rt_n | \alpha + \beta \cdot x_n, \sigma) +\\ 
    &amp; (1 - p_{task}) \cdot LogNormal(rt_n | \gamma, \sigma_2)
\end{aligned}
\end{equation}\]</span>
In addition, we need to work in log-space, taking into account that Stan defines log(PDF) rather than PDFs:
<span class="math display">\[\begin{equation}
\begin{aligned}
\log(p(rt | \Theta)) &amp;= \log(p_{task} \cdot LogNormal(rt_n | \alpha + \beta * x_n, \sigma) +\\ 
    &amp; (1 - p_{task}) \cdot LogNormal(rt_n | \gamma, \sigma_2)) \\
&amp;= \log( \exp( \log(p_{task}) +  \log(LogNormal(rt_n | \alpha + \beta * x_n, \sigma))) +\\ 
    &amp; \exp( \log(1 - p_{task}) + \log(LogNormal(rt_n | \gamma, \sigma_2)))) \\
\end{aligned}
\end{equation}\]</span>
<p>In Stan this translates into:</p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] x;
  vector[N] rt;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
  real gamma; //guessing
  real&lt;lower = 0&gt; sigma2;
  real&lt;lower = 0, upper = 1&gt; p_task;
}
model {
  // priors for the task component
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .1);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  // priors for the guessing component
  target += normal_lpdf(gamma | 6, 1);
  target += normal_lpdf(sigma2 | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += beta_lpdf(p_task | 1, 1);
  for(n in 1:N)
    target += log_sum_exp(log(p_task) + lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma),
                          log1m(p_task) + lognormal_lpdf(rt[n] | gamma, sigma2));
}</code></pre>
<p>In the previous code, we use <code>log_sum_exp(x, y)</code> and <code>log1m(x)</code> since they are more computationally stable than <code>log(exp(x) + exp(y))</code> and <code>log(1-x)</code> respectively. That is, they are less prone to numerical over/underflows.</p>
<p>Call the Stan model <code>stan_models/mixture1.stan</code>, and fit it to the simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls_dots_simdata &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> N,
                        <span class="dt">rt =</span> rt,
                        <span class="dt">x =</span> x) </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_mix_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mixture1.stan&#39;</span>,
               <span class="dt">data =</span> ls_dots_simdata)   </code></pre></div>
<pre><code>## Warning: The largest R-hat is 1.74, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>There a lot of warnings, the Rhats are too large, and number of effective samples is too low:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_mix_<span class="dv">1</span>) </code></pre></div>
<pre><code>##            mean     2.5%    97.5% n_eff Rhat
## alpha      5.57     4.94     5.82     3 1.71
## beta       0.06    -0.09     0.16    45 1.05
## sigma      0.48     0.33     0.66     2 2.24
## gamma      5.54     4.91     5.83     3 1.88
## sigma2     0.47     0.32     0.65     3 2.09
## p_task     0.53     0.09     0.91     3 1.54
## lp__   -6349.94 -6354.70 -6346.88    10 1.14</code></pre>
<p>A traceplot shows clearly that the chains aren't mixing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">traceplot</span>(fit_mix_<span class="dv">1</span>) </code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-409-1.svg" width="672" /></p>
<p>The problem with this model is that the mixture components are underlyingly exchangeable and thus not identifiable. Each chain doesn't know how each component was identified by the rest of the chains. A major problem is that even though the theoretical model assumes that guesses are faster than engaged responses, this is not explicit in our computational model. That is, our model lacks some of the theoretical information that we have, namely that the distribution of guesses times is faster than the distribution of engaged reaction times. This can be encoded with a strong prior for <span class="math inline">\(\gamma\)</span>, where we assume that its prior distribution is truncated on a lower bound by the value of <span class="math inline">\(\alpha\)</span>:</p>
<span class="math display">\[\begin{equation}
\gamma \sim Normal(6, 1), \text{for } \gamma &gt; \alpha
\end{equation}\]</span>
<p>Another softer constraint that we could add to our implementation is the assumption that participants are generally trying to do the task more likely than just guessing. The following prior has more probability mass closer to 1 than to 0:</p>
<span class="math display">\[\begin{equation}
p_{task} \sim Beta(8, 2)
\end{equation}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="cf">function</span>(x) <span class="kw">dbeta</span>(x, <span class="dv">8</span>, <span class="dv">2</span>))</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-410-1.svg" width="672" /></p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] x;
  vector[N] rt;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
  real&lt;upper = alpha&gt; gamma; //guessing
  real&lt;lower = 0&gt; sigma2;
  real&lt;lower = 0, upper = 1&gt; p_task;
}
model {
  // priors for the task component
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .3);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  // priors for the guessing component
  target += normal_lpdf(gamma | 6, 1) -
    normal_lcdf(alpha | 6, 1);
  target += normal_lpdf(sigma2 | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += beta_lpdf(p_task | 8, 2);
  for(n in 1:N)
    target += log_sum_exp(log(p_task) + lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma),
                          log1m(p_task) + lognormal_lpdf(rt[n] | gamma, sigma2)) ;
}</code></pre>
<p>Notice that once we change the higher boundary of <code>gamma</code> we also need to truncate the distribution in Stan by correcting the PDF with its CDF (rather than with the complement of the CDF as when we have a lower truncation); see also Box <a href="sec-pupil.html#thm:truncation">4.1</a>.</p>
<pre><code>  target += normal_lpdf(gamma | 6, 1) -
    normal_lcdf(alpha | 6, 1);</code></pre>
<p>Fit it to the same dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_mix_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mixture2.stan&#39;</span>, 
               <span class="dt">data =</span> ls_dots_simdata)  </code></pre></div>
<p>Now summaries and plots look fine.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_mix_<span class="dv">2</span>) </code></pre></div>
<pre><code>##            mean     2.5%    97.5% n_eff Rhat
## alpha      5.75     5.69     5.81  1599    1
## beta       0.08     0.02     0.15  2593    1
## sigma      0.41     0.36     0.45  1696    1
## gamma      5.24     4.76     5.53  1006    1
## sigma2     0.56     0.40     0.69  1278    1
## p_task     0.78     0.58     0.94  1262    1
## lp__   -6348.87 -6353.12 -6346.51  1201    1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">traceplot</span>(fit_mix_<span class="dv">2</span>) </code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-412-1.svg" width="672" /></p>
</div>
<div id="sec:multmix" class="section level4">
<h4><span class="header-section-number">15.1.1.3</span> A multivariate implementation of the model</h4>
<p>A problem with the previous implementation of the fast guess model is that we don't use the accuracy information. We can implement a closer version of the verbal description of the model: In particular we also want to model that in the guess mode accuracy is at chance level and that during the task-engaged mode accuracy approaches 100%.</p>
<p>This means that the mixture affects two pairs of distributions:</p>
<span class="math display">\[\begin{equation}
z_n \sim Bernoulli(p_{task})
\end{equation}\]</span>
<p>A response time distribution</p>
<span class="math display" id="eq:dismix2">\[\begin{equation}
rt_n \sim 
\begin{cases}
LogNormal(\alpha + \beta \cdot x_n, \sigma), &amp; \text{ if } z_n =1 \\
LogNormal(\gamma, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\tag{15.2}
\end{equation}\]</span>
<p>and an accuracy distribution</p>
<span class="math display" id="eq:dismix3">\[\begin{equation}
acc_n \sim 
\begin{cases}
Bernoulli(p_{correct}), &amp; \text{ if } z_n =1 \\
Bernoulli(.5), &amp; \text{ if } z_n=0
\end{cases}
\tag{15.3}
\end{equation}\]</span>
<p>We have a new parameter <span class="math inline">\(p_{correct}\)</span>, which represent the probability of making a correct answer in the engaged mode. The verbal description says that it's closer to 100%, and here we have freedom to choose whatever prior represents for us close to 100%. We interpret this as follows, but notice that this is not a hard constraint, and if a participant consistently shows lower (or higher) accuracy, <span class="math inline">\(p_{correct}\)</span> will change:</p>
<span class="math display">\[\begin{equation}
p_{correct} \sim Beta(995, 5)
\end{equation}\]</span>
<p>In our simulated data, we assume that the global motion detection task is done by a very accurate participant, with an accuracy of 99.9%. <!-- (But see exercise \@ref(ex) to see what happens when this is not the case) -->.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p_correct &lt;-<span class="st"> </span><span class="fl">.999</span>
acc &lt;-<span class="st"> </span><span class="kw">ifelse</span>(z, <span class="kw">rbern</span>(<span class="dt">n =</span> N, p_correct),
                  <span class="kw">rbern</span>(<span class="dt">n =</span> N, <span class="fl">.5</span>))
df_dots_simdata3 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">trial =</span> <span class="dv">1</span><span class="op">:</span>N,
                           <span class="dt">x =</span> x,
                           <span class="dt">rt =</span> rt,
                           <span class="dt">acc =</span> acc) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff =</span> <span class="kw">if_else</span>(x <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>))</code></pre></div>
<p>We plot again our simulated data, and this time we can see the effect of task difficulty on the simulated response times and accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_simdata3, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(acc), <span class="dt">y =</span> rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">.4</span>, <span class="dt">height =</span> <span class="dv">0</span>),
             <span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(diff <span class="op">~</span><span class="st"> </span>.) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-414-1.svg" width="672" /></p>
<p>We need now to marginalize the discrete parameters from both pairs of distributions.</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
p(rt | \Theta) = &amp; p_{task} \cdot \\
&amp; LogNormal(rt_n | \alpha + \beta \cdot x_n, \sigma) \cdot \\
&amp; Bernoulli(acc_n | p_{correct}) \\
&amp; +\\ 
&amp; (1 - p_{task}) \cdot \\
&amp; LogNormal(rt_n | \gamma, \sigma_2) \cdot\\
&amp; Bernoulli(acc_n | .5)
\end{aligned}
\end{equation}\]</span>
<p>In log-space:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\log(p(rt | \Theta)) =  \log(\exp(&amp;\\
&amp; \log(p_{task}) +\\
  &amp;\log(LogNormal(rt_n | \alpha + \beta * x_n, \sigma)) + \\
  &amp;\log(Bernoulli(acc_n | p_{correct})))\\
  +&amp;\\ 
 \exp(&amp;\\
 &amp; \log(1 - p_{task}) + \\
 &amp; \log(LogNormal(rt_n |\gamma, \sigma_2)) + \\
 &amp; \log(Bernoulli(acc_n | .5)))\\
    )&amp; \\
\end{aligned}
\end{equation}\]</span>
<p>Our model translates into the following Stan code:</p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] x;
  vector[N] rt;
  int acc[N];
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
  real&lt;upper = alpha&gt; gamma; //guessing
  real&lt;lower = 0&gt; sigma2;
  real&lt;lower = 0, upper = 1&gt; p_correct;
  real&lt;lower = 0, upper = 1&gt; p_task;
}
model {
  // priors for the task component
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .3);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  // priors for the guessing component
  target += normal_lpdf(gamma | 6, 1);
  target += normal_lpdf(sigma2 | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += beta_lpdf(p_correct | 995, 5);
  target += beta_lpdf(p_task | 8, 2);
  for(n in 1:N)
    target += log_sum_exp(log(p_task) +
                          lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma) +
                          bernoulli_lpmf(acc[n] | p_correct),
                          log1m(p_task) +
                          lognormal_lpdf(rt[n] | gamma, sigma2) +
                          bernoulli_lpmf(acc[n] | .5));
}</code></pre>
<p>Save it as <code>stan_models/mixture3.stan</code> and fit it to also accuracy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls_dots_simdata &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> N,
                        <span class="dt">rt =</span> rt,
                        <span class="dt">x =</span> x,
                        <span class="dt">acc =</span> acc)
fit_mix_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mixture3.stan&#39;</span>,
               <span class="dt">data =</span> ls_dots_simdata) </code></pre></div>
<p>We see that our model can be fit to both response times and accuracy now, and its parameters estimates have sensible values (given our simulated data).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_mix_<span class="dv">3</span>)</code></pre></div>
<pre><code>##               mean     2.5%    97.5% n_eff Rhat
## alpha         5.78     5.74     5.81  4045    1
## beta          0.07     0.01     0.13  5539    1
## sigma         0.40     0.38     0.42  4742    1
## gamma         5.15     5.06     5.25  4268    1
## sigma2        0.50     0.44     0.56  4513    1
## p_correct     0.99     0.99     1.00  4314    1
## p_task        0.80     0.76     0.83  4857    1
## lp__      -6633.52 -6638.15 -6630.87  1966    1</code></pre>
<p>Before we extend this model hierarchically we will account for the instructions given to the participant in the next section.</p>
</div>
<div id="an-implementation-of-the-model-that-accounts-for-instructions" class="section level4">
<h4><span class="header-section-number">15.1.1.4</span> An implementation of the model that accounts for instructions</h4>
<p>The actual global motion detection experiment that we started from has another manipulation that can help us to evaluate better the fast guess model. In some trials, the instructions emphasized accuracy (e.g., &quot;Be as accurate as possible.&quot;) and in others speed (e.g., &quot;Be as fast as possible.&quot;). The fast guess model also assumes that the probability of being in one of the two states depend on the speed incentives given during the instructions. This entails that now <span class="math inline">\(p_{task}\)</span> depends on the instructions <span class="math inline">\(x_2\)</span>, where we encode a speed incentive with <span class="math inline">\(-.5\)</span> and an accuracy incentive with <span class="math inline">\(.5\)</span>. Essentially we need to fit the following regression:</p>
<span class="math display">\[\begin{equation}
\alpha_{task} + x_2 \cdot \beta_{task}
\end{equation}\]</span>
<p>As we did in <a href="multinomial-processing-tree-mpt-models.html#sec:MPT-reg">14.2.1.4</a>, we need to bound the previous regression between 0 and 1, we achieve this using the logistic or inverse logit function:</p>
<span class="math display">\[\begin{equation}
p_{task} = logit^{-1}(\alpha_{task} + x_2 \cdot \beta_{task})
\end{equation}\]</span>
<p>This means that we need to interpret <span class="math inline">\(\alpha_{task} + x_2 \cdot \beta_{task}\)</span> in log-odds bounded by <span class="math inline">\((-\infty, \infty)\)</span> rather than as a probability; see also <a href="multinomial-processing-tree-mpt-models.html#sec:MPT-reg">14.2.1.4</a> in the previous chapter.</p>
<p>The likelihood defined before in <a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#sec:multmix">15.1.1.3</a> remains the same, and the only further change in our model is that rather than a prior on <span class="math inline">\(p_{task}\)</span> we need now priors for <span class="math inline">\(\alpha_{task}\)</span> and <span class="math inline">\(\beta_{task}\)</span>.</p>
<p>For <span class="math inline">\(\beta_{task}\)</span>, we assume an effect that can rather large and we won't assume a direction a prior (for now):</p>
<span class="math display">\[\begin{equation}
\beta \sim Normal(0, 1)
\end{equation}\]</span>
<p>This means that the participant could be affected by the instructions in the expected way with better accuracy in the task when the instructions emphasize accuracy (<span class="math inline">\(\beta &gt;0\)</span>), or the participant might be behaving in an unexpected way with accuracy degrading when accuracy is emphasized (<span class="math inline">\(\beta &lt;0\)</span>); <span class="math inline">\(\beta &lt;0\)</span> could represent a participant that misunderstands the instructions. It's certainly possible to include priors that encode the expected direction of the effect instead.</p>
<p>How can we choose a prior for <span class="math inline">\(\alpha_{task}\)</span> that encodes the same information that we had in the previous model in <span class="math inline">\(p_{task}\)</span>? One possibility is to create an auxiliary parameter <span class="math inline">\(p_{btask}\)</span>, that represents the baseline probability of being engaged in the task, with the same prior that we use in the previous section, and then transform it to an unconstrained space for our regression with the logit function:</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
&amp;p_{btask} \sim Beta(8, 2)\\
&amp;\alpha_{task} = logit(p_{btask})
\end{aligned}
\end{equation}\]</span>
<p>To verify that our priors make sense, we plot the difference in prior predicted probability of being engaged in the task under the two emphasis conditions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ns &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># number of samples for the plot</span>
<span class="co"># Priors</span>
p_btask &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="dt">n =</span> Ns, <span class="dt">shape1 =</span> <span class="dv">8</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)
beta_task &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> Ns, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
<span class="co"># Predicted probability of being engaged</span>
p_task_easy &lt;-<span class="st"> </span><span class="kw">plogis</span>(<span class="kw">qlogis</span>(p_btask) <span class="op">+</span><span class="st"> </span><span class="fl">.5</span> <span class="op">*</span><span class="st"> </span>beta_task)
p_task_hard &lt;-<span class="st"> </span><span class="kw">plogis</span>(<span class="kw">qlogis</span>(p_btask) <span class="op">+</span><span class="st"> </span><span class="fl">-.5</span> <span class="op">*</span><span class="st"> </span>beta_task)
<span class="co"># Predicted difference</span>
diff_p_pred &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">diff =</span> p_task_easy <span class="op">-</span><span class="st"> </span>p_task_hard)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">diff_p_pred <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(diff)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_histogram</span>()</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-418-1.svg" width="672" /></p>
<p>The previous plot shows that we are predicting a priori that the difference in <span class="math inline">\(p_{task}\)</span> will be mostly smaller than <span class="math inline">\(.3\)</span>, which seems to make sense.</p>
<p>We are ready to generate a new dataset, by deciding on true values for <span class="math inline">\(\beta_{task}\)</span> and <span class="math inline">\(p_{btask}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># New predictor</span>
x2 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>, <span class="fl">.5</span>), N<span class="op">/</span><span class="dv">2</span>)
<span class="co"># We verify that the predictors are crossed:</span>
predictors &lt;-<span class="st"> </span><span class="kw">tibble</span>(x, x2)
<span class="kw">head</span>(predictors, <span class="dv">4</span>)</code></pre></div>
<pre><code>## # A tibble: 4 x 2
##       x    x2
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  -0.5  -0.5
## 2  -0.5   0.5
## 3  -0.5  -0.5
## 4  -0.5   0.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(predictors, <span class="dv">4</span>)</code></pre></div>
<pre><code>## # A tibble: 4 x 2
##       x    x2
##   &lt;dbl&gt; &lt;dbl&gt;
## 1   0.5  -0.5
## 2   0.5   0.5
## 3   0.5  -0.5
## 4   0.5   0.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># New true values</span>
beta_task &lt;-<span class="st"> </span><span class="fl">0.5</span>
p_btask &lt;-<span class="st"> </span><span class="fl">.85</span>
<span class="co"># Generate data:</span>
alpha_task &lt;-<span class="st"> </span><span class="kw">qlogis</span>(p_task)
p_task &lt;-<span class="st"> </span><span class="kw">plogis</span>(alpha_task <span class="op">+</span><span class="st"> </span>x2 <span class="op">*</span><span class="st"> </span>beta_task)
z &lt;-<span class="st"> </span><span class="kw">rbern</span>(<span class="dt">n =</span> N, <span class="dt">prob =</span> p_task)
rt &lt;-<span class="st"> </span><span class="kw">ifelse</span>(z, <span class="kw">rlnorm</span>(<span class="dt">n =</span> N, <span class="dt">meanlog =</span> alpha <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>x, <span class="dt">sdlog =</span> sigma),
             <span class="kw">rlnorm</span>(<span class="dt">n =</span> N, <span class="dt">meanlog =</span> gamma, <span class="dt">sdlog =</span> sigma2))
acc &lt;-<span class="st"> </span><span class="kw">ifelse</span>(z, <span class="kw">rbern</span>(<span class="dt">n =</span> N, p_correct),
                  <span class="kw">rbern</span>(<span class="dt">n =</span> N, <span class="fl">.5</span>))

df_dots_simdata4 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">trial =</span> <span class="dv">1</span><span class="op">:</span>N, <span class="dt">x =</span> x, <span class="dt">rt =</span> rt, <span class="dt">acc =</span> acc, <span class="dt">x2 =</span> x2) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">diff =</span> <span class="kw">if_else</span>(x <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>),
         <span class="dt">emphasis =</span> <span class="kw">ifelse</span>(x2 <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;speed&quot;</span>))</code></pre></div>
<p>We can generate a plot now were both the difficulty of the task and the instructions are manipulated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_simdata4, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(acc), <span class="dt">y =</span> rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">.4</span>, <span class="dt">height =</span> <span class="dv">0</span>),
             <span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(diff <span class="op">~</span><span class="st"> </span>emphasis) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-420-1.svg" width="672" /></p>
<p>For the Stan implementation, we added a generated quantities plots that can be used for further (prior or posterior) predictive checks. We use the dummy variable <code>onlyprior</code> to indicate whether we use the data or we only sample from the priors. Notice that one can always do the predictive checks in R, transforming the code that we wrote for the simulation into a function, and writing the priors in R. However, it can be simpler to take advantage of Stan output format and rewrite the code in Stan. One downside of this, is that the <code>stanfit</code> object that stores the model output can become too large for the memory of the computer.</p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] x;
  vector[N] rt;
  int acc[N];
  vector[N] x2; //speed or accuracy emphasis
  int&lt;lower = 0, upper = 1&gt; onlyprior;
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
  real&lt;upper = alpha&gt; gamma; //guessing
  real&lt;lower = 0&gt; sigma2;
  real&lt;lower = 0, upper = 1&gt; p_correct;
  real&lt;lower = 0, upper = 1&gt; p_btask;
  real beta_task;
}
model {
  // priors for the task component
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .1);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  // priors for the guessing component
  target += normal_lpdf(gamma | 6, 1);
  target += normal_lpdf(sigma2 | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += normal_lpdf(beta_task | 0, 1);
  target += beta_lpdf(p_correct | 995, 5);
  target += beta_lpdf(p_btask | 8, 2);
  if(onlyprior != 1)
    for(n in 1:N){
      real lodds_task = logit(p_btask) + x2[n] * beta_task;
      target += log_sum_exp(log_inv_logit(lodds_task)+
                            lognormal_lpdf(rt[n] | alpha + x[n] * beta, sigma) +
                            bernoulli_lpmf(acc[n] | p_correct),
                            log1m_inv_logit(lodds_task) +
                            lognormal_lpdf(rt[n] | gamma, sigma2) +
                            bernoulli_lpmf(acc[n] | .5));
    }
}
generated quantities {
  real rt_pred[N];
  real acc_pred[N];
  int z[N]; 
  for(n in 1:N){
    real lodds_task = logit(p_btask) + x2[n] * beta_task;
    z[n] = bernoulli_rng(inv_logit(lodds_task));
    if(z[n]==1){
      rt_pred[n] = lognormal_rng(alpha + x[n] * beta, sigma);
      acc_pred[n] = bernoulli_rng(p_correct);
    } else{
      rt_pred[n] = lognormal_rng(gamma, sigma2);
      acc_pred[n] = bernoulli_rng(.5);
    }
  }
}
</code></pre>
<p>In the Stan code shown above, <code>log_inv_logit(x)</code> is applying the logistic function to <code>x</code> to transform it in a probability and then applying the logarithm; <code>log1m_inv_logit(x)</code> is applying the logistic function to <code>x</code>, and then applying the logarithm to its complement <span class="math inline">\((1 - p)\)</span>.</p>
<p>We save the code as <code>stan_models/mixture4.stan</code>, and before fitting it to the simulated data, we perform prior predictive checks.</p>
<div id="prior-predictive-checks-of-the-fast-guess-model" class="section level5">
<h5><span class="header-section-number">15.1.1.4.1</span> Prior predictive checks of the fast guess model</h5>
<p>We generate prior predictive distributions, by setting only prior to <code>1</code>. (Ignore the warnings, since we are not fitting the model to the data).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls_dots_simdata &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> N,
                        <span class="dt">rt =</span> rt,
                        <span class="dt">x =</span> x,
                        <span class="dt">x2 =</span> x2,
                        <span class="dt">acc =</span> acc,
                        <span class="dt">onlyprior =</span> <span class="dv">1</span>) 
fit_mix_<span class="dv">4</span>_priors &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mixture4.stan&#39;</span>,
                         <span class="dt">data =</span> ls_dots_simdata,
                         <span class="dt">chains =</span> <span class="dv">1</span>, <span class="dt">iter =</span> <span class="dv">1000</span>)  </code></pre></div>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<p>We plot prior predictive distributions of response times as follows. Notice that we are plotting them again our simulated data, by setting <code>y = rt</code> in <code>ppc_dens_overlay</code>, this distribution can be thought as a hand-picked instance of the prior predictive distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rt_pred &lt;-<span class="st"> </span><span class="kw">extract</span>(fit_mix_<span class="dv">4</span>_priors)<span class="op">$</span>rt_pred
<span class="kw">ppc_dens_overlay</span>(<span class="dt">y =</span> rt, <span class="dt">yrep =</span> rt_pred[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,]) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10000</span>))</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-422-1.svg" width="672" /> We see that we tend to generate some responses that are too large, but the general shape of the predictive distribution of the response times is fine.</p>
<p>If we want to plot the prior predicted distribution of differences in response time conditioning on task difficulty, we need to define a new function. Then we use the <code>bayesplot</code> function <code>ppc_stat()</code> that takes as an argument of <code>stat</code> any summary function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meanrt_diff &lt;-<span class="st"> </span><span class="cf">function</span>(rt){
  <span class="kw">mean</span>(rt[x <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(rt[x <span class="op">==</span><span class="st"> </span><span class="fl">-.5</span>])
}
<span class="kw">ppc_stat</span>(<span class="dt">y =</span> rt, <span class="dt">yrep =</span> rt_pred, <span class="dt">stat =</span> meanrt_diff) </code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-423-1.svg" width="672" /></p>
<!-- We can inspect the prior predictive distribution of accuracy by grouping accuracy by emphasis: -->
<p>We find that the range of response times look reasonable. There are, however, always more checks that can be done, for example, plotting other summary statistics, or predictions conditioned on other aspects of the data.</p>
</div>
<div id="fit-to-simulated-data" class="section level5">
<h5><span class="header-section-number">15.1.1.4.2</span> Fit to simulated data</h5>
<p>Fit it to data, by setting <code>onlyprior = 0</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls_dots_simdata &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> N,
                        <span class="dt">rt =</span> rt,
                        <span class="dt">x =</span> x,
                        <span class="dt">x2 =</span> x2,
                        <span class="dt">acc =</span> acc,
                        <span class="dt">onlyprior =</span> <span class="dv">0</span>) 
fit_mix_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mixture4.stan&#39;</span>,
                  <span class="dt">data =</span> ls_dots_simdata) </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_mix_<span class="dv">4</span>,
      <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,
               <span class="st">&quot;p_correct&quot;</span>, <span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>))</code></pre></div>
<pre><code>##           mean 2.5% 97.5% n_eff Rhat
## alpha     5.78 5.75  5.82  4959    1
## beta      0.06 0.00  0.12  7321    1
## sigma     0.42 0.39  0.44  5655    1
## gamma     5.21 5.11  5.31  4647    1
## sigma2    0.50 0.44  0.57  5522    1
## p_correct 1.00 0.99  1.00  5518    1
## p_btask   0.83 0.79  0.86  4806    1
## beta_task 0.71 0.26  1.19  6309    1</code></pre>
<p>We see that we fit the model without problems. Before we evaluate the recovery of the parameters more carefully, we implement a hierarchical version of the fast guesses model.</p>
</div>
</div>
<div id="a-hierarchical-implementation-of-the-fast-guesses-model" class="section level4">
<h4><span class="header-section-number">15.1.1.5</span> A hierarchical implementation of the fast guesses model</h4>
<p>So far we have evaluated the behavior of one simulated participant. As we discussed in <a href="sec-N400hierarchical.html#sec:distrmodel">5.1.6</a> in the context of distributional regression models, every parameter in a model can be made hierarchical in a straight-forward way. This doesn't mean, however, that we are going to be able to estimate those parameters or that our model will converge. The best advice here is to start simple with simulated data. Despite the fact that convergence with simulated data does not guarantee the convergence of the same model with real data, the reverse is in general true.</p>
<p>For our hierarchical version, we assume that both response times in general and the effect of task difficulty vary by participant, and that different participants have different guess times. This entails the following change to the response time distribution:</p>
<span class="math display">\[\begin{equation}
rt_n \sim 
\begin{cases}
LogNormal(\alpha + u_{subj[n],1} +  x_n \cdot  (\beta +  u_{subj[n], 2}), \sigma), &amp; \text{ if } z_n =1 \\
LogNormal(\gamma + u_{subj[n], 3}, \sigma_2), &amp; \text{ if } z_n=0
\end{cases}
\end{equation}\]</span>
<p>We assume that the three vectors of <span class="math inline">\(u\)</span> (adjustment to the intercept and slope of the task-engaged distribution, and the adjustment to the guess time distribution) follow a multinormal distribution centered in zero. For simplicity and lack of more information, we assume the same prior distribution to the three variance components and the same prior for the two correlation between the adjustments (<span class="math inline">\(\rho_{u_{1,2}}, \rho_{u_{1,3}}, \rho_{u_{2,3}}\)</span>):</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
u &amp;\sim N(0, \Sigma_u)\\
tau_{u_{1..3}} &amp; \sim  Normal_+(0, .5)\\
\rho_u &amp;\sim LKJcorr(2) 
\end{aligned}
\end{equation}\]</span>
<p>Before we fit the model to the real dataset, we simulate data again; this time we simulate 100 trials of each of 20 subjects.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We build the fake stimuli</span>
N_subj &lt;-<span class="st"> </span><span class="dv">20</span>
N_trials &lt;-<span class="st"> </span><span class="dv">100</span>
<span class="co"># Parameters true values</span>
alpha &lt;-<span class="st"> </span><span class="fl">5.8</span>
beta &lt;-<span class="st"> </span><span class="fl">0.05</span>
sigma &lt;-<span class="st"> </span><span class="fl">.4</span>
sigma2 &lt;-<span class="st"> </span><span class="fl">.5</span>
gamma &lt;-<span class="st"> </span><span class="fl">5.2</span>
beta_task &lt;-<span class="st"> </span><span class="fl">0.1</span>
p_btask &lt;-<span class="st"> </span><span class="fl">.85</span>
alpha_task &lt;-<span class="st"> </span><span class="kw">qlogis</span>(p_btask)
p_correct &lt;-<span class="st"> </span><span class="fl">.999</span>
tau_u &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">2</span>, <span class="fl">.005</span>, <span class="fl">.3</span>)
rho &lt;-<span class="st"> </span><span class="fl">.3</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## We build the stimuli here:
N &lt;-<span class="st"> </span>N_subj <span class="op">*</span><span class="st"> </span>N_trials
stimuli &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">rep</span>(<span class="kw">c</span>(<span class="kw">rep</span>(<span class="op">-</span>.<span class="dv">5</span>,N_trials<span class="op">/</span><span class="dv">2</span>), <span class="kw">rep</span>(.<span class="dv">5</span>, N_trials<span class="op">/</span><span class="dv">2</span>)), N_subj),
                  <span class="dt">x2 =</span> <span class="kw">rep</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>,.<span class="dv">5</span>), N_trials<span class="op">/</span><span class="dv">2</span>), N_subj),
                  <span class="dt">subj =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_subj, <span class="dt">each =</span> N_trials),
                  <span class="dt">trial =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>N_trials, N_subj)
                  )
stimuli</code></pre></div>
<pre><code>## # A tibble: 2,000 x 4
##       x    x2  subj trial
##   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;
## 1  -0.5  -0.5     1     1
## 2  -0.5   0.5     1     2
## 3  -0.5  -0.5     1     3
## 4  -0.5   0.5     1     4
## 5  -0.5  -0.5     1     5
## # … with 1,995 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Cor_u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(rho, <span class="dv">9</span>), <span class="dt">nrow =</span> <span class="dv">3</span>)
<span class="kw">diag</span>(Cor_u) &lt;-<span class="st"> </span><span class="dv">1</span>
Cor_u</code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]  1.0  0.3  0.3
## [2,]  0.3  1.0  0.3
## [3,]  0.3  0.3  1.0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Variance covariance matrix for &#39;subj&#39;:</span>
Sigma_u &lt;-<span class="st"> </span><span class="kw">diag</span>(tau_u, <span class="dv">3</span>, <span class="dv">3</span>) <span class="op">%*%</span><span class="st"> </span>Cor_u <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(tau_u, <span class="dv">3</span>, <span class="dv">3</span>)
<span class="co"># Create the correlated adjustments </span>
u &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> N_subj, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), Sigma_u)
<span class="co"># Check if they are correctly correlated</span>
<span class="kw">cor</span>(u)</code></pre></div>
<pre><code>##        [,1]   [,2]  [,3]
## [1,] 1.0000 0.0202 0.310
## [2,] 0.0202 1.0000 0.435
## [3,] 0.3102 0.4348 1.000</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#(there is random variation), and the SD</span>
<span class="kw">list</span>(<span class="kw">sd</span>(u[, <span class="dv">1</span>]), <span class="kw">sd</span>(u[, <span class="dv">2</span>]), <span class="kw">sd</span>(u[, <span class="dv">3</span>]))</code></pre></div>
<pre><code>## [[1]]
## [1] 0.19
## 
## [[2]]
## [1] 0.0058
## 
## [[3]]
## [1] 0.245</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create the data</span>
df_dots_simdata &lt;-<span class="st"> </span>stimuli <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">z =</span> <span class="kw">rbern</span>(<span class="dt">n =</span> N, <span class="dt">prob =</span> <span class="kw">plogis</span>(alpha_task <span class="op">+</span><span class="st"> </span>x2 <span class="op">*</span><span class="st"> </span>beta_task)),
         <span class="dt">rt =</span> <span class="kw">ifelse</span>(z,
             <span class="kw">rlnorm</span>(<span class="dt">n =</span> N, <span class="dt">meanlog =</span> alpha <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">1</span>] <span class="op">+</span>
<span class="st">                             </span>(beta <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">2</span>]) <span class="op">*</span><span class="st"> </span>x, <span class="dt">sdlog =</span> sigma),
             <span class="kw">rlnorm</span>(<span class="dt">n =</span> N, <span class="dt">meanlog =</span> gamma <span class="op">+</span><span class="st"> </span>u[subj, <span class="dv">3</span>], <span class="dt">sdlog =</span> sigma2)),
         <span class="dt">acc =</span> <span class="kw">ifelse</span>(z, <span class="kw">rbern</span>(<span class="dt">n =</span> N, p_correct),
              <span class="kw">rbern</span>(<span class="dt">n =</span> N, <span class="fl">.5</span>)),
         <span class="dt">diff =</span> <span class="kw">if_else</span>(x <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;hard&quot;</span>, <span class="st">&quot;easy&quot;</span>),
         <span class="dt">emphasis =</span> <span class="kw">ifelse</span>(x2 <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>, <span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;speed&quot;</span>))</code></pre></div>
<p>We verify that the distribution of the simulated response times conditional on the simulated accuracy and the experimental manipulations make sense with the following plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_simdata, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(acc), <span class="dt">y =</span> rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">.4</span>, <span class="dt">height =</span> <span class="dv">0</span>), <span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(diff <span class="op">~</span><span class="st"> </span>emphasis) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-427-1.svg" width="672" /></p>
<p>We implement the model in Stan as follows. The hierarchical extension uses the Cholesky factorization for the group-level effects (as we did in <a href="hierarchical-models-with-stan.html#sec:corrstan">12.1.3</a>).</p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] x;
  vector[N] rt;
  int acc[N];
  vector[N] x2; //speed or accuracy emphasis
  int&lt;lower = 1&gt; N_subj;
  int&lt;lower = 1, upper = N_subj&gt; subj[N];
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
  real&lt;upper = alpha&gt; gamma; //guessing
  real&lt;lower = 0&gt; sigma2;
  real&lt;lower = 0, upper = 1&gt; p_correct;
  real&lt;lower = 0, upper = 1&gt; p_btask;
  real beta_task;
  vector&lt;lower = 0&gt;[3]  tau_u;   
  matrix[3, N_subj] z_u;
  cholesky_factor_corr[3] L_u;
}
transformed parameters {
  matrix[N_subj, 3] u;
  u = (diag_pre_multiply(tau_u, L_u) * z_u)&#39;;
}
model {
  // priors for the task component
  target += normal_lpdf(alpha | 6, 1);
  target += normal_lpdf(beta | 0, .1);
  target += normal_lpdf(sigma | .5, .2)
    - normal_lccdf(0 | .5, .2);
  // priors for the guessing component
  target += normal_lpdf(gamma | 6, 1);
  target += normal_lpdf(sigma2 | .5, .2)
    - normal_lccdf(0 | .5, .2);
  target += normal_lpdf(tau_u | 0, .5)
    - 3* normal_lccdf(0 | 0, .5);
  target += normal_lpdf(beta_task | 0, 1);
  target += beta_lpdf(p_correct | 995, 5);
  target += beta_lpdf(p_btask | 8, 2);
  target += lkj_corr_cholesky_lpdf(L_u | 2);
  target += std_normal_lpdf(to_vector(z_u));

  for(n in 1:N){
    real lodds_task = logit(p_btask) + x2[n] * beta_task;
    target += log_sum_exp(log_inv_logit(lodds_task) +
                          lognormal_lpdf(rt[n] | alpha + u[subj[n], 1] +
                                         x[n] * (beta + u[subj[n], 2]), sigma) +
                          bernoulli_lpmf(acc[n] | p_correct),
                          log1m_inv_logit(lodds_task) +
                          lognormal_lpdf(rt[n] | gamma + u[subj[n], 3], sigma) +
                          bernoulli_lpmf(acc[n] |.5));
  }
}
generated quantities {
  corr_matrix[3] rho_u = L_u * L_u&#39;;
}</code></pre>
<p>Save it as <code>stan_models/mixtureh.stan</code> and fit it to the simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls_dots_simdata &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> N,
                          <span class="dt">rt =</span> df_dots_simdata<span class="op">$</span>rt,
                          <span class="dt">x =</span> df_dots_simdata<span class="op">$</span>x,
                          <span class="dt">x2 =</span> df_dots_simdata<span class="op">$</span>x2,
                          <span class="dt">acc =</span> df_dots_simdata<span class="op">$</span>acc,
                          <span class="dt">subj =</span> df_dots_simdata<span class="op">$</span>subj,
                          <span class="dt">N_subj =</span> N_subj)
fit_mix_h &lt;-<span class="st"> </span><span class="kw">stan</span>(<span class="dt">file =</span> <span class="st">&#39;stan_models/mixtureh.stan&#39;</span>,
                  <span class="dt">data =</span> ls_dots_simdata)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_mix_h, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,
                          <span class="st">&quot;p_correct&quot;</span>,<span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u&quot;</span>,
                          <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>, <span class="st">&quot;rho_u[2,3]&quot;</span>))</code></pre></div>
<pre><code>##             mean  2.5% 97.5% n_eff Rhat
## alpha       5.81  5.70  5.92   710 1.01
## beta        0.03 -0.02  0.08  5325 1.00
## sigma       0.41  0.40  0.43  6593 1.00
## gamma       5.18  5.00  5.35  2390 1.00
## sigma2      0.51  0.13  0.88  3483 1.00
## p_correct   0.99  0.99  1.00  5489 1.00
## p_btask     0.87  0.84  0.89  6514 1.00
## beta_task   0.21 -0.13  0.55  8539 1.00
## tau_u[1]    0.23  0.17  0.33  1028 1.00
## tau_u[2]    0.04  0.00  0.09  2222 1.00
## tau_u[3]    0.35  0.24  0.52  2163 1.00
## rho_u[1,2] -0.25 -0.85  0.58  4492 1.00
## rho_u[1,3]  0.19 -0.25  0.58  2571 1.00
## rho_u[2,3]  0.00 -0.70  0.70   501 1.01</code></pre>
<p>We see that we can fit the hierarchical extension of our model to simulated data. Next we'll evaluate whether we can recover the true values of the parameters.</p>
</div>
<div id="recovery-of-the-parameters" class="section level4">
<h4><span class="header-section-number">15.1.1.6</span> Recovery of the parameters</h4>
<p>By &quot;recovering&quot; the true values of the parameters, we mean that the true values are somewhere inside the bulk of the posterior distribution of the model.</p>
<p>We use <code>mcmc_recover_hist</code> to compare the posterior distributions of the relevant parameters of the model with their true values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df_fit_mix_h &lt;-<span class="st"> </span>fit_mix_h <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,
           <span class="st">&quot;p_correct&quot;</span>,<span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u[1]&quot;</span>,
           <span class="st">&quot;tau_u[2]&quot;</span>, <span class="st">&quot;tau_u[3]&quot;</span>, <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>,
           <span class="st">&quot;rho_u[2,3]&quot;</span>))
true_values &lt;-<span class="st"> </span><span class="kw">c</span>(alpha, beta, sigma, gamma, sigma2,
                p_correct, p_btask, beta_task, tau_u[<span class="dv">1</span>],
                tau_u[<span class="dv">2</span>], tau_u[<span class="dv">3</span>], <span class="kw">rep</span>(rho,<span class="dv">3</span>))
<span class="kw">mcmc_recover_hist</span>(df_fit_mix_h, true_values)</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-431-1.svg" width="672" /></p>
<p>The model seems to be underestimating the probability of being correct of the participants (<code>p_correct</code>) and overestimating the probability of being engaged in the task (<code>p_btask</code>). Notice, however that the numerical differences are very small. We can be relatively certain that the model is not seriously mis-specified, but A more principled approach using simulation based calibration is presented in <span class="citation">Talts et al. (<a href="#ref-talts2018validating">2018</a>)</span> and <span class="citation">Schad, Betancourt, and Vasishth (<a href="#ref-schad2020toward">2020</a>)</span>.</p>
<!-- We can verify that with enough data... -->
<div id="fitting-the-model-to-real-data" class="section level5">
<h5><span class="header-section-number">15.1.1.6.1</span> Fitting the model to real data</h5>
<p>After verifying that our model works as expected, we are ready to fit it to real data. We code the predictors <span class="math inline">\(x\)</span> and <span class="math inline">\(x_2\)</span> as we did for the simulated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df_dots_data &lt;-<span class="st"> </span>df_dots_data <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">if_else</span>(diff <span class="op">==</span><span class="st"> &quot;easy&quot;</span>, <span class="fl">-.5</span>, <span class="fl">.5</span>),
         <span class="dt">x2 =</span> <span class="kw">if_else</span>(emphasis <span class="op">==</span><span class="st"> &quot;accuracy&quot;</span>, <span class="fl">.5</span>, <span class="fl">-.5</span>))</code></pre></div>
<p>The main obstacle now is that fitting the entire dataset takes around 12 hours! If you want to get a taste of the fit of the model, you can sample 150 observations (from the ~2800) of each subject as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df_dots_data_short &lt;-<span class="st"> </span>df_dots_data <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(subject) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sample_n</span>(<span class="dv">150</span>)</code></pre></div>
<p>The complete model is fit as follows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ls_dots_data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">N =</span> <span class="kw">nrow</span>(df_dots_data),
                     <span class="dt">rt =</span> df_dots_data<span class="op">$</span>rt,
                     <span class="dt">x =</span> df_dots_data<span class="op">$</span>x,
                     <span class="dt">x2 =</span> df_dots_data<span class="op">$</span>x2,
                     <span class="dt">acc =</span> df_dots_data<span class="op">$</span>acc,
                     <span class="dt">subj =</span> <span class="kw">as.numeric</span>(df_dots_data<span class="op">$</span>subject),
                     <span class="dt">N_subj =</span> <span class="kw">length</span>(<span class="kw">unique</span>(df_dots_data<span class="op">$</span>subject)))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit_mix_data, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;alpha&quot;</span>, <span class="st">&quot;beta&quot;</span>, <span class="st">&quot;sigma&quot;</span>, <span class="st">&quot;gamma&quot;</span>, <span class="st">&quot;sigma2&quot;</span>,
                              <span class="st">&quot;p_correct&quot;</span>,<span class="st">&quot;p_btask&quot;</span>, <span class="st">&quot;beta_task&quot;</span>, <span class="st">&quot;tau_u&quot;</span>,
                              <span class="st">&quot;rho_u[1,2]&quot;</span>, <span class="st">&quot;rho_u[1,3]&quot;</span>, <span class="st">&quot;rho_u[2,3]&quot;</span>))</code></pre></div>
<pre><code>##            mean  2.5% 97.5% n_eff Rhat
## alpha      6.42  6.35  6.50   780 1.01
## beta       0.14  0.12  0.17  1242 1.00
## sigma      0.27  0.26  0.27  6142 1.00
## gamma      6.11  6.04  6.18  1190 1.00
## sigma2     0.50  0.12  0.90  2774 1.00
## p_correct  0.93  0.93  0.93  5011 1.00
## p_btask    0.92  0.90  0.94  3000 1.00
## beta_task  5.59  5.19  6.06  2835 1.00
## tau_u[1]   0.16  0.11  0.22  1042 1.01
## tau_u[2]   0.05  0.03  0.07  1384 1.00
## tau_u[3]   0.14  0.10  0.21  1279 1.00
## rho_u[1,2] 0.41  0.00  0.73  1870 1.00
## rho_u[1,3] 0.24 -0.19  0.61  1681 1.00
## rho_u[2,3] 0.05 -0.39  0.47  1457 1.00</code></pre>
<p>What can we say about the fit of the model? Our success fitting the fast guess model to real data doesn't imply that the model is a good account of the data. It just means that it's flexible enough. <em>Under the assumption that this model is true</em>, we can look at the parameters and conclude the following:</p>
<ul>
<li>Participants seemed to have a very high accuracy once they were engaged in the task. (<code>p_correct</code> is very high).</li>
<li>The instructions seemed to have a very strong effect on the mode of the participants (<code>beta_task</code> is very high).</li>
<li>The guess mode seemed to be much noisier than the task-engaged mode (compare <code>sigma</code> with <code>sigma2</code>).</li>
<li>Difference between participants (<code>tau</code> parameters) seem modest in comparison with the effect of the experimental manipulation (<code>beta</code>).</li>
<li>Slow participants seemed to show a stronger effect of the experimental manipulation (<code>rho_u1[1,2]</code> is mostly positive).</li>
</ul>
<p>If we want to know whether our model achieves descriptive adequacy, we need to look at the posterior predictive distributions of the model. However, by using posterior predictive checks, we won't be able to conclude that our model is not overfitting.</p>
<div id="posterior-predictive-checks" class="section level6">
<h6><span class="header-section-number">15.1.1.6.1.1</span> Posterior predictive checks</h6>
<p>For the posterior predictive checks, we can write the generated quantities block in a new file. The advantage is that we can generate as many observations as needed <em>after estimating the parameters</em>. Notice that there is no model block in the follow Stan program.</p>
<pre class="stan"><code>data {
  int&lt;lower = 1&gt; N;
  vector[N] x;
  vector[N] rt;
  int acc[N];
  vector[N] x2; //speed or accuracy emphasis
  int&lt;lower = 1&gt; N_subj;
  int&lt;lower = 1, upper = N_subj&gt; subj[N];
}
parameters {
  real alpha;
  real beta;
  real&lt;lower = 0&gt; sigma;
  real&lt;upper = alpha&gt; gamma; //guessing
  real&lt;lower = 0&gt; sigma2;
  real&lt;lower = 0, upper = 1&gt; p_correct;
  real&lt;lower = 0, upper = 1&gt; p_btask;
  real beta_task;
  vector&lt;lower = 0&gt;[3]  tau_u;   
  matrix[3, N_subj] z_u;
  cholesky_factor_corr[3] L_u;
  matrix[N_subj, 3] u;
}
generated quantities {
  real rt_pred[N];
  real acc_pred[N];
  int z[N];
  for(n in 1:N){
    real lodds_task = logit(p_btask) + x2[n] * beta_task;
    z[n] = bernoulli_rng(inv_logit(lodds_task));
    if(z[n]==1){
      rt_pred[n] = lognormal_rng(alpha + u[subj[n],1] +
                                 x[n] * (beta + u[subj[n], 2]), sigma);
      acc_pred[n] = bernoulli_rng(p_correct);
    } else{
      rt_pred[n] = lognormal_rng(gamma + u[subj[n], 3], sigma2);
      acc_pred[n] = bernoulli_rng(.5);
    }
  }
}</code></pre>
<p>Save the file as <code>stan_models/mixtureh_gen.stan</code>, and generate 500 simulated datasets as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gen_model &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="st">&quot;stan_models/mixtureh_gen.stan&quot;</span>)
draws_par &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(fit_mix_data)[<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>, ,drop =<span class="st"> </span><span class="ot">FALSE</span>]
gen_mix_data &lt;-<span class="st"> </span><span class="kw">gqs</span>(gen_model,
                    <span class="dt">data =</span> ls_dots_data,
                    <span class="dt">draws =</span> draws_par)</code></pre></div>
<p>We first take a look at the general distribution of response times generated by the posterior predictive model and by our real data in Figure <a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#fig:postpmix">15.2</a>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rt_pred &lt;-<span class="st"> </span><span class="kw">extract</span>(gen_mix_data)<span class="op">$</span>rt_pred
<span class="kw">ppc_dens_overlay</span>(<span class="dt">y =</span> ls_dots_data<span class="op">$</span>rt, <span class="dt">yrep =</span> rt_pred[<span class="dv">1</span><span class="op">:</span><span class="dv">100</span>,]) <span class="op">+</span>
<span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10000</span>))</code></pre></div>
<div class="figure"><span id="fig:postpmix"></span>
<img src="bookdown_files/figure-html/postpmix-1.svg" alt="Posterior predictive distribution of the hierarchical fast guess model in comparision with the observed response times." width="672" />
<p class="caption">
FIGURE 15.2: Posterior predictive distribution of the hierarchical fast guess model in comparision with the observed response times.
</p>
</div>
<p>We see that the distribution of the observed response times has heavier tails than the predictive distribution. This means that somewhere in our model we are failing to account for response time variability.</p>
<!-- We see that we tend to generate some responses that are too large, but the general shape of the predictive distribution of the response times is fine. -->
<!-- If we want to plot the prior predicted distribution of differences in response time conditioning on task difficulty, we need to define a new function. Then we use the `bayesplot` function `ppc_stat()` that takes as an argument of `stat` any summary function. -->
<p>Next we examine the effect of the experimental manipulation in Figure <a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#fig:postpmanip">15.3</a>: The model is underestimating the effect of the experimental manipulation and the observed difference between response times is well outside the bulk of the predictive distribution.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">meanrt_diff &lt;-<span class="st"> </span><span class="cf">function</span>(rt){
  <span class="kw">mean</span>(rt[ls_dots_data<span class="op">$</span>x <span class="op">==</span><span class="st"> </span><span class="fl">.5</span>]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(rt[ls_dots_data<span class="op">$</span>x <span class="op">==</span><span class="st"> </span><span class="fl">-.5</span>])
}
<span class="kw">ppc_stat</span>(<span class="dt">y =</span> ls_dots_data<span class="op">$</span>rt, <span class="dt">yrep =</span> rt_pred, <span class="dt">stat =</span> meanrt_diff) </code></pre></div>
<div class="figure"><span id="fig:postpmanip"></span>
<img src="bookdown_files/figure-html/postpmanip-1.svg" alt="Posterior predictive distribution of the difference in response time due to the experimental manipulation." width="672" />
<p class="caption">
FIGURE 15.3: Posterior predictive distribution of the difference in response time due to the experimental manipulation.
</p>
</div>
<p>We also look at some instances of the predictive distribution. Figure <a href="#fig:postppdobs"><strong>??</strong></a> shows to simulated datasets in red overlaid to the real observations in black. As we noticed in Figure <a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#fig:postpmix">15.2</a>, the model is predicting less variability than what we find in the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">acc_pred &lt;-<span class="st"> </span><span class="kw">extract</span>(gen_mix_data)<span class="op">$</span>acc_pred
df_dots_data &lt;-
<span class="st">  </span>df_dots_data <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">acc_pred1 =</span> acc_pred[<span class="dv">1</span>,], <span class="dt">rt_pred1 =</span> rt_pred[<span class="dv">1</span>,],
                          <span class="dt">acc_pred2 =</span> acc_pred[<span class="dv">2</span>,], <span class="dt">rt_pred2 =</span> rt_pred[<span class="dv">2</span>,])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df_dots_data, <span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(acc), <span class="dt">y =</span> rt)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">shape =</span> <span class="dv">0</span>,
             <span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">.4</span>, <span class="dt">height =</span> <span class="dv">0</span>),
             <span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">factor</span>(acc_pred1), <span class="dt">y =</span> rt_pred1), <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>,
             <span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">width =</span> <span class="fl">.4</span>, <span class="dt">height =</span> <span class="dv">0</span>),
             <span class="dt">alpha =</span> <span class="fl">.5</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">facet_wrap</span>(diff <span class="op">~</span><span class="st"> </span>emphasis) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Accuracy&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Response time&quot;</span>)</code></pre></div>

<div class="figure"><span id="fig:postppdobs-1"></span>
<img src="bookdown_files/figure-html/postppdobs-1.svg" alt="Two simulated datasets in red overlaid to the observations in black." width="672" />
<p class="caption">
FIGURE 15.4: Two simulated datasets in red overlaid to the observations in black.
</p>
</div>
<div class="figure"><span id="fig:postppdobs-2"></span>
<img src="bookdown_files/figure-html/postppdobs-2.svg" alt="Two simulated datasets in red overlaid to the observations in black." width="672" />
<p class="caption">
FIGURE 15.5: Two simulated datasets in red overlaid to the observations in black.
</p>
</div>
</div>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-britten_shadlen_newsome_movshon_1993">
<p>Britten, Kenneth H., Michael N. Shadlen, William T. Newsome, and J. Anthony Movshon. 1993. “Responses of Neurons in Macaque Mt to Stochastic Motion Signals.” <em>Visual Neuroscience</em> 10 (6). Cambridge University Press: 1157–69. doi:<a href="https://doi.org/10.1017/S0952523800010269">10.1017/S0952523800010269</a>.</p>
</div>
<div id="ref-brownSimplestCompleteModel2008">
<p>Brown, Scott D., and Andrew Heathcote. 2008. “The Simplest Complete Model of Choice Response Time: Linear Ballistic Accumulation.” <em>Cognitive Psychology</em> 57 (3): 153–78. doi:<a href="https://doi.org/10.1016/j.cogpsych.2007.12.002">10.1016/j.cogpsych.2007.12.002</a>.</p>
</div>
<div id="ref-Cooketal2006">
<p>Cook, Samantha R, Andrew Gelman, and Donald B Rubin. 2006. “Validation of Software for Bayesian Models Using Posterior Quantiles.” <em>Journal of Computational and Graphical Statistics</em> 15 (3). Taylor &amp; Francis: 675–92. doi:<a href="https://doi.org/10.1198/106186006X136976">10.1198/106186006X136976</a>.</p>
</div>
<div id="ref-Dutilh2019quality">
<p>Dutilh, Gilles, Jeffrey Annis, Scott D Brown, Peter Cassey, Nathan J Evans, Raoul PPP Grasman, Guy E Hawkins, et al. 2019. “The Quality of Response Time Data Inference: A Blinded, Collaborative Assessment of the Validity of Cognitive Models.” <em>Psychonomic Bulletin &amp; Review</em> 26 (4). Springer: 1051–69.</p>
</div>
<div id="ref-DutilhEtAl2011">
<p>Dutilh, Gilles, Eric-Jan Wagenmakers, Ingmar Visser, and Han L. J. van der Maas. 2011. “A Phase Transition Model for the Speed-Accuracy Trade-Off in Response Time Experiments.” <em>Cognitive Science</em> 35 (2): 211–50. doi:<a href="https://doi.org/10.1111/j.1551-6709.2010.01147.x">10.1111/j.1551-6709.2010.01147.x</a>.</p>
</div>
<div id="ref-DingEtAl">
<p>Han, Ding, Jana Wegrzyn, Hua Bi, Ruihua Wei, Bin Zhang, and Xiaorong Li. 2018. “Practice Makes the Deficiency of Global Motion Detection in People with Pattern-Related Visual Stress More Apparent.” <em>PLOS ONE</em> 13 (2). Public Library of Science: 1–13. doi:<a href="https://doi.org/10.1371/journal.pone.0193215">10.1371/journal.pone.0193215</a>.</p>
</div>
<div id="ref-Ollman1966">
<p>Ollman, Robert. 1966. “Fast Guesses in Choice Reaction Time.” <em>Psychonomic Science</em> 6 (4). Springer: 155–56.</p>
</div>
<div id="ref-Ratcliff1978">
<p>Ratcliff, Roger. 1978. “A Theory of Memory Retrieval.” <em>Psychological Review</em> 85 (2). American Psychological Association: 59.</p>
</div>
<div id="ref-Ratcliff2016">
<p>Ratcliff, Roger, Philip L. Smith, Scott D. Brown, and Gail McKoon. 2016. “Diffusion Decision Model: Current Issues and History.” <em>Trends in Cognitive Sciences</em> 20 (4): 260–81. doi:<a href="https://doi.org/https://doi.org/10.1016/j.tics.2016.01.007">https://doi.org/10.1016/j.tics.2016.01.007</a>.</p>
</div>
<div id="ref-schad2020toward">
<p>Schad, Daniel J., Michael Betancourt, and Shravan Vasishth. 2020. “Toward a Principled Bayesian Workflow in Cognitive Science.” <em>Psychological Methods</em>.</p>
</div>
<div id="ref-talts2018validating">
<p>Talts, Sean, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. 2018. “Validating Bayesian Inference Algorithms with Simulation-Based Calibration.” <em>arXiv Preprint arXiv:1804.06788</em>.</p>
</div>
<div id="ref-wickelgren1977speed">
<p>Wickelgren, Wayne A. 1977. “Speed-Accuracy Tradeoff and Information Processing Dynamics.” <em>Acta Psychologica</em> 41 (1): 67–85.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="31">
<li id="fn31"><p>The complete dataset can be found in <a href="https://osf.io/utkjf/" class="uri">https://osf.io/utkjf/</a><a href="a-mixture-model-of-the-speed-accuracy-trade-off.html#fnref31">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-mixture.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown/edit/master/inst/examples/26-mixtures.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
